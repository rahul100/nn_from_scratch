{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "The code below fits a simple neural network to a dataset. The goal of this exercise is to fill-in some missing functions in the implementation.\n",
        "\n",
        "Please read the instructions and the code carefully before beginning to implement.\n",
        "## The Model\n",
        "Our neural network conatins a single hidden layer which is followed by an output layer. The network is defined as follows.\n",
        "\n",
        "Given the input vector $x \\in \\mathbb{R}^{D_x}$, the hidden layer outputs the vector $h \\in \\mathbb{R}^{D_h}$ which is given as\n",
        "\\begin{align}h = f(Ax+b) \\end{align}where:\n",
        "* $A \\in \\mathbb{R}^{D_x \\times D_h}$ is a weights matrix\n",
        "* $b \\in \\mathbb{R}^{D_h}$ is a bias vector\n",
        "* $f$ is the Swish activation function, given by $f(x)=x*sigmoid(x)$\n",
        "\n",
        "The output layer takes $h$ as in input and produces the output vector $y$, which is given as\n",
        "   \n",
        "\n",
        "\\begin{align}y = Ch+d\\end{align}where:\n",
        "* $C \\in \\mathbb{R}^{D_h \\times D_y}$ is a weights matrix\n",
        "* $d \\in \\mathbb{R}^{D_y}$ is a bias vector\n",
        "\n",
        "## The Task\n",
        "\n",
        "### Implementing the Model\n",
        "The $NeuralNetSolver$ class contains the definition of the model, the forward and backward pass calculation and the fitting method. Please make the following changes in that class:\n",
        "1. (5 pts) Implement the $get\\_loss$ method to calculate a Mean Squared Error loss.\n",
        "2. (5 pts) Implement the $forward$ method according to the definition of the model.\n",
        "3. (40 pts) Implement the method $get\\_layer\\_1\\_grads$ which should return the gradients of the hidden layer's weights matrix $A$ and biases vector $b$. For a refresher on how to do the math, see e.g. [here](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf) (note that this example uses a different activation function).\n",
        "\n",
        "### Improving the Termination Criteria of the Solver\n",
        "(10 pts) The $TerminationCriteria$ class contains the criteria that determine when to stop training the model. The stub implementation simply stops after a predetermined number of iterations. Please change the implementation so that it would be more effective and robust.\n",
        "\n",
        "### Improving the Optimizer\n",
        "The $Optimizer$ class takes care of calculating the parameter updates, given the gradients. The stub implementation is a simple gradient descent and it is slow to converge. Please change it to have:\n",
        "1. (20 pts) A [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) term.\n",
        "2. (20 pts) An adaptive learning rate for each parameter, as defined in [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp)."
      ],
      "metadata": {
        "id": "VbZ_XVFAZjlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ekMTm7PQMWR_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "pdz528n_5Ea2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "from typing import Tuple, List, Collection, Mapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "sH2KbAMRMfke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    \"\"\"\n",
        "      This class is responsible for calculating parameter updates during optimization.\n",
        "    \"\"\"\n",
        "    LEARNING_RATE = 1e-2\n",
        "    BETA1 = 0.9\n",
        "    BETA2 = 0.999\n",
        "    EPSILON = 1e-8\n",
        "\n",
        "    def __init__(self, param_names: Collection[str]):\n",
        "        \"\"\"\n",
        "        :param param_names:\n",
        "          A list of the parameter names to be optimized.\n",
        "        \"\"\"\n",
        "        self.param_names = param_names\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        for param_name in param_names:\n",
        "            self.m[param_name] = np.zeros_like(self.param_names[param_name])\n",
        "            self.v[param_name] = np.zeros_like(self.param_names[param_name])\n",
        "\n",
        "    def step(self, param_grads: Mapping[str, np.array]) -> Mapping[str, np.array]:\n",
        "        \"\"\"\n",
        "            Calculate the parameter updates for a single step of optimization.\n",
        "        :param param_grads:\n",
        "            A dictionary of the parameter gradients.\n",
        "        :return:\n",
        "          A dictionary of parameter updates.\n",
        "        \"\"\"\n",
        "        param_updates = {}\n",
        "        for param_name, param_grad in param_grads.items():\n",
        "            self.m[param_name] = self.BETA1 * self.m[param_name] + (1 - self.BETA1) * param_grad\n",
        "            self.v[param_name] = self.BETA2 * self.v[param_name] + (1 - self.BETA2) * param_grad ** 2\n",
        "            m_hat = self.m[param_name] / (1 - self.BETA1)\n",
        "            v_hat = self.v[param_name] / (1 - self.BETA2)\n",
        "            param_updates[param_name] = self.LEARNING_RATE * m_hat / (np.sqrt(v_hat) + self.EPSILON)\n",
        "\n",
        "        return param_updates\n",
        "\n"
      ],
      "metadata": {
        "id": "wfPQMkhyavPr"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Termination Criteria"
      ],
      "metadata": {
        "id": "WRCKY78BZBrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TerminationCriteria:\n",
        "    def __init__(self, max_iter, min_train_loss):\n",
        "        self.max_iter = max_iter\n",
        "        self.min_train_loss = min_train_loss\n",
        "        self.best_loss = np.inf\n",
        "        self.patience = 10\n",
        "\n",
        "    def is_over(self, iter: int, training_loss: float) -> bool:\n",
        "        \"\"\"\n",
        "        : iter\n",
        "            The iteration number\n",
        "        :return:\n",
        "            True if the optimization should terminate, False otherwise.\n",
        "        \"\"\"\n",
        "        if training_loss < self.best_loss:\n",
        "            self.best_loss = training_loss\n",
        "            self.patience = 10\n",
        "        else:\n",
        "            self.patience -= 1\n",
        "        return iter > self.max_iter or training_loss < self.min_train_loss or self.patience == 0\n"
      ],
      "metadata": {
        "id": "9Vewiy3voc0W"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "lLTdrf3wMjwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetSolver:\n",
        "    HIDDEN_DIM = 32\n",
        "    INPUT_DIM = 1\n",
        "    OUTPUT_DIM = 1\n",
        "\n",
        "    MAX_ITER = 100000\n",
        "    MIN_TRAIN_LOSS = 1e-2\n",
        "    INIT_RANGE = 1\n",
        "    NUM_EVALS = 100\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        C'tor\n",
        "        :param input_dim:\n",
        "            The dimension of the input vector\n",
        "        \"\"\"\n",
        "        self.params_dict = {\n",
        "            'weights_1': np.random.normal(0, self.INIT_RANGE, (self.INPUT_DIM, self.HIDDEN_DIM)),\n",
        "            'bias_1': np.zeros(shape=(1, self.HIDDEN_DIM)),\n",
        "            'weights_2': np.random.normal(0, self.INIT_RANGE, (self.HIDDEN_DIM, self.OUTPUT_DIM)),\n",
        "            'bias_2': np.zeros(shape=(1, self.OUTPUT_DIM))}\n",
        "        # print(\"==================init shapes=====================\")\n",
        "        # print(\"W1: \" ,self.params_dict['weights_1'].shape)\n",
        "        # print(\"b1: \" ,self.params_dict['bias_1'].shape)\n",
        "\n",
        "        # print(\"W2: \" ,self.params_dict['weights_2'].shape)\n",
        "        # print(\"b2: \" ,self.params_dict['bias_2'].shape)\n",
        "\n",
        "        # print(\"========================================\")\n",
        "        self.optimizer = Optimizer(self.params_dict)\n",
        "        self.termination_criteria = TerminationCriteria(self.MAX_ITER, self.MIN_TRAIN_LOSS)\n",
        "\n",
        "    @staticmethod\n",
        "    def act_fn(x: np.array) -> np.array:\n",
        "        \"\"\"\n",
        "            The activation function.\n",
        "        :param x:\n",
        "            An input array\n",
        "        :return:\n",
        "            The result of activation function applied to the x element-wise.\n",
        "        \"\"\"\n",
        "        return x / (1 + np.exp(-x))\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "       return 1 / (1 + np.exp(-x))\n",
        "    @staticmethod\n",
        "    def swish_prime(x):\n",
        "      return NeuralNetSolver.act_fn(x) + NeuralNetSolver.sigmoid(x)*(1 -  NeuralNetSolver.act_fn(x))\n",
        "\n",
        "\n",
        "    def forward(self, x: np.array) -> np.array:\n",
        "        \"\"\"\n",
        "        Makes a prediction with the model.\n",
        "        :param x:\n",
        "            A batch of input feature vectors. The shape is [B, D_IN], where B is the batch dimension\n",
        "            and D_IN is the dimension of the feature vector.\n",
        "        :return:\n",
        "            A batch of predictions. The shape is [B, D_OUT], where B is the batch dimension\n",
        "            and D_OUT is the dimension of the prediction vector.\n",
        "        \"\"\"\n",
        "        assert x.shape[1] == self.params_dict['weights_1'].shape[0]\n",
        "\n",
        "\n",
        "        # PLEASE IMPLEMENT THE FORWARD PASS HERE\n",
        "        # result = np.zeros((x.shape[0], self.OUTPUT_DIM))\n",
        "        hidden1 = NeuralNetSolver.act_fn(x @ self.params_dict['weights_1'] + self.params_dict['bias_1'])\n",
        "        # print(\"Hidden layer after activation: \", hidden1.shape)\n",
        "        result = hidden1 @ self.params_dict['weights_2'] + self.params_dict['bias_2']\n",
        "\n",
        "        assert result.ndim == 2\n",
        "        assert result.shape[0] == x.shape[0]\n",
        "        assert result.shape[1] == self.OUTPUT_DIM\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_loss_grad(self, X: np.array, y: np.array) -> Mapping[str, np.array]:\n",
        "        \"\"\"\n",
        "            Calculates the gradient of the loss function.\n",
        "        :param X:\n",
        "            A batch of input feature vectors. The shape is [B, D_IN], where B is the batch dimension\n",
        "            and D_IN is the dimension of the feature vector.\n",
        "        :param y:\n",
        "            A batch of outputs. The shape is [B, D_OUT], where B is the batch dimension\n",
        "            and D_OUT is the dimension of the output vector.\n",
        "        :return:\n",
        "            A\n",
        "        \"\"\"\n",
        "        layer_1_out = X @ self.params_dict['weights_1'] + self.params_dict['bias_1']\n",
        "        # print(\"layer 1 out dimension without activation\", layer_1_out.shape)\n",
        "        layer_2_out = self.forward(X)\n",
        "        # print(\"layer 2 out dimension\", layer_2_out.shape)\n",
        "\n",
        "        weights2_grad, bias2_grad = self.get_layer_2_grads(X, y, layer_1_out, layer_2_out)\n",
        "\n",
        "        # print(\"weight2_gradient: \", weights2_grad.shape)\n",
        "        # print(\"biast2_gradient: \", bias2_grad.shape)\n",
        "\n",
        "        weights1_grad, bias1_grad = self.get_layer_1_grads(X, y, layer_1_out, layer_2_out)\n",
        "\n",
        "\n",
        "        return {'weights_1': weights1_grad, 'bias_1': bias1_grad,\n",
        "                'weights_2': weights2_grad, 'bias_2': bias2_grad}\n",
        "\n",
        "    def get_layer_1_grads(self, X: np.array, y: np.array,\n",
        "                          layer_1_out: np.array, layer_2_out: np.array) -> Tuple[\n",
        "        np.array, np.array]:\n",
        "        \"\"\"\n",
        "            Calculates the gradients for the first layer of the neural network.\n",
        "        :param X:\n",
        "            A batch of input feature vectors. The shape is [B, D_IN], where B is the batch dimension\n",
        "            and D_IN is the dimension of the feature vector.\n",
        "        :param y:\n",
        "            A batch of outputs. The shape is [B, D_OUT], where B is the batch dimension\n",
        "            and D_OUT is the dimension of the output vector.\n",
        "        :param layer_1_out:\n",
        "            The output of the first linear layer, without activation.\n",
        "        :param layer_2_out:\n",
        "            The output of the 2nd layer.\n",
        "        :return:\n",
        "            A [HIDDEN_DIM] numpy array of weight gradients and a [1] numpy array of bias gradients\n",
        "        \"\"\"\n",
        "\n",
        "        # PLEASE REPLACE THE FOLLOWING TWO LINES WITH A CALCULATION OF THE GRADIENTS\n",
        "\n",
        "        delta_o = (layer_2_out - y)\n",
        "        # print(\"Delta_o_shape: \", delta_o.shape)\n",
        "\n",
        "        delta_h = (delta_o @ (self.params_dict['weights_2'].T)) * self.swish_prime(layer_1_out)\n",
        "\n",
        "        # print(\"shape of delta_h\", delta_h.shape)\n",
        "\n",
        "        weights1_grad = (delta_h.T @ X).T\n",
        "        # np.zeros((self.INPUT_DIM, self.HIDDEN_DIM))\n",
        "        bias1_grad = delta_h.sum(axis=0,keepdims=True)\n",
        "        # np.zeros((1, self.HIDDEN_DIM))\n",
        "\n",
        "        # print(\"====W1,b1 gradients=======\")\n",
        "        # print(weights1_grad.shape)\n",
        "        # print(bias1_grad.shape)\n",
        "\n",
        "\n",
        "        assert weights1_grad.shape[0] == self.INPUT_DIM\n",
        "        assert weights1_grad.shape[1] == self.HIDDEN_DIM\n",
        "        assert weights1_grad.ndim == 2\n",
        "\n",
        "        assert bias1_grad.shape[0] == 1\n",
        "        assert bias1_grad.shape[1] == self.HIDDEN_DIM\n",
        "        assert bias1_grad.ndim == 2\n",
        "\n",
        "        return weights1_grad, bias1_grad\n",
        "\n",
        "    def get_layer_2_grads(self, X: np.array, y: np.array,\n",
        "                          layer_1_out: np.array, layer_2_out: np.array) -> Tuple[\n",
        "        np.array, np.array]:\n",
        "        \"\"\"\n",
        "               Calculates the gradients for the second layer of the neural network.\n",
        "           :param X:\n",
        "               A batch of input feature vectors. The shape is [B, D_IN], where B is the batch dimension\n",
        "               and D_IN is the dimension of the feature vector.\n",
        "           :param y:\n",
        "               A batch of outputs. The shape is [B, D_OUT], where B is the batch dimension\n",
        "               and D_OUT is the dimension of the output vector.\n",
        "           :param layer_1_out:\n",
        "               The output of the first linear layer, without activation.\n",
        "           :param layer_2_out:\n",
        "               The output of the 2nd layer.\n",
        "           :return:\n",
        "               A [HIDDEN_DIM] numpy array of weight gradients and a [1] numpy array of bias gradients\n",
        "           \"\"\"\n",
        "        weights2_grad = ((layer_2_out - y).T @ self.act_fn(layer_1_out)).T\n",
        "        bias2_grad = np.sum((layer_2_out - y), keepdims=True)\n",
        "\n",
        "        assert weights2_grad.ndim == 2\n",
        "        assert weights2_grad.shape[0] == self.HIDDEN_DIM\n",
        "        assert weights2_grad.shape[1] == self.OUTPUT_DIM\n",
        "\n",
        "        assert bias2_grad.shape[0] == 1\n",
        "        assert bias2_grad.ndim == 2\n",
        "\n",
        "        return weights2_grad, bias2_grad\n",
        "\n",
        "    @staticmethod\n",
        "    def get_loss(y, y_hat) -> np.array:\n",
        "        \"\"\"\n",
        "            Calculate the loss\n",
        "        :param y:\n",
        "            The ground-truth responses.\n",
        "        :param y_hat:\n",
        "            The predicted responses.\n",
        "        :return:\n",
        "            A scalar containing the loss.\n",
        "        \"\"\"\n",
        "        # PLEASE REPLACE WITH AN IMPLEMENTATION OF THE LOSS\n",
        "        # print(y)\n",
        "        # print(y_hat)\n",
        "        return np.mean((y-y_hat) ** 2)\n",
        "\n",
        "    def fit(self, X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array) -> \\\n",
        "    Tuple[List, List]:\n",
        "        \"\"\"\n",
        "            Fits the model to the given data.\n",
        "        :param X_train:\n",
        "            The training features\n",
        "        :param y_train:\n",
        "            The training outputs\n",
        "        :param X_test:\n",
        "            The test features\n",
        "        :param y_test:\n",
        "            The test output\n",
        "        :return:\n",
        "            The train losses and the test losses\n",
        "        \"\"\"\n",
        "        iter = 0\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        steps_for_eval = self.MAX_ITER // self.NUM_EVALS\n",
        "        print(f\"steps_for_eval={steps_for_eval}\")\n",
        "\n",
        "        train_loss = np.inf # sentinel\n",
        "        while not self.termination_criteria.is_over(iter,train_loss):\n",
        "            y_hat = self.forward(X_train)\n",
        "            train_loss = self.get_loss(y_hat, y_train)\n",
        "            print(f'Iteration {iter}, Train Loss={train_loss}')\n",
        "            loss_grad = self.get_loss_grad(X_train, y_train)\n",
        "\n",
        "            param_update_dict = self.optimizer.step(loss_grad)\n",
        "\n",
        "            for param_name, param_update in param_update_dict.items():\n",
        "                self.params_dict[param_name] -= param_update\n",
        "\n",
        "            iter += 1\n",
        "            if iter % steps_for_eval == 0:\n",
        "                y_hat = self.forward(X_test)\n",
        "                test_loss = self.get_loss(y_hat, y_test)\n",
        "                train_losses.append(train_loss)\n",
        "                test_losses.append(test_loss)\n",
        "\n",
        "                print(f'Iteration {iter}, Test Loss={test_loss}')\n",
        "        return train_losses, test_losses"
      ],
      "metadata": {
        "id": "i0XZEcX-MlTr"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data and Train"
      ],
      "metadata": {
        "id": "MWjSloB2PMMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.expand_dims(np.linspace(0, 100, 50), -1)\n",
        "\n",
        "y = np.expand_dims((5 * X[:, 0] + 3 * X[:, 0] ** 2 + 50), -1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "nn = NeuralNetSolver()\n",
        "train_losses, test_losses = nn.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js6vkSNAPPR9",
        "outputId": "c971b85d-166e-4c76-8cbf-63b5a8b39c7d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 11570, Train Loss=331080.75712803536\n",
            "Iteration 11571, Train Loss=330884.7180908242\n",
            "Iteration 11572, Train Loss=330688.7514229075\n",
            "Iteration 11573, Train Loss=330492.85769781703\n",
            "Iteration 11574, Train Loss=330297.0374732452\n",
            "Iteration 11575, Train Loss=330101.29129013384\n",
            "Iteration 11576, Train Loss=329905.619673241\n",
            "Iteration 11577, Train Loss=329710.0231298336\n",
            "Iteration 11578, Train Loss=329514.5021506621\n",
            "Iteration 11579, Train Loss=329319.0572084046\n",
            "Iteration 11580, Train Loss=329123.68875888956\n",
            "Iteration 11581, Train Loss=328928.3972394058\n",
            "Iteration 11582, Train Loss=328733.18307007616\n",
            "Iteration 11583, Train Loss=328538.0466521077\n",
            "Iteration 11584, Train Loss=328342.98836924334\n",
            "Iteration 11585, Train Loss=328148.00858598656\n",
            "Iteration 11586, Train Loss=327953.1076491252\n",
            "Iteration 11587, Train Loss=327758.2858859108\n",
            "Iteration 11588, Train Loss=327563.54360567435\n",
            "Iteration 11589, Train Loss=327368.88109792816\n",
            "Iteration 11590, Train Loss=327174.29863411543\n",
            "Iteration 11591, Train Loss=326979.7964655729\n",
            "Iteration 11592, Train Loss=326785.374825488\n",
            "Iteration 11593, Train Loss=326591.0339266166\n",
            "Iteration 11594, Train Loss=326396.7739635715\n",
            "Iteration 11595, Train Loss=326202.59511015157\n",
            "Iteration 11596, Train Loss=326008.49752211635\n",
            "Iteration 11597, Train Loss=325814.48133393395\n",
            "Iteration 11598, Train Loss=325620.54666226386\n",
            "Iteration 11599, Train Loss=325426.6936018504\n",
            "Iteration 11600, Train Loss=325232.9222300492\n",
            "Iteration 11601, Train Loss=325039.2326014415\n",
            "Iteration 11602, Train Loss=324845.6247539112\n",
            "Iteration 11603, Train Loss=324652.09870136436\n",
            "Iteration 11604, Train Loss=324458.6544421234\n",
            "Iteration 11605, Train Loss=324265.29194879753\n",
            "Iteration 11606, Train Loss=324072.0111801846\n",
            "Iteration 11607, Train Loss=323878.8120668317\n",
            "Iteration 11608, Train Loss=323685.69452831097\n",
            "Iteration 11609, Train Loss=323492.65845215623\n",
            "Iteration 11610, Train Loss=323299.7037195479\n",
            "Iteration 11611, Train Loss=323106.83017391566\n",
            "Iteration 11612, Train Loss=322914.03766002215\n",
            "Iteration 11613, Train Loss=322721.3259762792\n",
            "Iteration 11614, Train Loss=322528.6949356609\n",
            "Iteration 11615, Train Loss=322336.1442921596\n",
            "Iteration 11616, Train Loss=322143.673838232\n",
            "Iteration 11617, Train Loss=321951.283290308\n",
            "Iteration 11618, Train Loss=321758.97244964074\n",
            "Iteration 11619, Train Loss=321566.7410243714\n",
            "Iteration 11620, Train Loss=321374.5889062439\n",
            "Iteration 11621, Train Loss=321182.5159011514\n",
            "Iteration 11622, Train Loss=320990.52223393164\n",
            "Iteration 11623, Train Loss=320798.6081749381\n",
            "Iteration 11624, Train Loss=320606.7750422225\n",
            "Iteration 11625, Train Loss=320415.0248291624\n",
            "Iteration 11626, Train Loss=320223.36243620375\n",
            "Iteration 11627, Train Loss=320031.79593652126\n",
            "Iteration 11628, Train Loss=319840.3422692224\n",
            "Iteration 11629, Train Loss=319649.03090029483\n",
            "Iteration 11630, Train Loss=319457.92041975574\n",
            "Iteration 11631, Train Loss=319267.116332144\n",
            "Iteration 11632, Train Loss=319076.82482397987\n",
            "Iteration 11633, Train Loss=318887.42331725004\n",
            "Iteration 11634, Train Loss=318699.64421841985\n",
            "Iteration 11635, Train Loss=318514.8122481691\n",
            "Iteration 11636, Train Loss=318335.43420093023\n",
            "Iteration 11637, Train Loss=318165.6425611143\n",
            "Iteration 11638, Train Loss=318012.2540919328\n",
            "Iteration 11639, Train Loss=317881.905326103\n",
            "Iteration 11640, Train Loss=317776.0759212043\n",
            "Iteration 11641, Train Loss=317665.4035570561\n",
            "Iteration 11642, Train Loss=317490.72404007835\n",
            "Iteration 11643, Train Loss=317196.5892984204\n",
            "Iteration 11644, Train Loss=316852.78014338313\n",
            "Iteration 11645, Train Loss=316596.54223011946\n",
            "Iteration 11646, Train Loss=316468.0373212523\n",
            "Iteration 11647, Train Loss=316364.18493646535\n",
            "Iteration 11648, Train Loss=316167.3016717729\n",
            "Iteration 11649, Train Loss=315889.50412444555\n",
            "Iteration 11650, Train Loss=315647.5471600947\n",
            "Iteration 11651, Train Loss=315495.5804500569\n",
            "Iteration 11652, Train Loss=315357.6683274219\n",
            "Iteration 11653, Train Loss=315151.5491093818\n",
            "Iteration 11654, Train Loss=314906.2016911161\n",
            "Iteration 11655, Train Loss=314703.20493129134\n",
            "Iteration 11656, Train Loss=314548.9961960492\n",
            "Iteration 11657, Train Loss=314377.83718366723\n",
            "Iteration 11658, Train Loss=314161.07035697263\n",
            "Iteration 11659, Train Loss=313944.62938486366\n",
            "Iteration 11660, Train Loss=313766.05876175757\n",
            "Iteration 11661, Train Loss=313598.8943032601\n",
            "Iteration 11662, Train Loss=313404.7101045494\n",
            "Iteration 11663, Train Loss=313193.7235499326\n",
            "Iteration 11664, Train Loss=313000.7004184535\n",
            "Iteration 11665, Train Loss=312826.4634302758\n",
            "Iteration 11666, Train Loss=312642.98656161095\n",
            "Iteration 11667, Train Loss=312442.08819870977\n",
            "Iteration 11668, Train Loss=312244.2302071507\n",
            "Iteration 11669, Train Loss=312061.6162349505\n",
            "Iteration 11670, Train Loss=311880.78872138396\n",
            "Iteration 11671, Train Loss=311688.2374762685\n",
            "Iteration 11672, Train Loss=311491.30419391795\n",
            "Iteration 11673, Train Loss=311302.82682445005\n",
            "Iteration 11674, Train Loss=311120.3548978986\n",
            "Iteration 11675, Train Loss=310932.8380294313\n",
            "Iteration 11676, Train Loss=310739.1554640633\n",
            "Iteration 11677, Train Loss=310547.96969572786\n",
            "Iteration 11678, Train Loss=310362.572497798\n",
            "Iteration 11679, Train Loss=310176.91891945375\n",
            "Iteration 11680, Train Loss=309986.6111428587\n",
            "Iteration 11681, Train Loss=309795.1999374478\n",
            "Iteration 11682, Train Loss=309607.30938920967\n",
            "Iteration 11683, Train Loss=309421.4493859295\n",
            "Iteration 11684, Train Loss=309233.4336232715\n",
            "Iteration 11685, Train Loss=309043.1646782602\n",
            "Iteration 11686, Train Loss=308853.9633909167\n",
            "Iteration 11687, Train Loss=308666.9829911037\n",
            "Iteration 11688, Train Loss=308479.9185077873\n",
            "Iteration 11689, Train Loss=308291.0682410002\n",
            "Iteration 11690, Train Loss=308101.7195635947\n",
            "Iteration 11691, Train Loss=307913.66953791166\n",
            "Iteration 11692, Train Loss=307726.5103926611\n",
            "Iteration 11693, Train Loss=307538.65022731863\n",
            "Iteration 11694, Train Loss=307349.83630286774\n",
            "Iteration 11695, Train Loss=307161.26116611256\n",
            "Iteration 11696, Train Loss=306973.56186380587\n",
            "Iteration 11697, Train Loss=306786.01998546254\n",
            "Iteration 11698, Train Loss=306597.848231934\n",
            "Iteration 11699, Train Loss=306409.31102746807\n",
            "Iteration 11700, Train Loss=306221.14875505184\n",
            "Iteration 11701, Train Loss=306033.44039635814\n",
            "Iteration 11702, Train Loss=305845.6223778647\n",
            "Iteration 11703, Train Loss=305657.3991368616\n",
            "Iteration 11704, Train Loss=305469.094213985\n",
            "Iteration 11705, Train Loss=305281.085016514\n",
            "Iteration 11706, Train Loss=305093.2713759112\n",
            "Iteration 11707, Train Loss=304905.3051911158\n",
            "Iteration 11708, Train Loss=304717.11348331464\n",
            "Iteration 11709, Train Loss=304528.9389255691\n",
            "Iteration 11710, Train Loss=304340.95288809075\n",
            "Iteration 11711, Train Loss=304153.0381252044\n",
            "Iteration 11712, Train Loss=303965.0010261409\n",
            "Iteration 11713, Train Loss=303776.8450720079\n",
            "Iteration 11714, Train Loss=303588.72452237376\n",
            "Iteration 11715, Train Loss=303400.7137753308\n",
            "Iteration 11716, Train Loss=303212.72508960287\n",
            "Iteration 11717, Train Loss=303024.6541527988\n",
            "Iteration 11718, Train Loss=302836.52012980095\n",
            "Iteration 11719, Train Loss=302648.41477253364\n",
            "Iteration 11720, Train Loss=302460.3723821224\n",
            "Iteration 11721, Train Loss=302272.3374143002\n",
            "Iteration 11722, Train Loss=302084.25261061394\n",
            "Iteration 11723, Train Loss=301896.13355323597\n",
            "Iteration 11724, Train Loss=301708.0337660799\n",
            "Iteration 11725, Train Loss=301519.972440587\n",
            "Iteration 11726, Train Loss=301331.9179500065\n",
            "Iteration 11727, Train Loss=301143.83746644715\n",
            "Iteration 11728, Train Loss=300955.73969885346\n",
            "Iteration 11729, Train Loss=300767.6558440418\n",
            "Iteration 11730, Train Loss=300579.59949506476\n",
            "Iteration 11731, Train Loss=300391.5546895515\n",
            "Iteration 11732, Train Loss=300203.50199085067\n",
            "Iteration 11733, Train Loss=300015.4447199392\n",
            "Iteration 11734, Train Loss=299827.40093476436\n",
            "Iteration 11735, Train Loss=299639.38163914287\n",
            "Iteration 11736, Train Loss=299451.38054913224\n",
            "Iteration 11737, Train Loss=299263.38637197716\n",
            "Iteration 11738, Train Loss=299075.399295471\n",
            "Iteration 11739, Train Loss=298887.42940939835\n",
            "Iteration 11740, Train Loss=298699.48574954265\n",
            "Iteration 11741, Train Loss=298511.56767572387\n",
            "Iteration 11742, Train Loss=298323.6693775001\n",
            "Iteration 11743, Train Loss=298135.7897984028\n",
            "Iteration 11744, Train Loss=297947.9341329426\n",
            "Iteration 11745, Train Loss=297760.10940051434\n",
            "Iteration 11746, Train Loss=297572.3178593307\n",
            "Iteration 11747, Train Loss=297384.55753333785\n",
            "Iteration 11748, Train Loss=297196.8274226355\n",
            "Iteration 11749, Train Loss=297009.12986203626\n",
            "Iteration 11750, Train Loss=296821.4698236494\n",
            "Iteration 11751, Train Loss=296633.850573954\n",
            "Iteration 11752, Train Loss=296446.27265284676\n",
            "Iteration 11753, Train Loss=296258.7358202748\n",
            "Iteration 11754, Train Loss=296071.241018254\n",
            "Iteration 11755, Train Loss=295883.7914015053\n",
            "Iteration 11756, Train Loss=295696.3900298525\n",
            "Iteration 11757, Train Loss=295509.0387649976\n",
            "Iteration 11758, Train Loss=295321.73827571573\n",
            "Iteration 11759, Train Loss=295134.48913052416\n",
            "Iteration 11760, Train Loss=294947.293159004\n",
            "Iteration 11761, Train Loss=294760.15261715103\n",
            "Iteration 11762, Train Loss=294573.0696729368\n",
            "Iteration 11763, Train Loss=294386.0456134488\n",
            "Iteration 11764, Train Loss=294199.0811779009\n",
            "Iteration 11765, Train Loss=294012.17746124696\n",
            "Iteration 11766, Train Loss=293825.3358525211\n",
            "Iteration 11767, Train Loss=293638.5581432007\n",
            "Iteration 11768, Train Loss=293451.8457307412\n",
            "Iteration 11769, Train Loss=293265.1995970564\n",
            "Iteration 11770, Train Loss=293078.6205559481\n",
            "Iteration 11771, Train Loss=292892.10939765826\n",
            "Iteration 11772, Train Loss=292705.6672793816\n",
            "Iteration 11773, Train Loss=292519.29526667856\n",
            "Iteration 11774, Train Loss=292332.99433218676\n",
            "Iteration 11775, Train Loss=292146.765165849\n",
            "Iteration 11776, Train Loss=291960.60826588277\n",
            "Iteration 11777, Train Loss=291774.5242349952\n",
            "Iteration 11778, Train Loss=291588.51362989936\n",
            "Iteration 11779, Train Loss=291402.577106624\n",
            "Iteration 11780, Train Loss=291216.71513125574\n",
            "Iteration 11781, Train Loss=291030.92802855605\n",
            "Iteration 11782, Train Loss=290845.2160263929\n",
            "Iteration 11783, Train Loss=290659.5792462042\n",
            "Iteration 11784, Train Loss=290474.01788697514\n",
            "Iteration 11785, Train Loss=290288.53203943773\n",
            "Iteration 11786, Train Loss=290103.1217746654\n",
            "Iteration 11787, Train Loss=289917.7870211462\n",
            "Iteration 11788, Train Loss=289732.5275967095\n",
            "Iteration 11789, Train Loss=289547.34328586207\n",
            "Iteration 11790, Train Loss=289362.2337756537\n",
            "Iteration 11791, Train Loss=289177.1987731644\n",
            "Iteration 11792, Train Loss=288992.23787221446\n",
            "Iteration 11793, Train Loss=288807.35061841924\n",
            "Iteration 11794, Train Loss=288622.53646141523\n",
            "Iteration 11795, Train Loss=288437.79476171784\n",
            "Iteration 11796, Train Loss=288253.1248541617\n",
            "Iteration 11797, Train Loss=288068.52598177845\n",
            "Iteration 11798, Train Loss=287883.9973783457\n",
            "Iteration 11799, Train Loss=287699.53818238294\n",
            "Iteration 11800, Train Loss=287515.1474853201\n",
            "Iteration 11801, Train Loss=287330.8243091622\n",
            "Iteration 11802, Train Loss=287146.5676043477\n",
            "Iteration 11803, Train Loss=286962.3762948095\n",
            "Iteration 11804, Train Loss=286778.2492276233\n",
            "Iteration 11805, Train Loss=286594.1852338484\n",
            "Iteration 11806, Train Loss=286410.18307166995\n",
            "Iteration 11807, Train Loss=286226.24146484473\n",
            "Iteration 11808, Train Loss=286042.3590865397\n",
            "Iteration 11809, Train Loss=285858.5345599848\n",
            "Iteration 11810, Train Loss=285674.7664872798\n",
            "Iteration 11811, Train Loss=285491.05341674166\n",
            "Iteration 11812, Train Loss=285307.39388826676\n",
            "Iteration 11813, Train Loss=285123.78639422567\n",
            "Iteration 11814, Train Loss=284940.2294134696\n",
            "Iteration 11815, Train Loss=284756.72139529674\n",
            "Iteration 11816, Train Loss=284573.26076772326\n",
            "Iteration 11817, Train Loss=284389.8459524318\n",
            "Iteration 11818, Train Loss=284206.4753482655\n",
            "Iteration 11819, Train Loss=284023.14736376936\n",
            "Iteration 11820, Train Loss=283839.8603911194\n",
            "Iteration 11821, Train Loss=283656.612837164\n",
            "Iteration 11822, Train Loss=283473.4031070483\n",
            "Iteration 11823, Train Loss=283290.2296203751\n",
            "Iteration 11824, Train Loss=283107.0908146127\n",
            "Iteration 11825, Train Loss=282923.9851429587\n",
            "Iteration 11826, Train Loss=282740.91109443136\n",
            "Iteration 11827, Train Loss=282557.8671798118\n",
            "Iteration 11828, Train Loss=282374.851957597\n",
            "Iteration 11829, Train Loss=282191.8640196241\n",
            "Iteration 11830, Train Loss=282008.9020118819\n",
            "Iteration 11831, Train Loss=281825.96462953975\n",
            "Iteration 11832, Train Loss=281643.05062626797\n",
            "Iteration 11833, Train Loss=281460.15882176027\n",
            "Iteration 11834, Train Loss=281277.28809950006\n",
            "Iteration 11835, Train Loss=281094.43742334854\n",
            "Iteration 11836, Train Loss=280911.6058287608\n",
            "Iteration 11837, Train Loss=280728.79244171863\n",
            "Iteration 11838, Train Loss=280545.99647018156\n",
            "Iteration 11839, Train Loss=280363.2172189716\n",
            "Iteration 11840, Train Loss=280180.45408662595\n",
            "Iteration 11841, Train Loss=279997.7065725598\n",
            "Iteration 11842, Train Loss=279814.9742810037\n",
            "Iteration 11843, Train Loss=279632.2569202126\n",
            "Iteration 11844, Train Loss=279449.5543118398\n",
            "Iteration 11845, Train Loss=279266.8663846785\n",
            "Iteration 11846, Train Loss=279084.1931859298\n",
            "Iteration 11847, Train Loss=278901.53487312177\n",
            "Iteration 11848, Train Loss=278718.89172352024\n",
            "Iteration 11849, Train Loss=278536.26412741945\n",
            "Iteration 11850, Train Loss=278353.6525930329\n",
            "Iteration 11851, Train Loss=278171.0577429209\n",
            "Iteration 11852, Train Loss=277988.48031336075\n",
            "Iteration 11853, Train Loss=277805.92115401337\n",
            "Iteration 11854, Train Loss=277623.3812223311\n",
            "Iteration 11855, Train Loss=277440.86158533406\n",
            "Iteration 11856, Train Loss=277258.36341060355\n",
            "Iteration 11857, Train Loss=277075.8879684724\n",
            "Iteration 11858, Train Loss=276893.4366214806\n",
            "Iteration 11859, Train Loss=276711.0108253293\n",
            "Iteration 11860, Train Loss=276528.61211844673\n",
            "Iteration 11861, Train Loss=276346.2421205584\n",
            "Iteration 11862, Train Loss=276163.9025234427\n",
            "Iteration 11863, Train Loss=275981.59508663893\n",
            "Iteration 11864, Train Loss=275799.3216298523\n",
            "Iteration 11865, Train Loss=275617.08402594517\n",
            "Iteration 11866, Train Loss=275434.88419494446\n",
            "Iteration 11867, Train Loss=275252.72409488895\n",
            "Iteration 11868, Train Loss=275070.6057170961\n",
            "Iteration 11869, Train Loss=274888.5310756611\n",
            "Iteration 11870, Train Loss=274706.5022035002\n",
            "Iteration 11871, Train Loss=274524.52114132943\n",
            "Iteration 11872, Train Loss=274342.5899340787\n",
            "Iteration 11873, Train Loss=274160.71062002453\n",
            "Iteration 11874, Train Loss=273978.88522727933\n",
            "Iteration 11875, Train Loss=273797.11576361646\n",
            "Iteration 11876, Train Loss=273615.40421287745\n",
            "Iteration 11877, Train Loss=273433.7525258269\n",
            "Iteration 11878, Train Loss=273252.1626164546\n",
            "Iteration 11879, Train Loss=273070.63635404815\n",
            "Iteration 11880, Train Loss=272889.17555944796\n",
            "Iteration 11881, Train Loss=272707.7819984044\n",
            "Iteration 11882, Train Loss=272526.457377907\n",
            "Iteration 11883, Train Loss=272345.203340809\n",
            "Iteration 11884, Train Loss=272164.0214623596\n",
            "Iteration 11885, Train Loss=271982.91324603953\n",
            "Iteration 11886, Train Loss=271801.8801204208\n",
            "Iteration 11887, Train Loss=271620.9234361305\n",
            "Iteration 11888, Train Loss=271440.044463143\n",
            "Iteration 11889, Train Loss=271259.24438880075\n",
            "Iteration 11890, Train Loss=271078.52431558137\n",
            "Iteration 11891, Train Loss=270897.88526014425\n",
            "Iteration 11892, Train Loss=270717.328151584\n",
            "Iteration 11893, Train Loss=270536.8538314649\n",
            "Iteration 11894, Train Loss=270356.46305255697\n",
            "Iteration 11895, Train Loss=270176.15647985716\n",
            "Iteration 11896, Train Loss=269995.9346897441\n",
            "Iteration 11897, Train Loss=269815.7981720244\n",
            "Iteration 11898, Train Loss=269635.74732941855\n",
            "Iteration 11899, Train Loss=269455.78248070314\n",
            "Iteration 11900, Train Loss=269275.903860386\n",
            "Iteration 11901, Train Loss=269096.1116230757\n",
            "Iteration 11902, Train Loss=268916.4058431232\n",
            "Iteration 11903, Train Loss=268736.7865204583\n",
            "Iteration 11904, Train Loss=268557.2535798484\n",
            "Iteration 11905, Train Loss=268377.80687856616\n",
            "Iteration 11906, Train Loss=268198.446204751\n",
            "Iteration 11907, Train Loss=268019.171287488\n",
            "Iteration 11908, Train Loss=267839.9817934579\n",
            "Iteration 11909, Train Loss=267660.8773403625\n",
            "Iteration 11910, Train Loss=267481.8574905855\n",
            "Iteration 11911, Train Loss=267302.9217694738\n",
            "Iteration 11912, Train Loss=267124.06965396676\n",
            "Iteration 11913, Train Loss=266945.30059820146\n",
            "Iteration 11914, Train Loss=266766.6140138141\n",
            "Iteration 11915, Train Loss=266588.00930699194\n",
            "Iteration 11916, Train Loss=266409.48584501096\n",
            "Iteration 11917, Train Loss=266231.04301173566\n",
            "Iteration 11918, Train Loss=266052.6801513753\n",
            "Iteration 11919, Train Loss=265874.39665453415\n",
            "Iteration 11920, Train Loss=265696.1918640933\n",
            "Iteration 11921, Train Loss=265518.06521328597\n",
            "Iteration 11922, Train Loss=265340.01606852084\n",
            "Iteration 11923, Train Loss=265162.04395883187\n",
            "Iteration 11924, Train Loss=264984.14831453544\n",
            "Iteration 11925, Train Loss=264806.3288638834\n",
            "Iteration 11926, Train Loss=264628.5852050377\n",
            "Iteration 11927, Train Loss=264450.91752704873\n",
            "Iteration 11928, Train Loss=264273.32594038284\n",
            "Iteration 11929, Train Loss=264095.81188272417\n",
            "Iteration 11930, Train Loss=263918.3772095414\n",
            "Iteration 11931, Train Loss=263741.0272229974\n",
            "Iteration 11932, Train Loss=263563.7700374667\n",
            "Iteration 11933, Train Loss=263386.62401392375\n",
            "Iteration 11934, Train Loss=263209.6202933956\n",
            "Iteration 11935, Train Loss=263032.8238599001\n",
            "Iteration 11936, Train Loss=262856.3519931572\n",
            "Iteration 11937, Train Loss=262680.4419235642\n",
            "Iteration 11938, Train Loss=262505.53577802796\n",
            "Iteration 11939, Train Loss=262332.5143415424\n",
            "Iteration 11940, Train Loss=262163.0179960669\n",
            "Iteration 11941, Train Loss=262000.21362085632\n",
            "Iteration 11942, Train Loss=261849.54698163195\n",
            "Iteration 11943, Train Loss=261720.0693534352\n",
            "Iteration 11944, Train Loss=261621.26807458093\n",
            "Iteration 11945, Train Loss=261554.33661885926\n",
            "Iteration 11946, Train Loss=261478.07177984854\n",
            "Iteration 11947, Train Loss=261306.95160570828\n",
            "Iteration 11948, Train Loss=260980.02592906245\n",
            "Iteration 11949, Train Loss=260615.66111533792\n",
            "Iteration 11950, Train Loss=260395.03961947787\n",
            "Iteration 11951, Train Loss=260324.99446004507\n",
            "Iteration 11952, Train Loss=260237.53270437475\n",
            "Iteration 11953, Train Loss=260006.73579151146\n",
            "Iteration 11954, Train Loss=259715.72277710255\n",
            "Iteration 11955, Train Loss=259523.67474797662\n",
            "Iteration 11956, Train Loss=259425.07061016932\n",
            "Iteration 11957, Train Loss=259281.06359890458\n",
            "Iteration 11958, Train Loss=259043.42873753578\n",
            "Iteration 11959, Train Loss=258817.26730309747\n",
            "Iteration 11960, Train Loss=258672.94749569223\n",
            "Iteration 11961, Train Loss=258540.87655492965\n",
            "Iteration 11962, Train Loss=258345.63439113632\n",
            "Iteration 11963, Train Loss=258126.89067360162\n",
            "Iteration 11964, Train Loss=257956.60945707504\n",
            "Iteration 11965, Train Loss=257815.22865469317\n",
            "Iteration 11966, Train Loss=257640.11833109474\n",
            "Iteration 11967, Train Loss=257436.30318984116\n",
            "Iteration 11968, Train Loss=257255.84111080357\n",
            "Iteration 11969, Train Loss=257103.02604374802\n",
            "Iteration 11970, Train Loss=256936.0017438458\n",
            "Iteration 11971, Train Loss=256744.68844498997\n",
            "Iteration 11972, Train Loss=256561.89245240777\n",
            "Iteration 11973, Train Loss=256400.41259274996\n",
            "Iteration 11974, Train Loss=256235.4811036621\n",
            "Iteration 11975, Train Loss=256053.15001607113\n",
            "Iteration 11976, Train Loss=255871.38463186397\n",
            "Iteration 11977, Train Loss=255704.17758766905\n",
            "Iteration 11978, Train Loss=255538.9425376486\n",
            "Iteration 11979, Train Loss=255362.471157288\n",
            "Iteration 11980, Train Loss=255183.00902578497\n",
            "Iteration 11981, Train Loss=255012.43499309066\n",
            "Iteration 11982, Train Loss=254845.9269521052\n",
            "Iteration 11983, Train Loss=254673.13011229114\n",
            "Iteration 11984, Train Loss=254496.23052130378\n",
            "Iteration 11985, Train Loss=254323.98440382452\n",
            "Iteration 11986, Train Loss=254156.05987323736\n",
            "Iteration 11987, Train Loss=253985.33371070042\n",
            "Iteration 11988, Train Loss=253810.82897996236\n",
            "Iteration 11989, Train Loss=253638.08867611393\n",
            "Iteration 11990, Train Loss=253468.9376289648\n",
            "Iteration 11991, Train Loss=253299.24341282964\n",
            "Iteration 11992, Train Loss=253126.72022491106\n",
            "Iteration 11993, Train Loss=252954.25609324762\n",
            "Iteration 11994, Train Loss=252784.26497579544\n",
            "Iteration 11995, Train Loss=252614.9282240363\n",
            "Iteration 11996, Train Loss=252443.9129856397\n",
            "Iteration 11997, Train Loss=252272.15925346772\n",
            "Iteration 11998, Train Loss=252101.76650997615\n",
            "Iteration 11999, Train Loss=251932.45162891195\n",
            "Iteration 12000, Test Loss=137338.10324660983\n",
            "Iteration 12000, Train Loss=251762.45633580946\n",
            "Iteration 12001, Train Loss=251591.58889994601\n",
            "Iteration 12002, Train Loss=251421.207463603\n",
            "Iteration 12003, Train Loss=251251.81445112103\n",
            "Iteration 12004, Train Loss=251082.44330383054\n",
            "Iteration 12005, Train Loss=250912.4306959891\n",
            "Iteration 12006, Train Loss=250742.38066797546\n",
            "Iteration 12007, Train Loss=250572.98625729728\n",
            "Iteration 12008, Train Loss=250403.95893447063\n",
            "Iteration 12009, Train Loss=250234.6585832835\n",
            "Iteration 12010, Train Loss=250065.1250876234\n",
            "Iteration 12011, Train Loss=249895.8897852806\n",
            "Iteration 12012, Train Loss=249727.07836356357\n",
            "Iteration 12013, Train Loss=249558.30326134368\n",
            "Iteration 12014, Train Loss=249389.3424738894\n",
            "Iteration 12015, Train Loss=249220.43187506188\n",
            "Iteration 12016, Train Loss=249051.83115609759\n",
            "Iteration 12017, Train Loss=248883.43577972328\n",
            "Iteration 12018, Train Loss=248715.00098389882\n",
            "Iteration 12019, Train Loss=248546.52837480808\n",
            "Iteration 12020, Train Loss=248378.2123022632\n",
            "Iteration 12021, Train Loss=248210.12415803364\n",
            "Iteration 12022, Train Loss=248042.13360022017\n",
            "Iteration 12023, Train Loss=247874.13418608002\n",
            "Iteration 12024, Train Loss=247706.1932503498\n",
            "Iteration 12025, Train Loss=247538.42049538653\n",
            "Iteration 12026, Train Loss=247370.80698876613\n",
            "Iteration 12027, Train Loss=247203.26192495445\n",
            "Iteration 12028, Train Loss=247035.75553725247\n",
            "Iteration 12029, Train Loss=246868.35155745587\n",
            "Iteration 12030, Train Loss=246701.0998336221\n",
            "Iteration 12031, Train Loss=246533.9723575933\n",
            "Iteration 12032, Train Loss=246366.9187687601\n",
            "Iteration 12033, Train Loss=246199.93985740692\n",
            "Iteration 12034, Train Loss=246033.07948941793\n",
            "Iteration 12035, Train Loss=245866.35708201193\n",
            "Iteration 12036, Train Loss=245699.74841555915\n",
            "Iteration 12037, Train Loss=245533.22898919025\n",
            "Iteration 12038, Train Loss=245366.80760584134\n",
            "Iteration 12039, Train Loss=245200.51125855456\n",
            "Iteration 12040, Train Loss=245034.34701512926\n",
            "Iteration 12041, Train Loss=244868.298978078\n",
            "Iteration 12042, Train Loss=244702.35623906986\n",
            "Iteration 12043, Train Loss=244536.52743472572\n",
            "Iteration 12044, Train Loss=244370.8288621907\n",
            "Iteration 12045, Train Loss=244205.26363014904\n",
            "Iteration 12046, Train Loss=244039.82271778537\n",
            "Iteration 12047, Train Loss=243874.50161760906\n",
            "Iteration 12048, Train Loss=243709.30682786478\n",
            "Iteration 12049, Train Loss=243544.24852211648\n",
            "Iteration 12050, Train Loss=243379.3289349569\n",
            "Iteration 12051, Train Loss=243214.54354549822\n",
            "Iteration 12052, Train Loss=243049.8906384158\n",
            "Iteration 12053, Train Loss=242885.3746018259\n",
            "Iteration 12054, Train Loss=242721.00207135262\n",
            "Iteration 12055, Train Loss=242556.7751736688\n",
            "Iteration 12056, Train Loss=242392.6919927275\n",
            "Iteration 12057, Train Loss=242228.7519168008\n",
            "Iteration 12058, Train Loss=242064.95764778033\n",
            "Iteration 12059, Train Loss=241901.31355938409\n",
            "Iteration 12060, Train Loss=241737.82161184782\n",
            "Iteration 12061, Train Loss=241574.48122470017\n",
            "Iteration 12062, Train Loss=241411.29211984295\n",
            "Iteration 12063, Train Loss=241248.2557048835\n",
            "Iteration 12064, Train Loss=241085.3747054013\n",
            "Iteration 12065, Train Loss=240922.65067635514\n",
            "Iteration 12066, Train Loss=240760.0835887366\n",
            "Iteration 12067, Train Loss=240597.67316823575\n",
            "Iteration 12068, Train Loss=240435.41985858456\n",
            "Iteration 12069, Train Loss=240273.32509868205\n",
            "Iteration 12070, Train Loss=240111.3898676346\n",
            "Iteration 12071, Train Loss=239949.61424428382\n",
            "Iteration 12072, Train Loss=239787.99785547424\n",
            "Iteration 12073, Train Loss=239626.54049230175\n",
            "Iteration 12074, Train Loss=239465.2426215969\n",
            "Iteration 12075, Train Loss=239304.1046199602\n",
            "Iteration 12076, Train Loss=239143.1264652134\n",
            "Iteration 12077, Train Loss=238982.3077019862\n",
            "Iteration 12078, Train Loss=238821.64777284386\n",
            "Iteration 12079, Train Loss=238661.1465070323\n",
            "Iteration 12080, Train Loss=238500.80379192898\n",
            "Iteration 12081, Train Loss=238340.61945370314\n",
            "Iteration 12082, Train Loss=238180.59302182728\n",
            "Iteration 12083, Train Loss=238020.72386026444\n",
            "Iteration 12084, Train Loss=237861.01149802413\n",
            "Iteration 12085, Train Loss=237701.45554015785\n",
            "Iteration 12086, Train Loss=237542.05570972935\n",
            "Iteration 12087, Train Loss=237382.81159327878\n",
            "Iteration 12088, Train Loss=237223.72267142264\n",
            "Iteration 12089, Train Loss=237064.788458711\n",
            "Iteration 12090, Train Loss=236906.00851054507\n",
            "Iteration 12091, Train Loss=236747.38254575647\n",
            "Iteration 12092, Train Loss=236588.91026381086\n",
            "Iteration 12093, Train Loss=236430.59135896983\n",
            "Iteration 12094, Train Loss=236272.42551311426\n",
            "Iteration 12095, Train Loss=236114.4124184237\n",
            "Iteration 12096, Train Loss=235956.5518919506\n",
            "Iteration 12097, Train Loss=235798.8437799033\n",
            "Iteration 12098, Train Loss=235641.2879962772\n",
            "Iteration 12099, Train Loss=235483.88444666765\n",
            "Iteration 12100, Train Loss=235326.63304466833\n",
            "Iteration 12101, Train Loss=235169.53376434481\n",
            "Iteration 12102, Train Loss=235012.58660435345\n",
            "Iteration 12103, Train Loss=234855.79164349978\n",
            "Iteration 12104, Train Loss=234699.148962746\n",
            "Iteration 12105, Train Loss=234542.65866439752\n",
            "Iteration 12106, Train Loss=234386.32086063005\n",
            "Iteration 12107, Train Loss=234230.13566601445\n",
            "Iteration 12108, Train Loss=234074.10323846163\n",
            "Iteration 12109, Train Loss=233918.22373021845\n",
            "Iteration 12110, Train Loss=233762.49731615564\n",
            "Iteration 12111, Train Loss=233606.92415195255\n",
            "Iteration 12112, Train Loss=233451.50438043504\n",
            "Iteration 12113, Train Loss=233296.23813807956\n",
            "Iteration 12114, Train Loss=233141.12553494805\n",
            "Iteration 12115, Train Loss=232986.16668106773\n",
            "Iteration 12116, Train Loss=232831.36164886894\n",
            "Iteration 12117, Train Loss=232676.71048963108\n",
            "Iteration 12118, Train Loss=232522.21321377408\n",
            "Iteration 12119, Train Loss=232367.8697904353\n",
            "Iteration 12120, Train Loss=232213.6801567159\n",
            "Iteration 12121, Train Loss=232059.64419919136\n",
            "Iteration 12122, Train Loss=231905.7617725445\n",
            "Iteration 12123, Train Loss=231752.03267623656\n",
            "Iteration 12124, Train Loss=231598.45666631468\n",
            "Iteration 12125, Train Loss=231445.03344698233\n",
            "Iteration 12126, Train Loss=231291.76267001632\n",
            "Iteration 12127, Train Loss=231138.64394388784\n",
            "Iteration 12128, Train Loss=230985.67682205018\n",
            "Iteration 12129, Train Loss=230832.8608179391\n",
            "Iteration 12130, Train Loss=230680.1953921224\n",
            "Iteration 12131, Train Loss=230527.67996292067\n",
            "Iteration 12132, Train Loss=230375.3139033395\n",
            "Iteration 12133, Train Loss=230223.09654300162\n",
            "Iteration 12134, Train Loss=230071.02717609573\n",
            "Iteration 12135, Train Loss=229919.10505586438\n",
            "Iteration 12136, Train Loss=229767.32940698377\n",
            "Iteration 12137, Train Loss=229615.69941889597\n",
            "Iteration 12138, Train Loss=229464.21425576851\n",
            "Iteration 12139, Train Loss=229312.87305516953\n",
            "Iteration 12140, Train Loss=229161.6749320876\n",
            "Iteration 12141, Train Loss=229010.61898448382\n",
            "Iteration 12142, Train Loss=228859.70429171668\n",
            "Iteration 12143, Train Loss=228708.9299237506\n",
            "Iteration 12144, Train Loss=228558.29493751345\n",
            "Iteration 12145, Train Loss=228407.7983853348\n",
            "Iteration 12146, Train Loss=228257.43931336608\n",
            "Iteration 12147, Train Loss=228107.21676630876\n",
            "Iteration 12148, Train Loss=227957.12978981127\n",
            "Iteration 12149, Train Loss=227807.17743094248\n",
            "Iteration 12150, Train Loss=227657.358743608\n",
            "Iteration 12151, Train Loss=227507.6727863593\n",
            "Iteration 12152, Train Loss=227358.1186283677\n",
            "Iteration 12153, Train Loss=227208.69534711028\n",
            "Iteration 12154, Train Loss=227059.4020326343\n",
            "Iteration 12155, Train Loss=226910.23778701824\n",
            "Iteration 12156, Train Loss=226761.20172582573\n",
            "Iteration 12157, Train Loss=226612.29297975\n",
            "Iteration 12158, Train Loss=226463.51069353693\n",
            "Iteration 12159, Train Loss=226314.8540289388\n",
            "Iteration 12160, Train Loss=226166.3221623348\n",
            "Iteration 12161, Train Loss=226017.9142876189\n",
            "Iteration 12162, Train Loss=225869.62961393976\n",
            "Iteration 12163, Train Loss=225721.46736738124\n",
            "Iteration 12164, Train Loss=225573.42678977345\n",
            "Iteration 12165, Train Loss=225425.50713868634\n",
            "Iteration 12166, Train Loss=225277.70768749752\n",
            "Iteration 12167, Train Loss=225130.02772391058\n",
            "Iteration 12168, Train Loss=224982.46655086064\n",
            "Iteration 12169, Train Loss=224835.02348422646\n",
            "Iteration 12170, Train Loss=224687.69785388518\n",
            "Iteration 12171, Train Loss=224540.48900139745\n",
            "Iteration 12172, Train Loss=224393.3962805871\n",
            "Iteration 12173, Train Loss=224246.41905576945\n",
            "Iteration 12174, Train Loss=224099.55670153978\n",
            "Iteration 12175, Train Loss=223952.80860176758\n",
            "Iteration 12176, Train Loss=223806.17414858798\n",
            "Iteration 12177, Train Loss=223659.65274209908\n",
            "Iteration 12178, Train Loss=223513.24378878577\n",
            "Iteration 12179, Train Loss=223366.94670165953\n",
            "Iteration 12180, Train Loss=223220.76089847917\n",
            "Iteration 12181, Train Loss=223074.68580200995\n",
            "Iteration 12182, Train Loss=222928.72083837944\n",
            "Iteration 12183, Train Loss=222782.8654372121\n",
            "Iteration 12184, Train Loss=222637.11903034436\n",
            "Iteration 12185, Train Loss=222491.4810516949\n",
            "Iteration 12186, Train Loss=222345.95093644975\n",
            "Iteration 12187, Train Loss=222200.52812064165\n",
            "Iteration 12188, Train Loss=222055.21204079292\n",
            "Iteration 12189, Train Loss=221910.00213326942\n",
            "Iteration 12190, Train Loss=221764.89783429736\n",
            "Iteration 12191, Train Loss=221619.8985792011\n",
            "Iteration 12192, Train Loss=221475.00380267337\n",
            "Iteration 12193, Train Loss=221330.2129380189\n",
            "Iteration 12194, Train Loss=221185.52541756406\n",
            "Iteration 12195, Train Loss=221040.94067199406\n",
            "Iteration 12196, Train Loss=220896.45813081143\n",
            "Iteration 12197, Train Loss=220752.07722184202\n",
            "Iteration 12198, Train Loss=220607.79737167218\n",
            "Iteration 12199, Train Loss=220463.61800534913\n",
            "Iteration 12200, Train Loss=220319.53854676746\n",
            "Iteration 12201, Train Loss=220175.55841856735\n",
            "Iteration 12202, Train Loss=220031.67704245742\n",
            "Iteration 12203, Train Loss=219887.89383929092\n",
            "Iteration 12204, Train Loss=219744.20822933092\n",
            "Iteration 12205, Train Loss=219600.61963247467\n",
            "Iteration 12206, Train Loss=219457.12746846786\n",
            "Iteration 12207, Train Loss=219313.73115724573\n",
            "Iteration 12208, Train Loss=219170.43011910998\n",
            "Iteration 12209, Train Loss=219027.22377515538\n",
            "Iteration 12210, Train Loss=218884.1115474109\n",
            "Iteration 12211, Train Loss=218741.09285933943\n",
            "Iteration 12212, Train Loss=218598.1671359334\n",
            "Iteration 12213, Train Loss=218455.33380427695\n",
            "Iteration 12214, Train Loss=218312.59229359333\n",
            "Iteration 12215, Train Loss=218169.942035852\n",
            "Iteration 12216, Train Loss=218027.38246577414\n",
            "Iteration 12217, Train Loss=217884.9130214688\n",
            "Iteration 12218, Train Loss=217742.53314438165\n",
            "Iteration 12219, Train Loss=217600.2422799846\n",
            "Iteration 12220, Train Loss=217458.03987762472\n",
            "Iteration 12221, Train Loss=217315.92539129313\n",
            "Iteration 12222, Train Loss=217173.89827934644\n",
            "Iteration 12223, Train Loss=217031.95800538003\n",
            "Iteration 12224, Train Loss=216890.10403777706\n",
            "Iteration 12225, Train Loss=216748.33585073668\n",
            "Iteration 12226, Train Loss=216606.6529235817\n",
            "Iteration 12227, Train Loss=216465.05474202352\n",
            "Iteration 12228, Train Loss=216323.54079712648\n",
            "Iteration 12229, Train Loss=216182.11058692686\n",
            "Iteration 12230, Train Loss=216040.76361490026\n",
            "Iteration 12231, Train Loss=215899.49939213513\n",
            "Iteration 12232, Train Loss=215758.3174350528\n",
            "Iteration 12233, Train Loss=215617.21726844407\n",
            "Iteration 12234, Train Loss=215476.19842205383\n",
            "Iteration 12235, Train Loss=215335.26043498045\n",
            "Iteration 12236, Train Loss=215194.4028505138\n",
            "Iteration 12237, Train Loss=215053.62522268426\n",
            "Iteration 12238, Train Loss=214912.92710838103\n",
            "Iteration 12239, Train Loss=214772.30807737197\n",
            "Iteration 12240, Train Loss=214631.76770009697\n",
            "Iteration 12241, Train Loss=214491.30556341895\n",
            "Iteration 12242, Train Loss=214350.92125147488\n",
            "Iteration 12243, Train Loss=214210.61437112917\n",
            "Iteration 12244, Train Loss=214070.38452168825\n",
            "Iteration 12245, Train Loss=213930.2313373325\n",
            "Iteration 12246, Train Loss=213790.15443930525\n",
            "Iteration 12247, Train Loss=213650.1535095661\n",
            "Iteration 12248, Train Loss=213510.22821710567\n",
            "Iteration 12249, Train Loss=213370.37835333598\n",
            "Iteration 12250, Train Loss=213230.603727546\n",
            "Iteration 12251, Train Loss=213090.90443733527\n",
            "Iteration 12252, Train Loss=212951.28075914047\n",
            "Iteration 12253, Train Loss=212811.7337536631\n",
            "Iteration 12254, Train Loss=212672.26532506067\n",
            "Iteration 12255, Train Loss=212532.87977502943\n",
            "Iteration 12256, Train Loss=212393.58481912492\n",
            "Iteration 12257, Train Loss=212254.3961589629\n",
            "Iteration 12258, Train Loss=212115.34269465395\n",
            "Iteration 12259, Train Loss=211976.48158114945\n",
            "Iteration 12260, Train Loss=211837.92047238903\n",
            "Iteration 12261, Train Loss=211699.87111405804\n",
            "Iteration 12262, Train Loss=211562.73793649153\n",
            "Iteration 12263, Train Loss=211427.31533474763\n",
            "Iteration 12264, Train Loss=211295.1137107824\n",
            "Iteration 12265, Train Loss=211169.04183964067\n",
            "Iteration 12266, Train Loss=211054.29498308143\n",
            "Iteration 12267, Train Loss=210959.76710918365\n",
            "Iteration 12268, Train Loss=210896.65396686332\n",
            "Iteration 12269, Train Loss=210871.56713269503\n",
            "Iteration 12270, Train Loss=210856.05849698806\n",
            "Iteration 12271, Train Loss=210765.82001216483\n",
            "Iteration 12272, Train Loss=210504.21474783766\n",
            "Iteration 12273, Train Loss=210144.419646997\n",
            "Iteration 12274, Train Loss=209899.15276694074\n",
            "Iteration 12275, Train Loss=209847.59244682724\n",
            "Iteration 12276, Train Loss=209833.8413206092\n",
            "Iteration 12277, Train Loss=209676.53543588932\n",
            "Iteration 12278, Train Loss=209407.08386117275\n",
            "Iteration 12279, Train Loss=209213.2721306813\n",
            "Iteration 12280, Train Loss=209148.51619720508\n",
            "Iteration 12281, Train Loss=209070.06780152646\n",
            "Iteration 12282, Train Loss=208879.860955713\n",
            "Iteration 12283, Train Loss=208669.30913554126\n",
            "Iteration 12284, Train Loss=208547.77398045777\n",
            "Iteration 12285, Train Loss=208465.58972733354\n",
            "Iteration 12286, Train Loss=208320.35314488717\n",
            "Iteration 12287, Train Loss=208130.1110400681\n",
            "Iteration 12288, Train Loss=207984.58929605476\n",
            "Iteration 12289, Train Loss=207884.28623705968\n",
            "Iteration 12290, Train Loss=207755.6383231977\n",
            "Iteration 12291, Train Loss=207586.29675777926\n",
            "Iteration 12292, Train Loss=207434.63000632977\n",
            "Iteration 12293, Train Loss=207319.23929848598\n",
            "Iteration 12294, Train Loss=207194.93208079776\n",
            "Iteration 12295, Train Loss=207040.2938732699\n",
            "Iteration 12296, Train Loss=206889.46507809195\n",
            "Iteration 12297, Train Loss=206764.126933409\n",
            "Iteration 12298, Train Loss=206639.28221456878\n",
            "Iteration 12299, Train Loss=206494.18072366624\n",
            "Iteration 12300, Train Loss=206346.3846447898\n",
            "Iteration 12301, Train Loss=206215.0110895881\n",
            "Iteration 12302, Train Loss=206088.18277214046\n",
            "Iteration 12303, Train Loss=205948.88124152398\n",
            "Iteration 12304, Train Loss=205804.50355516418\n",
            "Iteration 12305, Train Loss=205669.76604057016\n",
            "Iteration 12306, Train Loss=205540.6758773865\n",
            "Iteration 12307, Train Loss=205404.78531015417\n",
            "Iteration 12308, Train Loss=205263.5234135597\n",
            "Iteration 12309, Train Loss=205127.17202920647\n",
            "Iteration 12310, Train Loss=204996.0751641408\n",
            "Iteration 12311, Train Loss=204861.98623459175\n",
            "Iteration 12312, Train Loss=204723.33552403218\n",
            "Iteration 12313, Train Loss=204586.5065774573\n",
            "Iteration 12314, Train Loss=204453.82562125477\n",
            "Iteration 12315, Train Loss=204320.53269313573\n",
            "Iteration 12316, Train Loss=204183.91183769525\n",
            "Iteration 12317, Train Loss=204047.3158902925\n",
            "Iteration 12318, Train Loss=203913.54229326994\n",
            "Iteration 12319, Train Loss=203780.42042895508\n",
            "Iteration 12320, Train Loss=203645.27256266514\n",
            "Iteration 12321, Train Loss=203509.30324826334\n",
            "Iteration 12322, Train Loss=203374.9201667733\n",
            "Iteration 12323, Train Loss=203241.65525845037\n",
            "Iteration 12324, Train Loss=203107.46705431436\n",
            "Iteration 12325, Train Loss=202972.2800609329\n",
            "Iteration 12326, Train Loss=202837.71543887537\n",
            "Iteration 12327, Train Loss=202704.21325406554\n",
            "Iteration 12328, Train Loss=202570.5719906293\n",
            "Iteration 12329, Train Loss=202436.1408518235\n",
            "Iteration 12330, Train Loss=202301.72507103725\n",
            "Iteration 12331, Train Loss=202168.05137422067\n",
            "Iteration 12332, Train Loss=202034.66266366577\n",
            "Iteration 12333, Train Loss=201900.85147789784\n",
            "Iteration 12334, Train Loss=201766.78912766342\n",
            "Iteration 12335, Train Loss=201633.09379296962\n",
            "Iteration 12336, Train Loss=201499.802578445\n",
            "Iteration 12337, Train Loss=201366.42879035044\n",
            "Iteration 12338, Train Loss=201232.79779621295\n",
            "Iteration 12339, Train Loss=201099.2458375523\n",
            "Iteration 12340, Train Loss=200966.01840544076\n",
            "Iteration 12341, Train Loss=200832.92185311444\n",
            "Iteration 12342, Train Loss=200699.6956408786\n",
            "Iteration 12343, Train Loss=200566.4102832614\n",
            "Iteration 12344, Train Loss=200433.29729496458\n",
            "Iteration 12345, Train Loss=200300.37939403282\n",
            "Iteration 12346, Train Loss=200167.47796428003\n",
            "Iteration 12347, Train Loss=200034.5107224767\n",
            "Iteration 12348, Train Loss=199901.59209594296\n",
            "Iteration 12349, Train Loss=199768.8289725977\n",
            "Iteration 12350, Train Loss=199636.16936811394\n",
            "Iteration 12351, Train Loss=199503.5075688572\n",
            "Iteration 12352, Train Loss=199370.8437640121\n",
            "Iteration 12353, Train Loss=199238.26336880182\n",
            "Iteration 12354, Train Loss=199105.7994007101\n",
            "Iteration 12355, Train Loss=198973.3965417454\n",
            "Iteration 12356, Train Loss=198841.00554668112\n",
            "Iteration 12357, Train Loss=198708.64952175057\n",
            "Iteration 12358, Train Loss=198576.37809653432\n",
            "Iteration 12359, Train Loss=198444.1934627064\n",
            "Iteration 12360, Train Loss=198312.0565498244\n",
            "Iteration 12361, Train Loss=198179.9483247812\n",
            "Iteration 12362, Train Loss=198047.89092064352\n",
            "Iteration 12363, Train Loss=197915.90989265183\n",
            "Iteration 12364, Train Loss=197783.9988395681\n",
            "Iteration 12365, Train Loss=197652.13401134932\n",
            "Iteration 12366, Train Loss=197520.30927700715\n",
            "Iteration 12367, Train Loss=197388.54015491353\n",
            "Iteration 12368, Train Loss=197256.83924586567\n",
            "Iteration 12369, Train Loss=197125.19997071757\n",
            "Iteration 12370, Train Loss=196993.60866027704\n",
            "Iteration 12371, Train Loss=196862.06364299948\n",
            "Iteration 12372, Train Loss=196730.57451557444\n",
            "Iteration 12373, Train Loss=196599.14765859005\n",
            "Iteration 12374, Train Loss=196467.77842784574\n",
            "Iteration 12375, Train Loss=196336.4589643191\n",
            "Iteration 12376, Train Loss=196205.1887043562\n",
            "Iteration 12377, Train Loss=196073.97319736698\n",
            "Iteration 12378, Train Loss=195942.81596277378\n",
            "Iteration 12379, Train Loss=195811.7142387356\n",
            "Iteration 12380, Train Loss=195680.66339041153\n",
            "Iteration 12381, Train Loss=195549.66290278715\n",
            "Iteration 12382, Train Loss=195418.71580156178\n",
            "Iteration 12383, Train Loss=195287.82423674484\n",
            "Iteration 12384, Train Loss=195156.98679044895\n",
            "Iteration 12385, Train Loss=195026.2006641633\n",
            "Iteration 12386, Train Loss=194895.46519795508\n",
            "Iteration 12387, Train Loss=194764.7818843003\n",
            "Iteration 12388, Train Loss=194634.15211707997\n",
            "Iteration 12389, Train Loss=194503.5753156351\n",
            "Iteration 12390, Train Loss=194373.04979785986\n",
            "Iteration 12391, Train Loss=194242.5748316716\n",
            "Iteration 12392, Train Loss=194112.15098695018\n",
            "Iteration 12393, Train Loss=193981.77914784764\n",
            "Iteration 12394, Train Loss=193851.45919286367\n",
            "Iteration 12395, Train Loss=193721.1901635843\n",
            "Iteration 12396, Train Loss=193590.97137481064\n",
            "Iteration 12397, Train Loss=193460.80286376405\n",
            "Iteration 12398, Train Loss=193330.68512080575\n",
            "Iteration 12399, Train Loss=193200.61822015588\n",
            "Iteration 12400, Train Loss=193070.6016798119\n",
            "Iteration 12401, Train Loss=192940.6349525183\n",
            "Iteration 12402, Train Loss=192810.71781013175\n",
            "Iteration 12403, Train Loss=192680.85043683238\n",
            "Iteration 12404, Train Loss=192551.032930853\n",
            "Iteration 12405, Train Loss=192421.2651016454\n",
            "Iteration 12406, Train Loss=192291.54657911597\n",
            "Iteration 12407, Train Loss=192161.8770580397\n",
            "Iteration 12408, Train Loss=192032.25651242264\n",
            "Iteration 12409, Train Loss=191902.68497297872\n",
            "Iteration 12410, Train Loss=191773.16239225445\n",
            "Iteration 12411, Train Loss=191643.68856271225\n",
            "Iteration 12412, Train Loss=191514.26322205082\n",
            "Iteration 12413, Train Loss=191384.88623874538\n",
            "Iteration 12414, Train Loss=191255.55755648448\n",
            "Iteration 12415, Train Loss=191126.27716372622\n",
            "Iteration 12416, Train Loss=190997.0449624395\n",
            "Iteration 12417, Train Loss=190867.86078553385\n",
            "Iteration 12418, Train Loss=190738.7244887195\n",
            "Iteration 12419, Train Loss=190609.63596355016\n",
            "Iteration 12420, Train Loss=190480.59518062588\n",
            "Iteration 12421, Train Loss=190351.60208989325\n",
            "Iteration 12422, Train Loss=190222.6566141041\n",
            "Iteration 12423, Train Loss=190093.75865295256\n",
            "Iteration 12424, Train Loss=189964.9081029144\n",
            "Iteration 12425, Train Loss=189836.10491596657\n",
            "Iteration 12426, Train Loss=189707.34905201185\n",
            "Iteration 12427, Train Loss=189578.64048870542\n",
            "Iteration 12428, Train Loss=189449.97918226715\n",
            "Iteration 12429, Train Loss=189321.3650771032\n",
            "Iteration 12430, Train Loss=189192.79813634418\n",
            "Iteration 12431, Train Loss=189064.27832941664\n",
            "Iteration 12432, Train Loss=188935.8056599122\n",
            "Iteration 12433, Train Loss=188807.38012621328\n",
            "Iteration 12434, Train Loss=188679.00173090017\n",
            "Iteration 12435, Train Loss=188550.67047564164\n",
            "Iteration 12436, Train Loss=188422.38636228483\n",
            "Iteration 12437, Train Loss=188294.1494169108\n",
            "Iteration 12438, Train Loss=188165.95966849002\n",
            "Iteration 12439, Train Loss=188037.8171659035\n",
            "Iteration 12440, Train Loss=187909.72195638917\n",
            "Iteration 12441, Train Loss=187781.67409231828\n",
            "Iteration 12442, Train Loss=187653.67363548177\n",
            "Iteration 12443, Train Loss=187525.72065124477\n",
            "Iteration 12444, Train Loss=187397.81522542785\n",
            "Iteration 12445, Train Loss=187269.95744644612\n",
            "Iteration 12446, Train Loss=187142.1474175055\n",
            "Iteration 12447, Train Loss=187014.38524533977\n",
            "Iteration 12448, Train Loss=186886.6710436775\n",
            "Iteration 12449, Train Loss=186759.00493828673\n",
            "Iteration 12450, Train Loss=186631.38705977108\n",
            "Iteration 12451, Train Loss=186503.81755565343\n",
            "Iteration 12452, Train Loss=186376.29657750408\n",
            "Iteration 12453, Train Loss=186248.82429002185\n",
            "Iteration 12454, Train Loss=186121.4008641592\n",
            "Iteration 12455, Train Loss=185994.0264790287\n",
            "Iteration 12456, Train Loss=185866.70132543045\n",
            "Iteration 12457, Train Loss=185739.4255999629\n",
            "Iteration 12458, Train Loss=185612.19951361863\n",
            "Iteration 12459, Train Loss=185485.02328261998\n",
            "Iteration 12460, Train Loss=185357.89713545327\n",
            "Iteration 12461, Train Loss=185230.82130755653\n",
            "Iteration 12462, Train Loss=185103.79604313444\n",
            "Iteration 12463, Train Loss=184976.82159668105\n",
            "Iteration 12464, Train Loss=184849.89822912836\n",
            "Iteration 12465, Train Loss=184723.02621359404\n",
            "Iteration 12466, Train Loss=184596.2058288312\n",
            "Iteration 12467, Train Loss=184469.43736482458\n",
            "Iteration 12468, Train Loss=184342.72111802318\n",
            "Iteration 12469, Train Loss=184216.05739368484\n",
            "Iteration 12470, Train Loss=184089.4465054485\n",
            "Iteration 12471, Train Loss=183962.88877357394\n",
            "Iteration 12472, Train Loss=183836.38452812607\n",
            "Iteration 12473, Train Loss=183709.9341045815\n",
            "Iteration 12474, Train Loss=183583.53784806008\n",
            "Iteration 12475, Train Loss=183457.19610904035\n",
            "Iteration 12476, Train Loss=183330.90924616213\n",
            "Iteration 12477, Train Loss=183204.67762424954\n",
            "Iteration 12478, Train Loss=183078.50161443168\n",
            "Iteration 12479, Train Loss=182952.38159495188\n",
            "Iteration 12480, Train Loss=182826.31794886646\n",
            "Iteration 12481, Train Loss=182700.31106660469\n",
            "Iteration 12482, Train Loss=182574.36134262575\n",
            "Iteration 12483, Train Loss=182448.4691780887\n",
            "Iteration 12484, Train Loss=182322.6349781109\n",
            "Iteration 12485, Train Loss=182196.85915320835\n",
            "Iteration 12486, Train Loss=182071.14211818215\n",
            "Iteration 12487, Train Loss=181945.4842918003\n",
            "Iteration 12488, Train Loss=181819.88609737408\n",
            "Iteration 12489, Train Loss=181694.3479610021\n",
            "Iteration 12490, Train Loss=181568.8703131699\n",
            "Iteration 12491, Train Loss=181443.45358640846\n",
            "Iteration 12492, Train Loss=181318.09821698494\n",
            "Iteration 12493, Train Loss=181192.8046428812\n",
            "Iteration 12494, Train Loss=181067.57330481414\n",
            "Iteration 12495, Train Loss=180942.40464512623\n",
            "Iteration 12496, Train Loss=180817.29910778452\n",
            "Iteration 12497, Train Loss=180692.25713831003\n",
            "Iteration 12498, Train Loss=180567.27918283409\n",
            "Iteration 12499, Train Loss=180442.36568880425\n",
            "Iteration 12500, Train Loss=180317.51710349077\n",
            "Iteration 12501, Train Loss=180192.73387500676\n",
            "Iteration 12502, Train Loss=180068.01645074275\n",
            "Iteration 12503, Train Loss=179943.36527826195\n",
            "Iteration 12504, Train Loss=179818.78080404366\n",
            "Iteration 12505, Train Loss=179694.2634739568\n",
            "Iteration 12506, Train Loss=179569.81373252926\n",
            "Iteration 12507, Train Loss=179445.4320228747\n",
            "Iteration 12508, Train Loss=179321.11878652804\n",
            "Iteration 12509, Train Loss=179196.8744628722\n",
            "Iteration 12510, Train Loss=179072.69948941426\n",
            "Iteration 12511, Train Loss=178948.59430088903\n",
            "Iteration 12512, Train Loss=178824.5593297888\n",
            "Iteration 12513, Train Loss=178700.5950053335\n",
            "Iteration 12514, Train Loss=178576.7017540593\n",
            "Iteration 12515, Train Loss=178452.879998851\n",
            "Iteration 12516, Train Loss=178329.13015941932\n",
            "Iteration 12517, Train Loss=178205.45265152608\n",
            "Iteration 12518, Train Loss=178081.84788726422\n",
            "Iteration 12519, Train Loss=177958.31627452807\n",
            "Iteration 12520, Train Loss=177834.8582170677\n",
            "Iteration 12521, Train Loss=177711.47411421494\n",
            "Iteration 12522, Train Loss=177588.16436072136\n",
            "Iteration 12523, Train Loss=177464.9293467123\n",
            "Iteration 12524, Train Loss=177341.76945734848\n",
            "Iteration 12525, Train Loss=177218.68507295937\n",
            "Iteration 12526, Train Loss=177095.67656858574\n",
            "Iteration 12527, Train Loss=176972.74431423174\n",
            "Iteration 12528, Train Loss=176849.88867434132\n",
            "Iteration 12529, Train Loss=176727.11000812618\n",
            "Iteration 12530, Train Loss=176604.4086690127\n",
            "Iteration 12531, Train Loss=176481.78500501002\n",
            "Iteration 12532, Train Loss=176359.2393581559\n",
            "Iteration 12533, Train Loss=176236.77206490774\n",
            "Iteration 12534, Train Loss=176114.383455597\n",
            "Iteration 12535, Train Loss=175992.0738548319\n",
            "Iteration 12536, Train Loss=175869.84358096428\n",
            "Iteration 12537, Train Loss=175747.6929465115\n",
            "Iteration 12538, Train Loss=175625.62225762373\n",
            "Iteration 12539, Train Loss=175503.63181452928\n",
            "Iteration 12540, Train Loss=175381.72191099363\n",
            "Iteration 12541, Train Loss=175259.89283481071\n",
            "Iteration 12542, Train Loss=175138.14486722165\n",
            "Iteration 12543, Train Loss=175016.47828347728\n",
            "Iteration 12544, Train Loss=174894.8933521909\n",
            "Iteration 12545, Train Loss=174773.3903360018\n",
            "Iteration 12546, Train Loss=174651.96949082948\n",
            "Iteration 12547, Train Loss=174530.63106667515\n",
            "Iteration 12548, Train Loss=174409.375306718\n",
            "Iteration 12549, Train Loss=174288.20244833268\n",
            "Iteration 12550, Train Loss=174167.11272193573\n",
            "Iteration 12551, Train Loss=174046.10635232241\n",
            "Iteration 12552, Train Loss=173925.18355714303\n",
            "Iteration 12553, Train Loss=173804.3445487012\n",
            "Iteration 12554, Train Loss=173683.58953187743\n",
            "Iteration 12555, Train Loss=173562.91870662116\n",
            "Iteration 12556, Train Loss=173442.33226503019\n",
            "Iteration 12557, Train Loss=173321.83039489793\n",
            "Iteration 12558, Train Loss=173201.4132755086\n",
            "Iteration 12559, Train Loss=173081.0810827872\n",
            "Iteration 12560, Train Loss=172960.83398311882\n",
            "Iteration 12561, Train Loss=172840.67214100325\n",
            "Iteration 12562, Train Loss=172720.595709777\n",
            "Iteration 12563, Train Loss=172600.6048432387\n",
            "Iteration 12564, Train Loss=172480.69968146604\n",
            "Iteration 12565, Train Loss=172360.88036887202\n",
            "Iteration 12566, Train Loss=172241.1470321519\n",
            "Iteration 12567, Train Loss=172121.49980900227\n",
            "Iteration 12568, Train Loss=172001.93881337\n",
            "Iteration 12569, Train Loss=171882.4641823375\n",
            "Iteration 12570, Train Loss=171763.07602098415\n",
            "Iteration 12571, Train Loss=171643.77448140475\n",
            "Iteration 12572, Train Loss=171524.55967567625\n",
            "Iteration 12573, Train Loss=171405.4318148674\n",
            "Iteration 12574, Train Loss=171286.3910761582\n",
            "Iteration 12575, Train Loss=171167.43786292447\n",
            "Iteration 12576, Train Loss=171048.57262286954\n",
            "Iteration 12577, Train Loss=170929.7963800992\n",
            "Iteration 12578, Train Loss=170811.11057576357\n",
            "Iteration 12579, Train Loss=170692.51829489632\n",
            "Iteration 12580, Train Loss=170574.02454083838\n",
            "Iteration 12581, Train Loss=170455.63948218117\n",
            "Iteration 12582, Train Loss=170337.38096328685\n",
            "Iteration 12583, Train Loss=170219.28430243203\n",
            "Iteration 12584, Train Loss=170101.41433339164\n",
            "Iteration 12585, Train Loss=169983.89817116628\n",
            "Iteration 12586, Train Loss=169866.97526807073\n",
            "Iteration 12587, Train Loss=169751.11398340046\n",
            "Iteration 12588, Train Loss=169637.20466870317\n",
            "Iteration 12589, Train Loss=169526.97234290637\n",
            "Iteration 12590, Train Loss=169423.61667505186\n",
            "Iteration 12591, Train Loss=169332.98670966938\n",
            "Iteration 12592, Train Loss=169264.41472842864\n",
            "Iteration 12593, Train Loss=169230.16467638197\n",
            "Iteration 12594, Train Loss=169233.80099962148\n",
            "Iteration 12595, Train Loss=169244.576394815\n",
            "Iteration 12596, Train Loss=169164.59645964543\n",
            "Iteration 12597, Train Loss=168908.08607659434\n",
            "Iteration 12598, Train Loss=168558.72725017913\n",
            "Iteration 12599, Train Loss=168341.39479026973\n",
            "Iteration 12600, Train Loss=168322.4952109091\n",
            "Iteration 12601, Train Loss=168331.83956577204\n",
            "Iteration 12602, Train Loss=168189.2550733511\n",
            "Iteration 12603, Train Loss=167934.27526455722\n",
            "Iteration 12604, Train Loss=167763.27564984706\n",
            "Iteration 12605, Train Loss=167724.76301629207\n",
            "Iteration 12606, Train Loss=167668.767377956\n",
            "Iteration 12607, Train Loss=167497.5256629589\n",
            "Iteration 12608, Train Loss=167305.46481750472\n",
            "Iteration 12609, Train Loss=167205.7454816356\n",
            "Iteration 12610, Train Loss=167147.79958495032\n",
            "Iteration 12611, Train Loss=167025.05875838426\n",
            "Iteration 12612, Train Loss=166854.4212478663\n",
            "Iteration 12613, Train Loss=166728.2709512413\n",
            "Iteration 12614, Train Loss=166650.6816435915\n",
            "Iteration 12615, Train Loss=166546.42791878077\n",
            "Iteration 12616, Train Loss=166398.39160832416\n",
            "Iteration 12617, Train Loss=166265.72127741697\n",
            "Iteration 12618, Train Loss=166171.6905380434\n",
            "Iteration 12619, Train Loss=166071.62973536566\n",
            "Iteration 12620, Train Loss=165939.76308754124\n",
            "Iteration 12621, Train Loss=165808.53721803805\n",
            "Iteration 12622, Train Loss=165703.74755982758\n",
            "Iteration 12623, Train Loss=165602.61267214478\n",
            "Iteration 12624, Train Loss=165480.90374036823\n",
            "Iteration 12625, Train Loss=165353.64426087122\n",
            "Iteration 12626, Train Loss=165242.59494636656\n",
            "Iteration 12627, Train Loss=165138.70300964033\n",
            "Iteration 12628, Train Loss=165023.03493877663\n",
            "Iteration 12629, Train Loss=164900.06146606055\n",
            "Iteration 12630, Train Loss=164785.8061251294\n",
            "Iteration 12631, Train Loss=164679.04358409494\n",
            "Iteration 12632, Train Loss=164566.63335785182\n",
            "Iteration 12633, Train Loss=164447.4824771198\n",
            "Iteration 12634, Train Loss=164332.00607899128\n",
            "Iteration 12635, Train Loss=164222.80925133446\n",
            "Iteration 12636, Train Loss=164111.91738201023\n",
            "Iteration 12637, Train Loss=163995.831662547\n",
            "Iteration 12638, Train Loss=163880.37045302376\n",
            "Iteration 12639, Train Loss=163769.39874436677\n",
            "Iteration 12640, Train Loss=163658.9442530198\n",
            "Iteration 12641, Train Loss=163545.130702677\n",
            "Iteration 12642, Train Loss=163430.38833758864\n",
            "Iteration 12643, Train Loss=163318.33353881564\n",
            "Iteration 12644, Train Loss=163207.74414686012\n",
            "Iteration 12645, Train Loss=163095.4464333963\n",
            "Iteration 12646, Train Loss=162981.73901883335\n",
            "Iteration 12647, Train Loss=162869.23698524557\n",
            "Iteration 12648, Train Loss=162758.29421972804\n",
            "Iteration 12649, Train Loss=162646.88456151998\n",
            "Iteration 12650, Train Loss=162534.23705989975\n",
            "Iteration 12651, Train Loss=162421.79720116034\n",
            "Iteration 12652, Train Loss=162310.5401478162\n",
            "Iteration 12653, Train Loss=162199.55064865592\n",
            "Iteration 12654, Train Loss=162087.80544303713\n",
            "Iteration 12655, Train Loss=161975.76482809673\n",
            "Iteration 12656, Train Loss=161864.37879172357\n",
            "Iteration 12657, Train Loss=161753.53765068031\n",
            "Iteration 12658, Train Loss=161642.4450917638\n",
            "Iteration 12659, Train Loss=161530.95952157915\n",
            "Iteration 12660, Train Loss=161419.67647224682\n",
            "Iteration 12661, Train Loss=161308.88987697643\n",
            "Iteration 12662, Train Loss=161198.20961649617\n",
            "Iteration 12663, Train Loss=161087.2757270473\n",
            "Iteration 12664, Train Loss=160976.28772980106\n",
            "Iteration 12665, Train Loss=160865.59751066542\n",
            "Iteration 12666, Train Loss=160755.16135863462\n",
            "Iteration 12667, Train Loss=160644.68099382037\n",
            "Iteration 12668, Train Loss=160534.08757346202\n",
            "Iteration 12669, Train Loss=160423.59736276156\n",
            "Iteration 12670, Train Loss=160313.34078326664\n",
            "Iteration 12671, Train Loss=160203.191020435\n",
            "Iteration 12672, Train Loss=160092.99632135156\n",
            "Iteration 12673, Train Loss=159982.80028485376\n",
            "Iteration 12674, Train Loss=159872.7418086834\n",
            "Iteration 12675, Train Loss=159762.83819164\n",
            "Iteration 12676, Train Loss=159652.98418656073\n",
            "Iteration 12677, Train Loss=159543.12377269188\n",
            "Iteration 12678, Train Loss=159433.31736322056\n",
            "Iteration 12679, Train Loss=159323.63499493248\n",
            "Iteration 12680, Train Loss=159214.05638348116\n",
            "Iteration 12681, Train Loss=159104.51604702184\n",
            "Iteration 12682, Train Loss=158995.00198691493\n",
            "Iteration 12683, Train Loss=158885.56026884736\n",
            "Iteration 12684, Train Loss=158776.2209243079\n",
            "Iteration 12685, Train Loss=158666.9596495773\n",
            "Iteration 12686, Train Loss=158557.74053730402\n",
            "Iteration 12687, Train Loss=158448.5663553605\n",
            "Iteration 12688, Train Loss=158339.4660975964\n",
            "Iteration 12689, Train Loss=158230.45115322983\n",
            "Iteration 12690, Train Loss=158121.50303036044\n",
            "Iteration 12691, Train Loss=158012.60282665974\n",
            "Iteration 12692, Train Loss=157903.75582795098\n",
            "Iteration 12693, Train Loss=157794.9788423452\n",
            "Iteration 12694, Train Loss=157686.27589294603\n",
            "Iteration 12695, Train Loss=157577.63491186046\n",
            "Iteration 12696, Train Loss=157469.04566526544\n",
            "Iteration 12697, Train Loss=157360.51217385614\n",
            "Iteration 12698, Train Loss=157252.0438771484\n",
            "Iteration 12699, Train Loss=157143.64231954474\n",
            "Iteration 12700, Train Loss=157035.30015388233\n",
            "Iteration 12701, Train Loss=156927.01138125028\n",
            "Iteration 12702, Train Loss=156818.77821742804\n",
            "Iteration 12703, Train Loss=156710.60589026046\n",
            "Iteration 12704, Train Loss=156602.4952528113\n",
            "Iteration 12705, Train Loss=156494.44200512697\n",
            "Iteration 12706, Train Loss=156386.44228953583\n",
            "Iteration 12707, Train Loss=156278.49690833507\n",
            "Iteration 12708, Train Loss=156170.60866661038\n",
            "Iteration 12709, Train Loss=156062.77824078084\n",
            "Iteration 12710, Train Loss=155955.00320847245\n",
            "Iteration 12711, Train Loss=155847.2809271798\n",
            "Iteration 12712, Train Loss=155739.61129951174\n",
            "Iteration 12713, Train Loss=155631.9956797611\n",
            "Iteration 12714, Train Loss=155524.43463429314\n",
            "Iteration 12715, Train Loss=155416.9268737565\n",
            "Iteration 12716, Train Loss=155309.4705637589\n",
            "Iteration 12717, Train Loss=155202.06513181244\n",
            "Iteration 12718, Train Loss=155094.71103315405\n",
            "Iteration 12719, Train Loss=154987.40867174725\n",
            "Iteration 12720, Train Loss=154880.15741546522\n",
            "Iteration 12721, Train Loss=154772.9560353724\n",
            "Iteration 12722, Train Loss=154665.80379396476\n",
            "Iteration 12723, Train Loss=154558.70060199284\n",
            "Iteration 12724, Train Loss=154451.64665314992\n",
            "Iteration 12725, Train Loss=154344.64165654936\n",
            "Iteration 12726, Train Loss=154237.6848491517\n",
            "Iteration 12727, Train Loss=154130.775535742\n",
            "Iteration 12728, Train Loss=154023.9133448305\n",
            "Iteration 12729, Train Loss=153917.09825993568\n",
            "Iteration 12730, Train Loss=153810.3301225241\n",
            "Iteration 12731, Train Loss=153703.60850505205\n",
            "Iteration 12732, Train Loss=153596.9328692702\n",
            "Iteration 12733, Train Loss=153490.30276621343\n",
            "Iteration 12734, Train Loss=153383.71802475041\n",
            "Iteration 12735, Train Loss=153277.17850000758\n",
            "Iteration 12736, Train Loss=153170.68397295097\n",
            "Iteration 12737, Train Loss=153064.2340946809\n",
            "Iteration 12738, Train Loss=152957.82848425754\n",
            "Iteration 12739, Train Loss=152851.46690990255\n",
            "Iteration 12740, Train Loss=152745.14920559927\n",
            "Iteration 12741, Train Loss=152638.87525762216\n",
            "Iteration 12742, Train Loss=152532.64487862148\n",
            "Iteration 12743, Train Loss=152426.45783103406\n",
            "Iteration 12744, Train Loss=152320.31392039516\n",
            "Iteration 12745, Train Loss=152214.21299100338\n",
            "Iteration 12746, Train Loss=152108.15497944024\n",
            "Iteration 12747, Train Loss=152002.13980939589\n",
            "Iteration 12748, Train Loss=151896.16739551423\n",
            "Iteration 12749, Train Loss=151790.2376463646\n",
            "Iteration 12750, Train Loss=151684.3504771741\n",
            "Iteration 12751, Train Loss=151578.50587388736\n",
            "Iteration 12752, Train Loss=151472.7038377747\n",
            "Iteration 12753, Train Loss=151366.94440618338\n",
            "Iteration 12754, Train Loss=151261.22760875817\n",
            "Iteration 12755, Train Loss=151155.55347792612\n",
            "Iteration 12756, Train Loss=151049.92207923508\n",
            "Iteration 12757, Train Loss=150944.33349333287\n",
            "Iteration 12758, Train Loss=150838.7878516598\n",
            "Iteration 12759, Train Loss=150733.2852915306\n",
            "Iteration 12760, Train Loss=150627.82597132033\n",
            "Iteration 12761, Train Loss=150522.410061132\n",
            "Iteration 12762, Train Loss=150417.03774254824\n",
            "Iteration 12763, Train Loss=150311.70923480202\n",
            "Iteration 12764, Train Loss=150206.42476918484\n",
            "Iteration 12765, Train Loss=150101.18461226375\n",
            "Iteration 12766, Train Loss=149995.98903911444\n",
            "Iteration 12767, Train Loss=149890.83834296154\n",
            "Iteration 12768, Train Loss=149785.73283657964\n",
            "Iteration 12769, Train Loss=149680.6728450216\n",
            "Iteration 12770, Train Loss=149575.65872487627\n",
            "Iteration 12771, Train Loss=149470.6908425818\n",
            "Iteration 12772, Train Loss=149365.76959080977\n",
            "Iteration 12773, Train Loss=149260.89537257917\n",
            "Iteration 12774, Train Loss=149156.0686064723\n",
            "Iteration 12775, Train Loss=149051.28972950048\n",
            "Iteration 12776, Train Loss=148946.55918867124\n",
            "Iteration 12777, Train Loss=148841.87745466913\n",
            "Iteration 12778, Train Loss=148737.2450055784\n",
            "Iteration 12779, Train Loss=148632.66233876796\n",
            "Iteration 12780, Train Loss=148528.12996004536\n",
            "Iteration 12781, Train Loss=148423.6483869006\n",
            "Iteration 12782, Train Loss=148319.21814995157\n",
            "Iteration 12783, Train Loss=148214.8397861667\n",
            "Iteration 12784, Train Loss=148110.51384810588\n",
            "Iteration 12785, Train Loss=148006.2408919909\n",
            "Iteration 12786, Train Loss=147902.0214865937\n",
            "Iteration 12787, Train Loss=147797.8562045615\n",
            "Iteration 12788, Train Loss=147693.74562544125\n",
            "Iteration 12789, Train Loss=147589.69033497974\n",
            "Iteration 12790, Train Loss=147485.69092093984\n",
            "Iteration 12791, Train Loss=147381.74797859992\n",
            "Iteration 12792, Train Loss=147277.86210230913\n",
            "Iteration 12793, Train Loss=147174.03389214692\n",
            "Iteration 12794, Train Loss=147070.2639464789\n",
            "Iteration 12795, Train Loss=146966.5528654309\n",
            "Iteration 12796, Train Loss=146862.90124821808\n",
            "Iteration 12797, Train Loss=146759.30969166648\n",
            "Iteration 12798, Train Loss=146655.77879244718\n",
            "Iteration 12799, Train Loss=146552.30914177743\n",
            "Iteration 12800, Train Loss=146448.90132996396\n",
            "Iteration 12801, Train Loss=146345.55594032008\n",
            "Iteration 12802, Train Loss=146242.2735528786\n",
            "Iteration 12803, Train Loss=146139.05474046833\n",
            "Iteration 12804, Train Loss=146035.90006956962\n",
            "Iteration 12805, Train Loss=145932.81009977456\n",
            "Iteration 12806, Train Loss=145829.785381553\n",
            "Iteration 12807, Train Loss=145726.82645839357\n",
            "Iteration 12808, Train Loss=145623.93386275598\n",
            "Iteration 12809, Train Loss=145521.10811913223\n",
            "Iteration 12810, Train Loss=145418.3497400816\n",
            "Iteration 12811, Train Loss=145315.65922845138\n",
            "Iteration 12812, Train Loss=145213.0370749994\n",
            "Iteration 12813, Train Loss=145110.48375877226\n",
            "Iteration 12814, Train Loss=145007.99974682042\n",
            "Iteration 12815, Train Loss=144905.5854927257\n",
            "Iteration 12816, Train Loss=144803.24143796\n",
            "Iteration 12817, Train Loss=144700.9680093079\n",
            "Iteration 12818, Train Loss=144598.76562090224\n",
            "Iteration 12819, Train Loss=144496.63467158272\n",
            "Iteration 12820, Train Loss=144394.5755466136\n",
            "Iteration 12821, Train Loss=144292.58861583192\n",
            "Iteration 12822, Train Loss=144190.67423441424\n",
            "Iteration 12823, Train Loss=144088.83274221155\n",
            "Iteration 12824, Train Loss=143987.06446340214\n",
            "Iteration 12825, Train Loss=143885.36970694052\n",
            "Iteration 12826, Train Loss=143783.7487653527\n",
            "Iteration 12827, Train Loss=143682.2019158923\n",
            "Iteration 12828, Train Loss=143580.72941894754\n",
            "Iteration 12829, Train Loss=143479.33151940847\n",
            "Iteration 12830, Train Loss=143378.00844516855\n",
            "Iteration 12831, Train Loss=143276.7604082561\n",
            "Iteration 12832, Train Loss=143175.58760378876\n",
            "Iteration 12833, Train Loss=143074.49021061213\n",
            "Iteration 12834, Train Loss=142973.46839086042\n",
            "Iteration 12835, Train Loss=142872.5222900266\n",
            "Iteration 12836, Train Loss=142771.65203711606\n",
            "Iteration 12837, Train Loss=142670.85774423095\n",
            "Iteration 12838, Train Loss=142570.1395071693\n",
            "Iteration 12839, Train Loss=142469.49740469982\n",
            "Iteration 12840, Train Loss=142368.93149940987\n",
            "Iteration 12841, Train Loss=142268.44183686492\n",
            "Iteration 12842, Train Loss=142168.02844651174\n",
            "Iteration 12843, Train Loss=142067.6913408939\n",
            "Iteration 12844, Train Loss=141967.43051646583\n",
            "Iteration 12845, Train Loss=141867.2459529705\n",
            "Iteration 12846, Train Loss=141767.13761408016\n",
            "Iteration 12847, Train Loss=141667.10544699823\n",
            "Iteration 12848, Train Loss=141567.14938288517\n",
            "Iteration 12849, Train Loss=141467.2693366968\n",
            "Iteration 12850, Train Loss=141367.4652074002\n",
            "Iteration 12851, Train Loss=141267.7368780281\n",
            "Iteration 12852, Train Loss=141168.08421571614\n",
            "Iteration 12853, Train Loss=141068.50707193156\n",
            "Iteration 12854, Train Loss=140969.00528237637\n",
            "Iteration 12855, Train Loss=140869.57866734746\n",
            "Iteration 12856, Train Loss=140770.22703154036\n",
            "Iteration 12857, Train Loss=140670.9501645171\n",
            "Iteration 12858, Train Loss=140571.7478404288\n",
            "Iteration 12859, Train Loss=140472.61981857213\n",
            "Iteration 12860, Train Loss=140373.56584305508\n",
            "Iteration 12861, Train Loss=140274.58564341848\n",
            "Iteration 12862, Train Loss=140175.67893426496\n",
            "Iteration 12863, Train Loss=140076.8454159434\n",
            "Iteration 12864, Train Loss=139978.08477412787\n",
            "Iteration 12865, Train Loss=139879.39668058488\n",
            "Iteration 12866, Train Loss=139780.7807926932\n",
            "Iteration 12867, Train Loss=139682.2367543041\n",
            "Iteration 12868, Train Loss=139583.7641951842\n",
            "Iteration 12869, Train Loss=139485.36273200889\n",
            "Iteration 12870, Train Loss=139387.03196767942\n",
            "Iteration 12871, Train Loss=139288.77149250483\n",
            "Iteration 12872, Train Loss=139190.58088333876\n",
            "Iteration 12873, Train Loss=139092.45970502557\n",
            "Iteration 12874, Train Loss=138994.4075092629\n",
            "Iteration 12875, Train Loss=138896.42383643147\n",
            "Iteration 12876, Train Loss=138798.50821404695\n",
            "Iteration 12877, Train Loss=138700.66015914697\n",
            "Iteration 12878, Train Loss=138602.87917612804\n",
            "Iteration 12879, Train Loss=138505.16475994204\n",
            "Iteration 12880, Train Loss=138407.51639301222\n",
            "Iteration 12881, Train Loss=138309.9335496369\n",
            "Iteration 12882, Train Loss=138212.4156914898\n",
            "Iteration 12883, Train Loss=138114.96227386454\n",
            "Iteration 12884, Train Loss=138017.57273898105\n",
            "Iteration 12885, Train Loss=137920.246525064\n",
            "Iteration 12886, Train Loss=137822.98305622375\n",
            "Iteration 12887, Train Loss=137725.78175600586\n",
            "Iteration 12888, Train Loss=137628.6420318042\n",
            "Iteration 12889, Train Loss=137531.56329561723\n",
            "Iteration 12890, Train Loss=137434.5449396906\n",
            "Iteration 12891, Train Loss=137337.5863691051\n",
            "Iteration 12892, Train Loss=137240.6869632292\n",
            "Iteration 12893, Train Loss=137143.846128302\n",
            "Iteration 12894, Train Loss=137047.06323591675\n",
            "Iteration 12895, Train Loss=136950.33771053952\n",
            "Iteration 12896, Train Loss=136853.6689314362\n",
            "Iteration 12897, Train Loss=136757.0563842536\n",
            "Iteration 12898, Train Loss=136660.49950794634\n",
            "Iteration 12899, Train Loss=136563.99797225563\n",
            "Iteration 12900, Train Loss=136467.5514556407\n",
            "Iteration 12901, Train Loss=136371.1601954555\n",
            "Iteration 12902, Train Loss=136274.824738361\n",
            "Iteration 12903, Train Loss=136178.5471584484\n",
            "Iteration 12904, Train Loss=136082.331097862\n",
            "Iteration 12905, Train Loss=135986.1848517699\n",
            "Iteration 12906, Train Loss=135890.12318338233\n",
            "Iteration 12907, Train Loss=135794.17628012542\n",
            "Iteration 12908, Train Loss=135698.39949686345\n",
            "Iteration 12909, Train Loss=135602.9024894143\n",
            "Iteration 12910, Train Loss=135507.89124429453\n",
            "Iteration 12911, Train Loss=135413.7702062934\n",
            "Iteration 12912, Train Loss=135321.3082323543\n",
            "Iteration 12913, Train Loss=135232.00160501135\n",
            "Iteration 12914, Train Loss=135148.6508397324\n",
            "Iteration 12915, Train Loss=135076.46532796885\n",
            "Iteration 12916, Train Loss=135024.15135146322\n",
            "Iteration 12917, Train Loss=135004.42389746997\n",
            "Iteration 12918, Train Loss=135026.36846122838\n",
            "Iteration 12919, Train Loss=135073.86463188595\n",
            "Iteration 12920, Train Loss=135063.7351307556\n",
            "Iteration 12921, Train Loss=134882.38467155947\n",
            "Iteration 12922, Train Loss=134544.32912956888\n",
            "Iteration 12923, Train Loss=134274.5516726584\n",
            "Iteration 12924, Train Loss=134227.61608168806\n",
            "Iteration 12925, Train Loss=134285.20808663947\n",
            "Iteration 12926, Train Loss=134222.63446980738\n",
            "Iteration 12927, Train Loss=133995.26143624386\n",
            "Iteration 12928, Train Loss=133795.49657929182\n",
            "Iteration 12929, Train Loss=133751.73104922814\n",
            "Iteration 12930, Train Loss=133747.05317997345\n",
            "Iteration 12931, Train Loss=133628.27016834155\n",
            "Iteration 12932, Train Loss=133440.52046056517\n",
            "Iteration 12933, Train Loss=133331.00118730846\n",
            "Iteration 12934, Train Loss=133299.48748527677\n",
            "Iteration 12935, Train Loss=133223.48909859447\n",
            "Iteration 12936, Train Loss=133074.37921332673\n",
            "Iteration 12937, Train Loss=132947.91546351457\n",
            "Iteration 12938, Train Loss=132886.68047903982\n",
            "Iteration 12939, Train Loss=132819.28352691134\n",
            "Iteration 12940, Train Loss=132698.1382264301\n",
            "Iteration 12941, Train Loss=132574.11225185968\n",
            "Iteration 12942, Train Loss=132494.04706038968\n",
            "Iteration 12943, Train Loss=132423.3419829476\n",
            "Iteration 12944, Train Loss=132318.77056699502\n",
            "Iteration 12945, Train Loss=132202.1319374239\n",
            "Iteration 12946, Train Loss=132111.84745808737\n",
            "Iteration 12947, Train Loss=132035.30380659955\n",
            "Iteration 12948, Train Loss=131939.58539541042\n",
            "Iteration 12949, Train Loss=131830.40694141082\n",
            "Iteration 12950, Train Loss=131735.21530121248\n",
            "Iteration 12951, Train Loss=131653.13850384095\n",
            "Iteration 12952, Train Loss=131561.82594650766\n",
            "Iteration 12953, Train Loss=131458.7743527452\n",
            "Iteration 12954, Train Loss=131361.74463276906\n",
            "Iteration 12955, Train Loss=131275.29434701707\n",
            "Iteration 12956, Train Loss=131185.74509479597\n",
            "Iteration 12957, Train Loss=131087.3388438955\n",
            "Iteration 12958, Train Loss=130990.20451588265\n",
            "Iteration 12959, Train Loss=130900.60745963198\n",
            "Iteration 12960, Train Loss=130811.37222796874\n",
            "Iteration 12961, Train Loss=130716.2382133406\n",
            "Iteration 12962, Train Loss=130619.92894261508\n",
            "Iteration 12963, Train Loss=130528.30565835591\n",
            "Iteration 12964, Train Loss=130438.61589531004\n",
            "Iteration 12965, Train Loss=130345.62505035682\n",
            "Iteration 12966, Train Loss=130250.5453613083\n",
            "Iteration 12967, Train Loss=130157.83935906556\n",
            "Iteration 12968, Train Loss=130067.41099323504\n",
            "Iteration 12969, Train Loss=129975.64472135447\n",
            "Iteration 12970, Train Loss=129881.86016635616\n",
            "Iteration 12971, Train Loss=129788.81403502173\n",
            "Iteration 12972, Train Loss=129697.66702505229\n",
            "Iteration 12973, Train Loss=129606.45999048406\n",
            "Iteration 12974, Train Loss=129513.8001381732\n",
            "Iteration 12975, Train Loss=129420.93683213885\n",
            "Iteration 12976, Train Loss=129329.29023874905\n",
            "Iteration 12977, Train Loss=129238.21036860762\n",
            "Iteration 12978, Train Loss=129146.3890404151\n",
            "Iteration 12979, Train Loss=129054.00448545923\n",
            "Iteration 12980, Train Loss=128962.16003013015\n",
            "Iteration 12981, Train Loss=128871.01025811318\n",
            "Iteration 12982, Train Loss=128779.71135483833\n",
            "Iteration 12983, Train Loss=128687.90026051558\n",
            "Iteration 12984, Train Loss=128596.14721505107\n",
            "Iteration 12985, Train Loss=128504.91604366756\n",
            "Iteration 12986, Train Loss=128413.88749677976\n",
            "Iteration 12987, Train Loss=128322.58965168128\n",
            "Iteration 12988, Train Loss=128231.13253604574\n",
            "Iteration 12989, Train Loss=128139.92869688128\n",
            "Iteration 12990, Train Loss=128049.02655891776\n",
            "Iteration 12991, Train Loss=127958.11071707206\n",
            "Iteration 12992, Train Loss=127867.03417720296\n",
            "Iteration 12993, Train Loss=127776.00152441312\n",
            "Iteration 12994, Train Loss=127685.20156971311\n",
            "Iteration 12995, Train Loss=127594.53869485615\n",
            "Iteration 12996, Train Loss=127503.82686441569\n",
            "Iteration 12997, Train Loss=127413.0728950972\n",
            "Iteration 12998, Train Loss=127322.43044049053\n",
            "Iteration 12999, Train Loss=127231.95218839249\n",
            "Iteration 13000, Test Loss=46208.759491591125\n",
            "Iteration 13000, Train Loss=127141.53587017341\n",
            "Iteration 13001, Train Loss=127051.09929464819\n",
            "Iteration 13002, Train Loss=126960.69060366915\n",
            "Iteration 13003, Train Loss=126870.39680951303\n",
            "Iteration 13004, Train Loss=126780.21494775142\n",
            "Iteration 13005, Train Loss=126690.07415507881\n",
            "Iteration 13006, Train Loss=126599.9471208611\n",
            "Iteration 13007, Train Loss=126509.87807323807\n",
            "Iteration 13008, Train Loss=126419.90945615957\n",
            "Iteration 13009, Train Loss=126330.02450319877\n",
            "Iteration 13010, Train Loss=126240.18153601894\n",
            "Iteration 13011, Train Loss=126150.37539751575\n",
            "Iteration 13012, Train Loss=126060.63645521563\n",
            "Iteration 13013, Train Loss=125970.98394300215\n",
            "Iteration 13014, Train Loss=125881.40255428967\n",
            "Iteration 13015, Train Loss=125791.86927302662\n",
            "Iteration 13016, Train Loss=125702.38547503014\n",
            "Iteration 13017, Train Loss=125612.96973919687\n",
            "Iteration 13018, Train Loss=125523.63084682003\n",
            "Iteration 13019, Train Loss=125434.35823272448\n",
            "Iteration 13020, Train Loss=125345.13916556192\n",
            "Iteration 13021, Train Loss=125255.97572276245\n",
            "Iteration 13022, Train Loss=125166.87870583143\n",
            "Iteration 13023, Train Loss=125077.85257629857\n",
            "Iteration 13024, Train Loss=124988.89091310585\n",
            "Iteration 13025, Train Loss=124899.98635587159\n",
            "Iteration 13026, Train Loss=124811.14010701496\n",
            "Iteration 13027, Train Loss=124722.35826621344\n",
            "Iteration 13028, Train Loss=124633.64352684603\n",
            "Iteration 13029, Train Loss=124544.99232476747\n",
            "Iteration 13030, Train Loss=124456.4001608383\n",
            "Iteration 13031, Train Loss=124367.86727792227\n",
            "Iteration 13032, Train Loss=124279.39697115756\n",
            "Iteration 13033, Train Loss=124190.99110145301\n",
            "Iteration 13034, Train Loss=124102.6478915986\n",
            "Iteration 13035, Train Loss=124014.36449662938\n",
            "Iteration 13036, Train Loss=123926.14050990224\n",
            "Iteration 13037, Train Loss=123837.9775197707\n",
            "Iteration 13038, Train Loss=123749.87684787172\n",
            "Iteration 13039, Train Loss=123661.83776165354\n",
            "Iteration 13040, Train Loss=123573.85848033603\n",
            "Iteration 13041, Train Loss=123485.93828583955\n",
            "Iteration 13042, Train Loss=123398.07771716497\n",
            "Iteration 13043, Train Loss=123310.27763432704\n",
            "Iteration 13044, Train Loss=123222.53785163851\n",
            "Iteration 13045, Train Loss=123134.85732481061\n",
            "Iteration 13046, Train Loss=123047.2352891651\n",
            "Iteration 13047, Train Loss=122959.67167006098\n",
            "Iteration 13048, Train Loss=122872.16690649041\n",
            "Iteration 13049, Train Loss=122784.72102527905\n",
            "Iteration 13050, Train Loss=122697.33348701146\n",
            "Iteration 13051, Train Loss=122610.00364975902\n",
            "Iteration 13052, Train Loss=122522.7311429793\n",
            "Iteration 13053, Train Loss=122435.51605664473\n",
            "Iteration 13054, Train Loss=122348.35841951105\n",
            "Iteration 13055, Train Loss=122261.2579981689\n",
            "Iteration 13056, Train Loss=122174.21434492998\n",
            "Iteration 13057, Train Loss=122087.22702532276\n",
            "Iteration 13058, Train Loss=122000.2958959071\n",
            "Iteration 13059, Train Loss=121913.42088750497\n",
            "Iteration 13060, Train Loss=121826.60190245009\n",
            "Iteration 13061, Train Loss=121739.83867257732\n",
            "Iteration 13062, Train Loss=121653.13084274027\n",
            "Iteration 13063, Train Loss=121566.47817183565\n",
            "Iteration 13064, Train Loss=121479.88049073164\n",
            "Iteration 13065, Train Loss=121393.33771971308\n",
            "Iteration 13066, Train Loss=121306.84970237061\n",
            "Iteration 13067, Train Loss=121220.41621280144\n",
            "Iteration 13068, Train Loss=121134.03702569744\n",
            "Iteration 13069, Train Loss=121047.7119348865\n",
            "Iteration 13070, Train Loss=120961.44083360342\n",
            "Iteration 13071, Train Loss=120875.22360644086\n",
            "Iteration 13072, Train Loss=120789.06013302591\n",
            "Iteration 13073, Train Loss=120702.9502596757\n",
            "Iteration 13074, Train Loss=120616.89381776526\n",
            "Iteration 13075, Train Loss=120530.8906918186\n",
            "Iteration 13076, Train Loss=120444.94077597761\n",
            "Iteration 13077, Train Loss=120359.04400495361\n",
            "Iteration 13078, Train Loss=120273.20029358643\n",
            "Iteration 13079, Train Loss=120187.40954882554\n",
            "Iteration 13080, Train Loss=120101.67168675926\n",
            "Iteration 13081, Train Loss=120015.98662453322\n",
            "Iteration 13082, Train Loss=119930.35432146167\n",
            "Iteration 13083, Train Loss=119844.77473397196\n",
            "Iteration 13084, Train Loss=119759.24783526329\n",
            "Iteration 13085, Train Loss=119673.77359221065\n",
            "Iteration 13086, Train Loss=119588.35197049598\n",
            "Iteration 13087, Train Loss=119502.98295514463\n",
            "Iteration 13088, Train Loss=119417.66653140471\n",
            "Iteration 13089, Train Loss=119332.40271114747\n",
            "Iteration 13090, Train Loss=119247.1915020467\n",
            "Iteration 13091, Train Loss=119162.03292140753\n",
            "Iteration 13092, Train Loss=119076.92698813076\n",
            "Iteration 13093, Train Loss=118991.87372142135\n",
            "Iteration 13094, Train Loss=118906.87315667688\n",
            "Iteration 13095, Train Loss=118821.92532731974\n",
            "Iteration 13096, Train Loss=118737.03028332265\n",
            "Iteration 13097, Train Loss=118652.18807129418\n",
            "Iteration 13098, Train Loss=118567.39874406606\n",
            "Iteration 13099, Train Loss=118482.66235694713\n",
            "Iteration 13100, Train Loss=118397.97896462232\n",
            "Iteration 13101, Train Loss=118313.34863211102\n",
            "Iteration 13102, Train Loss=118228.77142090746\n",
            "Iteration 13103, Train Loss=118144.24740244236\n",
            "Iteration 13104, Train Loss=118059.77664438428\n",
            "Iteration 13105, Train Loss=117975.3592179891\n",
            "Iteration 13106, Train Loss=117890.99519472229\n",
            "Iteration 13107, Train Loss=117806.68464419572\n",
            "Iteration 13108, Train Loss=117722.4276408725\n",
            "Iteration 13109, Train Loss=117638.22425454343\n",
            "Iteration 13110, Train Loss=117554.07456012964\n",
            "Iteration 13111, Train Loss=117469.97862758003\n",
            "Iteration 13112, Train Loss=117385.93652834259\n",
            "Iteration 13113, Train Loss=117301.94833123342\n",
            "Iteration 13114, Train Loss=117218.01410247092\n",
            "Iteration 13115, Train Loss=117134.13390870931\n",
            "Iteration 13116, Train Loss=117050.30781137936\n",
            "Iteration 13117, Train Loss=116966.53587339554\n",
            "Iteration 13118, Train Loss=116882.81815173035\n",
            "Iteration 13119, Train Loss=116799.15470332089\n",
            "Iteration 13120, Train Loss=116715.54558029659\n",
            "Iteration 13121, Train Loss=116631.99083212235\n",
            "Iteration 13122, Train Loss=116548.49050551793\n",
            "Iteration 13123, Train Loss=116465.04464230435\n",
            "Iteration 13124, Train Loss=116381.65328304672\n",
            "Iteration 13125, Train Loss=116298.31646232207\n",
            "Iteration 13126, Train Loss=116215.03421350979\n",
            "Iteration 13127, Train Loss=116131.80656422356\n",
            "Iteration 13128, Train Loss=116048.63353976033\n",
            "Iteration 13129, Train Loss=115965.51516083484\n",
            "Iteration 13130, Train Loss=115882.45144436261\n",
            "Iteration 13131, Train Loss=115799.44240409593\n",
            "Iteration 13132, Train Loss=115716.48804885425\n",
            "Iteration 13133, Train Loss=115633.58838523051\n",
            "Iteration 13134, Train Loss=115550.74341448126\n",
            "Iteration 13135, Train Loss=115467.9531357972\n",
            "Iteration 13136, Train Loss=115385.21754339403\n",
            "Iteration 13137, Train Loss=115302.53662896887\n",
            "Iteration 13138, Train Loss=115219.91038013215\n",
            "Iteration 13139, Train Loss=115137.33878129085\n",
            "Iteration 13140, Train Loss=115054.8218137883\n",
            "Iteration 13141, Train Loss=114972.35945521209\n",
            "Iteration 13142, Train Loss=114889.95168090364\n",
            "Iteration 13143, Train Loss=114807.59846224538\n",
            "Iteration 13144, Train Loss=114725.2997688163\n",
            "Iteration 13145, Train Loss=114643.05556644333\n",
            "Iteration 13146, Train Loss=114560.86581922832\n",
            "Iteration 13147, Train Loss=114478.73048805648\n",
            "Iteration 13148, Train Loss=114396.64953193846\n",
            "Iteration 13149, Train Loss=114314.6229073687\n",
            "Iteration 13150, Train Loss=114232.65056876349\n",
            "Iteration 13151, Train Loss=114150.73246872188\n",
            "Iteration 13152, Train Loss=114068.86855763635\n",
            "Iteration 13153, Train Loss=113987.0587846566\n",
            "Iteration 13154, Train Loss=113905.30309675718\n",
            "Iteration 13155, Train Loss=113823.60144007063\n",
            "Iteration 13156, Train Loss=113741.95375875197\n",
            "Iteration 13157, Train Loss=113660.35999634743\n",
            "Iteration 13158, Train Loss=113578.82009476409\n",
            "Iteration 13159, Train Loss=113497.33399541308\n",
            "Iteration 13160, Train Loss=113415.90163848382\n",
            "Iteration 13161, Train Loss=113334.52296371709\n",
            "Iteration 13162, Train Loss=113253.19791007834\n",
            "Iteration 13163, Train Loss=113171.92641611588\n",
            "Iteration 13164, Train Loss=113090.70842002533\n",
            "Iteration 13165, Train Loss=113009.54385963865\n",
            "Iteration 13166, Train Loss=112928.43267281288\n",
            "Iteration 13167, Train Loss=112847.37479712928\n",
            "Iteration 13168, Train Loss=112766.3701705165\n",
            "Iteration 13169, Train Loss=112685.41873075414\n",
            "Iteration 13170, Train Loss=112604.52041623548\n",
            "Iteration 13171, Train Loss=112523.6751653713\n",
            "Iteration 13172, Train Loss=112442.88291740502\n",
            "Iteration 13173, Train Loss=112362.1436117858\n",
            "Iteration 13174, Train Loss=112281.45718898196\n",
            "Iteration 13175, Train Loss=112200.82358987202\n",
            "Iteration 13176, Train Loss=112120.24275652017\n",
            "Iteration 13177, Train Loss=112039.71463160419\n",
            "Iteration 13178, Train Loss=111959.23915914842\n",
            "Iteration 13179, Train Loss=111878.81628398827\n",
            "Iteration 13180, Train Loss=111798.44595246375\n",
            "Iteration 13181, Train Loss=111718.1281119166\n",
            "Iteration 13182, Train Loss=111637.86271135687\n",
            "Iteration 13183, Train Loss=111557.6497009721\n",
            "Iteration 13184, Train Loss=111477.48903279073\n",
            "Iteration 13185, Train Loss=111397.38066018288\n",
            "Iteration 13186, Train Loss=111317.32453854836\n",
            "Iteration 13187, Train Loss=111237.3206247704\n",
            "Iteration 13188, Train Loss=111157.36887797019\n",
            "Iteration 13189, Train Loss=111077.46925887899\n",
            "Iteration 13190, Train Loss=110997.62173069935\n",
            "Iteration 13191, Train Loss=110917.82625833887\n",
            "Iteration 13192, Train Loss=110838.08280945294\n",
            "Iteration 13193, Train Loss=110758.39135345732\n",
            "Iteration 13194, Train Loss=110678.75186284294\n",
            "Iteration 13195, Train Loss=110599.1643118652\n",
            "Iteration 13196, Train Loss=110519.62867825897\n",
            "Iteration 13197, Train Loss=110440.14494144097\n",
            "Iteration 13198, Train Loss=110360.71308482994\n",
            "Iteration 13199, Train Loss=110281.3330933162\n",
            "Iteration 13200, Train Loss=110202.00495649083\n",
            "Iteration 13201, Train Loss=110122.72866500448\n",
            "Iteration 13202, Train Loss=110043.50421518541\n",
            "Iteration 13203, Train Loss=109964.33160368595\n",
            "Iteration 13204, Train Loss=109885.21083426697\n",
            "Iteration 13205, Train Loss=109806.14190975628\n",
            "Iteration 13206, Train Loss=109727.12484227176\n",
            "Iteration 13207, Train Loss=109648.1596409174\n",
            "Iteration 13208, Train Loss=109569.24632755213\n",
            "Iteration 13209, Train Loss=109490.38491764129\n",
            "Iteration 13210, Train Loss=109411.5754451884\n",
            "Iteration 13211, Train Loss=109332.81793248346\n",
            "Iteration 13212, Train Loss=109254.11243056311\n",
            "Iteration 13213, Train Loss=109175.45897095658\n",
            "Iteration 13214, Train Loss=109096.8576333391\n",
            "Iteration 13215, Train Loss=109018.30846855948\n",
            "Iteration 13216, Train Loss=108939.81161625563\n",
            "Iteration 13217, Train Loss=108861.36718454356\n",
            "Iteration 13218, Train Loss=108782.97546610571\n",
            "Iteration 13219, Train Loss=108704.63676341204\n",
            "Iteration 13220, Train Loss=108626.3518190884\n",
            "Iteration 13221, Train Loss=108548.12162020833\n",
            "Iteration 13222, Train Loss=108469.94835765015\n",
            "Iteration 13223, Train Loss=108391.83546809052\n",
            "Iteration 13224, Train Loss=108313.79008894689\n",
            "Iteration 13225, Train Loss=108235.82455910268\n",
            "Iteration 13226, Train Loss=108157.96364624398\n",
            "Iteration 13227, Train Loss=108080.25263816254\n",
            "Iteration 13228, Train Loss=108002.78125395204\n",
            "Iteration 13229, Train Loss=107925.71902321954\n",
            "Iteration 13230, Train Loss=107849.40089384033\n",
            "Iteration 13231, Train Loss=107774.4703767316\n",
            "Iteration 13232, Train Loss=107702.19434261574\n",
            "Iteration 13233, Train Loss=107634.99004495738\n",
            "Iteration 13234, Train Loss=107577.47275770006\n",
            "Iteration 13235, Train Loss=107537.76774984012\n",
            "Iteration 13236, Train Loss=107528.96485228495\n",
            "Iteration 13237, Train Loss=107564.95451340561\n",
            "Iteration 13238, Train Loss=107643.6890481039\n",
            "Iteration 13239, Train Loss=107700.48805749443\n",
            "Iteration 13240, Train Loss=107604.21859287897\n",
            "Iteration 13241, Train Loss=107292.88691052534\n",
            "Iteration 13242, Train Loss=106965.62570896777\n",
            "Iteration 13243, Train Loss=106867.86481417654\n",
            "Iteration 13244, Train Loss=106956.408078106\n",
            "Iteration 13245, Train Loss=106971.73337266916\n",
            "Iteration 13246, Train Loss=106779.8535778793\n",
            "Iteration 13247, Train Loss=106554.16413550517\n",
            "Iteration 13248, Train Loss=106497.69490565844\n",
            "Iteration 13249, Train Loss=106532.8748308606\n",
            "Iteration 13250, Train Loss=106459.5432669444\n",
            "Iteration 13251, Train Loss=106278.20305643261\n",
            "Iteration 13252, Train Loss=106163.13670702395\n",
            "Iteration 13253, Train Loss=106153.48269541949\n",
            "Iteration 13254, Train Loss=106112.01450019688\n",
            "Iteration 13255, Train Loss=105979.14920961959\n",
            "Iteration 13256, Train Loss=105858.33961098769\n",
            "Iteration 13257, Train Loss=105815.60300104297\n",
            "Iteration 13258, Train Loss=105774.55851127696\n",
            "Iteration 13259, Train Loss=105670.66801458127\n",
            "Iteration 13260, Train Loss=105558.7620703807\n",
            "Iteration 13261, Train Loss=105497.9408971157\n",
            "Iteration 13262, Train Loss=105449.59962963783\n",
            "Iteration 13263, Train Loss=105361.59520230358\n",
            "Iteration 13264, Train Loss=105259.99810551698\n",
            "Iteration 13265, Train Loss=105189.95618615013\n",
            "Iteration 13266, Train Loss=105134.26392894606\n",
            "Iteration 13267, Train Loss=105054.67557313092\n",
            "Iteration 13268, Train Loss=104961.63771617148\n",
            "Iteration 13269, Train Loss=104887.10395040526\n",
            "Iteration 13270, Train Loss=104825.50848316195\n",
            "Iteration 13271, Train Loss=104750.44642036295\n",
            "Iteration 13272, Train Loss=104663.85935603463\n",
            "Iteration 13273, Train Loss=104587.3480209184\n",
            "Iteration 13274, Train Loss=104521.41234871952\n",
            "Iteration 13275, Train Loss=104448.64162920059\n",
            "Iteration 13276, Train Loss=104366.80410841729\n",
            "Iteration 13277, Train Loss=104289.69987210653\n",
            "Iteration 13278, Train Loss=104220.67256796136\n",
            "Iteration 13279, Train Loss=104148.95424062436\n",
            "Iteration 13280, Train Loss=104070.54161492965\n",
            "Iteration 13281, Train Loss=103993.63775419528\n",
            "Iteration 13282, Train Loss=103922.50410880739\n",
            "Iteration 13283, Train Loss=103851.086281652\n",
            "Iteration 13284, Train Loss=103775.11657701814\n",
            "Iteration 13285, Train Loss=103698.85762289804\n",
            "Iteration 13286, Train Loss=103626.37962117628\n",
            "Iteration 13287, Train Loss=103554.85728400026\n",
            "Iteration 13288, Train Loss=103480.56293300026\n",
            "Iteration 13289, Train Loss=103405.16867331331\n",
            "Iteration 13290, Train Loss=103331.95069780099\n",
            "Iteration 13291, Train Loss=103260.12999221457\n",
            "Iteration 13292, Train Loss=103186.92697783267\n",
            "Iteration 13293, Train Loss=103112.45176397872\n",
            "Iteration 13294, Train Loss=103038.96625399883\n",
            "Iteration 13295, Train Loss=102966.8189381558\n",
            "Iteration 13296, Train Loss=102894.25849475681\n",
            "Iteration 13297, Train Loss=102820.64264331487\n",
            "Iteration 13298, Train Loss=102747.24020233788\n",
            "Iteration 13299, Train Loss=102674.84696928182\n",
            "Iteration 13300, Train Loss=102602.61883297614\n",
            "Iteration 13301, Train Loss=102529.71925258308\n",
            "Iteration 13302, Train Loss=102456.63089835094\n",
            "Iteration 13303, Train Loss=102384.14117254902\n",
            "Iteration 13304, Train Loss=102312.0579250216\n",
            "Iteration 13305, Train Loss=102239.69855547309\n",
            "Iteration 13306, Train Loss=102167.03805947365\n",
            "Iteration 13307, Train Loss=102094.61994625893\n",
            "Iteration 13308, Train Loss=102022.61087581833\n",
            "Iteration 13309, Train Loss=101950.62036706255\n",
            "Iteration 13310, Train Loss=101878.40334542464\n",
            "Iteration 13311, Train Loss=101806.20077701574\n",
            "Iteration 13312, Train Loss=101734.28234436635\n",
            "Iteration 13313, Train Loss=101662.53531835249\n",
            "Iteration 13314, Train Loss=101590.70755189657\n",
            "Iteration 13315, Train Loss=101518.81047671875\n",
            "Iteration 13316, Train Loss=101447.04912299427\n",
            "Iteration 13317, Train Loss=101375.48227540412\n",
            "Iteration 13318, Train Loss=101303.96449973674\n",
            "Iteration 13319, Train Loss=101232.39822752305\n",
            "Iteration 13320, Train Loss=101160.86689422214\n",
            "Iteration 13321, Train Loss=101089.4774851293\n",
            "Iteration 13322, Train Loss=101018.20178411136\n",
            "Iteration 13323, Train Loss=100946.9433355472\n",
            "Iteration 13324, Train Loss=100875.68809119328\n",
            "Iteration 13325, Train Loss=100804.50860047572\n",
            "Iteration 13326, Train Loss=100733.44343862051\n",
            "Iteration 13327, Train Loss=100662.44951954352\n",
            "Iteration 13328, Train Loss=100591.47861341969\n",
            "Iteration 13329, Train Loss=100520.54506183285\n",
            "Iteration 13330, Train Loss=100449.69380078945\n",
            "Iteration 13331, Train Loss=100378.93157429686\n",
            "Iteration 13332, Train Loss=100308.22503539185\n",
            "Iteration 13333, Train Loss=100237.55436578758\n",
            "Iteration 13334, Train Loss=100166.93701622414\n",
            "Iteration 13335, Train Loss=100096.3968254205\n",
            "Iteration 13336, Train Loss=100025.93011970425\n",
            "Iteration 13337, Train Loss=99955.51585830403\n",
            "Iteration 13338, Train Loss=99885.14719026488\n",
            "Iteration 13339, Train Loss=99814.8372176623\n",
            "Iteration 13340, Train Loss=99744.59776866365\n",
            "Iteration 13341, Train Loss=99674.42369826169\n",
            "Iteration 13342, Train Loss=99604.30273194941\n",
            "Iteration 13343, Train Loss=99534.23283793063\n",
            "Iteration 13344, Train Loss=99464.22234636766\n",
            "Iteration 13345, Train Loss=99394.27714887234\n",
            "Iteration 13346, Train Loss=99324.39329467478\n",
            "Iteration 13347, Train Loss=99254.5637188582\n",
            "Iteration 13348, Train Loss=99184.78776266958\n",
            "Iteration 13349, Train Loss=99115.07030423597\n",
            "Iteration 13350, Train Loss=99045.41448359819\n",
            "Iteration 13351, Train Loss=98975.81783102188\n",
            "Iteration 13352, Train Loss=98906.27619432847\n",
            "Iteration 13353, Train Loss=98836.78910745228\n",
            "Iteration 13354, Train Loss=98767.35925294373\n",
            "Iteration 13355, Train Loss=98697.98848168863\n",
            "Iteration 13356, Train Loss=98628.67545517653\n",
            "Iteration 13357, Train Loss=98559.41766489968\n",
            "Iteration 13358, Train Loss=98490.21456109414\n",
            "Iteration 13359, Train Loss=98421.06748648977\n",
            "Iteration 13360, Train Loss=98351.9776146125\n",
            "Iteration 13361, Train Loss=98282.94433997091\n",
            "Iteration 13362, Train Loss=98213.96613701782\n",
            "Iteration 13363, Train Loss=98145.04239161975\n",
            "Iteration 13364, Train Loss=98076.17363984977\n",
            "Iteration 13365, Train Loss=98007.36062256464\n",
            "Iteration 13366, Train Loss=97938.60315334113\n",
            "Iteration 13367, Train Loss=97869.90033723142\n",
            "Iteration 13368, Train Loss=97801.25158299509\n",
            "Iteration 13369, Train Loss=97732.65695507059\n",
            "Iteration 13370, Train Loss=97664.1168688318\n",
            "Iteration 13371, Train Loss=97595.63133224222\n",
            "Iteration 13372, Train Loss=97527.19986642073\n",
            "Iteration 13373, Train Loss=97458.82197949232\n",
            "Iteration 13374, Train Loss=97390.4974892786\n",
            "Iteration 13375, Train Loss=97322.22655480728\n",
            "Iteration 13376, Train Loss=97254.00922706329\n",
            "Iteration 13377, Train Loss=97185.84528655346\n",
            "Iteration 13378, Train Loss=97117.73437868542\n",
            "Iteration 13379, Train Loss=97049.67623272733\n",
            "Iteration 13380, Train Loss=96981.67082212919\n",
            "Iteration 13381, Train Loss=96913.71815231042\n",
            "Iteration 13382, Train Loss=96845.81813622698\n",
            "Iteration 13383, Train Loss=96777.97055017353\n",
            "Iteration 13384, Train Loss=96710.17513774878\n",
            "Iteration 13385, Train Loss=96642.43176580945\n",
            "Iteration 13386, Train Loss=96574.74036326286\n",
            "Iteration 13387, Train Loss=96507.1008782349\n",
            "Iteration 13388, Train Loss=96439.5131762958\n",
            "Iteration 13389, Train Loss=96371.97706336115\n",
            "Iteration 13390, Train Loss=96304.49237302865\n",
            "Iteration 13391, Train Loss=96237.05897357658\n",
            "Iteration 13392, Train Loss=96169.6767912715\n",
            "Iteration 13393, Train Loss=96102.34572603807\n",
            "Iteration 13394, Train Loss=96035.06564463073\n",
            "Iteration 13395, Train Loss=95967.83639485858\n",
            "Iteration 13396, Train Loss=95900.6578236953\n",
            "Iteration 13397, Train Loss=95833.52982234125\n",
            "Iteration 13398, Train Loss=95766.45228386615\n",
            "Iteration 13399, Train Loss=95699.42510475888\n",
            "Iteration 13400, Train Loss=95632.44815830428\n",
            "Iteration 13401, Train Loss=95565.52130320862\n",
            "Iteration 13402, Train Loss=95498.64441181469\n",
            "Iteration 13403, Train Loss=95431.81735885012\n",
            "Iteration 13404, Train Loss=95365.04003911198\n",
            "Iteration 13405, Train Loss=95298.31233660868\n",
            "Iteration 13406, Train Loss=95231.63412952806\n",
            "Iteration 13407, Train Loss=95165.00529047633\n",
            "Iteration 13408, Train Loss=95098.42568710519\n",
            "Iteration 13409, Train Loss=95031.89520097691\n",
            "Iteration 13410, Train Loss=94965.41371022598\n",
            "Iteration 13411, Train Loss=94898.98109967116\n",
            "Iteration 13412, Train Loss=94832.59724596077\n",
            "Iteration 13413, Train Loss=94766.26202140667\n",
            "Iteration 13414, Train Loss=94699.9752995914\n",
            "Iteration 13415, Train Loss=94633.73695035386\n",
            "Iteration 13416, Train Loss=94567.5468518871\n",
            "Iteration 13417, Train Loss=94501.4048773266\n",
            "Iteration 13418, Train Loss=94435.31090198488\n",
            "Iteration 13419, Train Loss=94369.26479653863\n",
            "Iteration 13420, Train Loss=94303.26642855618\n",
            "Iteration 13421, Train Loss=94237.31566748556\n",
            "Iteration 13422, Train Loss=94171.41237891074\n",
            "Iteration 13423, Train Loss=94105.5564328592\n",
            "Iteration 13424, Train Loss=94039.74769475206\n",
            "Iteration 13425, Train Loss=93973.9860307543\n",
            "Iteration 13426, Train Loss=93908.27130398233\n",
            "Iteration 13427, Train Loss=93842.60337503903\n",
            "Iteration 13428, Train Loss=93776.98210537776\n",
            "Iteration 13429, Train Loss=93711.40735279385\n",
            "Iteration 13430, Train Loss=93645.87897728654\n",
            "Iteration 13431, Train Loss=93580.39683491678\n",
            "Iteration 13432, Train Loss=93514.96078197483\n",
            "Iteration 13433, Train Loss=93449.57067211476\n",
            "Iteration 13434, Train Loss=93384.22635697875\n",
            "Iteration 13435, Train Loss=93318.92768794307\n",
            "Iteration 13436, Train Loss=93253.674513214\n",
            "Iteration 13437, Train Loss=93188.46668183501\n",
            "Iteration 13438, Train Loss=93123.30403943213\n",
            "Iteration 13439, Train Loss=93058.18643163957\n",
            "Iteration 13440, Train Loss=92993.11370147261\n",
            "Iteration 13441, Train Loss=92928.08569047434\n",
            "Iteration 13442, Train Loss=92863.1022390483\n",
            "Iteration 13443, Train Loss=92798.16318505854\n",
            "Iteration 13444, Train Loss=92733.26836624934\n",
            "Iteration 13445, Train Loss=92668.41761743673\n",
            "Iteration 13446, Train Loss=92603.61077325282\n",
            "Iteration 13447, Train Loss=92538.8476657299\n",
            "Iteration 13448, Train Loss=92474.12812593099\n",
            "Iteration 13449, Train Loss=92409.45198318004\n",
            "Iteration 13450, Train Loss=92344.8190649793\n",
            "Iteration 13451, Train Loss=92280.22919796125\n",
            "Iteration 13452, Train Loss=92215.68220641914\n",
            "Iteration 13453, Train Loss=92151.1779141554\n",
            "Iteration 13454, Train Loss=92086.71614258655\n",
            "Iteration 13455, Train Loss=92022.29671245956\n",
            "Iteration 13456, Train Loss=91957.91944250544\n",
            "Iteration 13457, Train Loss=91893.58415030327\n",
            "Iteration 13458, Train Loss=91829.29065200433\n",
            "Iteration 13459, Train Loss=91765.03876212427\n",
            "Iteration 13460, Train Loss=91700.82829425942\n",
            "Iteration 13461, Train Loss=91636.65906009199\n",
            "Iteration 13462, Train Loss=91572.53087061408\n",
            "Iteration 13463, Train Loss=91508.44353490134\n",
            "Iteration 13464, Train Loss=91444.39686128059\n",
            "Iteration 13465, Train Loss=91380.39065639656\n",
            "Iteration 13466, Train Loss=91316.42472590775\n",
            "Iteration 13467, Train Loss=91252.49887414763\n",
            "Iteration 13468, Train Loss=91188.61290419273\n",
            "Iteration 13469, Train Loss=91124.76661813068\n",
            "Iteration 13470, Train Loss=91060.95981660443\n",
            "Iteration 13471, Train Loss=90997.19229948768\n",
            "Iteration 13472, Train Loss=90933.46386515477\n",
            "Iteration 13473, Train Loss=90869.77431128299\n",
            "Iteration 13474, Train Loss=90806.12343412815\n",
            "Iteration 13475, Train Loss=90742.51102920428\n",
            "Iteration 13476, Train Loss=90678.9368907769\n",
            "Iteration 13477, Train Loss=90615.40081225435\n",
            "Iteration 13478, Train Loss=90551.90258601031\n",
            "Iteration 13479, Train Loss=90488.44200344096\n",
            "Iteration 13480, Train Loss=90425.01885510187\n",
            "Iteration 13481, Train Loss=90361.63293048297\n",
            "Iteration 13482, Train Loss=90298.28401838188\n",
            "Iteration 13483, Train Loss=90234.97190649806\n",
            "Iteration 13484, Train Loss=90171.6963819204\n",
            "Iteration 13485, Train Loss=90108.45723067207\n",
            "Iteration 13486, Train Loss=90045.25423818549\n",
            "Iteration 13487, Train Loss=89982.08718890483\n",
            "Iteration 13488, Train Loss=89918.9558666716\n",
            "Iteration 13489, Train Loss=89855.86005443832\n",
            "Iteration 13490, Train Loss=89792.79953452526\n",
            "Iteration 13491, Train Loss=89729.7740884688\n",
            "Iteration 13492, Train Loss=89666.78349714354\n",
            "Iteration 13493, Train Loss=89603.82754074196\n",
            "Iteration 13494, Train Loss=89540.905998767\n",
            "Iteration 13495, Train Loss=89478.01865013211\n",
            "Iteration 13496, Train Loss=89415.16527304832\n",
            "Iteration 13497, Train Loss=89352.34564520966\n",
            "Iteration 13498, Train Loss=89289.55954361192\n",
            "Iteration 13499, Train Loss=89226.80674479007\n",
            "Iteration 13500, Train Loss=89164.08702459566\n",
            "Iteration 13501, Train Loss=89101.40015846967\n",
            "Iteration 13502, Train Loss=89038.74592118901\n",
            "Iteration 13503, Train Loss=88976.12408715948\n",
            "Iteration 13504, Train Loss=88913.53443015472\n",
            "Iteration 13505, Train Loss=88850.97672360743\n",
            "Iteration 13506, Train Loss=88788.45074035223\n",
            "Iteration 13507, Train Loss=88725.95625291564\n",
            "Iteration 13508, Train Loss=88663.49303325592\n",
            "Iteration 13509, Train Loss=88601.06085305935\n",
            "Iteration 13510, Train Loss=88538.65948347207\n",
            "Iteration 13511, Train Loss=88476.28869540952\n",
            "Iteration 13512, Train Loss=88413.94825926729\n",
            "Iteration 13513, Train Loss=88351.63794525895\n",
            "Iteration 13514, Train Loss=88289.35752309408\n",
            "Iteration 13515, Train Loss=88227.10676235682\n",
            "Iteration 13516, Train Loss=88164.88543213333\n",
            "Iteration 13517, Train Loss=88102.69330145602\n",
            "Iteration 13518, Train Loss=88040.53013884975\n",
            "Iteration 13519, Train Loss=87978.3957128772\n",
            "Iteration 13520, Train Loss=87916.28979156367\n",
            "Iteration 13521, Train Loss=87854.21214309068\n",
            "Iteration 13522, Train Loss=87792.16253504317\n",
            "Iteration 13523, Train Loss=87730.1407353224\n",
            "Iteration 13524, Train Loss=87668.14651112564\n",
            "Iteration 13525, Train Loss=87606.1796301878\n",
            "Iteration 13526, Train Loss=87544.23985936533\n",
            "Iteration 13527, Train Loss=87482.32696636723\n",
            "Iteration 13528, Train Loss=87420.44071773751\n",
            "Iteration 13529, Train Loss=87358.58088133993\n",
            "Iteration 13530, Train Loss=87296.74722341631\n",
            "Iteration 13531, Train Loss=87234.93951223584\n",
            "Iteration 13532, Train Loss=87173.15751372179\n",
            "Iteration 13533, Train Loss=87111.40099694193\n",
            "Iteration 13534, Train Loss=87049.66972747691\n",
            "Iteration 13535, Train Loss=86987.96347588414\n",
            "Iteration 13536, Train Loss=86926.2820074698\n",
            "Iteration 13537, Train Loss=86864.62509566663\n",
            "Iteration 13538, Train Loss=86802.9925060744\n",
            "Iteration 13539, Train Loss=86741.38401818802\n",
            "Iteration 13540, Train Loss=86679.79940036377\n",
            "Iteration 13541, Train Loss=86618.23844638102\n",
            "Iteration 13542, Train Loss=86556.70093653657\n",
            "Iteration 13543, Train Loss=86495.1867021351\n",
            "Iteration 13544, Train Loss=86433.69556768503\n",
            "Iteration 13545, Train Loss=86372.2274726082\n",
            "Iteration 13546, Train Loss=86310.7823970874\n",
            "Iteration 13547, Train Loss=86249.36061502041\n",
            "Iteration 13548, Train Loss=86187.96264862172\n",
            "Iteration 13549, Train Loss=86126.5898639086\n",
            "Iteration 13550, Train Loss=86065.24468268213\n",
            "Iteration 13551, Train Loss=86003.93219092772\n",
            "Iteration 13552, Train Loss=85942.66157143781\n",
            "Iteration 13553, Train Loss=85881.45103032504\n",
            "Iteration 13554, Train Loss=85820.33429236598\n",
            "Iteration 13555, Train Loss=85759.3772322925\n",
            "Iteration 13556, Train Loss=85698.70448950096\n",
            "Iteration 13557, Train Loss=85638.55934623002\n",
            "Iteration 13558, Train Loss=85579.40842646279\n",
            "Iteration 13559, Train Loss=85522.16276045053\n",
            "Iteration 13560, Train Loss=85468.56709703054\n",
            "Iteration 13561, Train Loss=85421.96648768184\n",
            "Iteration 13562, Train Loss=85388.45407049838\n",
            "Iteration 13563, Train Loss=85378.51766746063\n",
            "Iteration 13564, Train Loss=85406.58014074215\n",
            "Iteration 13565, Train Loss=85483.46428943015\n",
            "Iteration 13566, Train Loss=85584.70605772334\n",
            "Iteration 13567, Train Loss=85612.58967297948\n",
            "Iteration 13568, Train Loss=85432.53642530818\n",
            "Iteration 13569, Train Loss=85088.9669358485\n",
            "Iteration 13570, Train Loss=84848.25241338338\n",
            "Iteration 13571, Train Loss=84865.01631491656\n",
            "Iteration 13572, Train Loss=84976.01105908681\n",
            "Iteration 13573, Train Loss=84926.72434780902\n",
            "Iteration 13574, Train Loss=84703.56327749691\n",
            "Iteration 13575, Train Loss=84542.06342614762\n",
            "Iteration 13576, Train Loss=84558.218209335\n",
            "Iteration 13577, Train Loss=84589.91602293488\n",
            "Iteration 13578, Train Loss=84479.96167110752\n",
            "Iteration 13579, Train Loss=84316.2965797789\n",
            "Iteration 13580, Train Loss=84258.84574268358\n",
            "Iteration 13581, Train Loss=84271.94971881101\n",
            "Iteration 13582, Train Loss=84214.5372975214\n",
            "Iteration 13583, Train Loss=84085.6051187664\n",
            "Iteration 13584, Train Loss=84003.23266676157\n",
            "Iteration 13585, Train Loss=83987.93458705916\n",
            "Iteration 13586, Train Loss=83945.88500665395\n",
            "Iteration 13587, Train Loss=83845.82284234659\n",
            "Iteration 13588, Train Loss=83760.2744698755\n",
            "Iteration 13589, Train Loss=83724.5946780629\n",
            "Iteration 13590, Train Loss=83683.88291722636\n",
            "Iteration 13591, Train Loss=83602.41911347101\n",
            "Iteration 13592, Train Loss=83520.70930632499\n",
            "Iteration 13593, Train Loss=83472.49444333723\n",
            "Iteration 13594, Train Loss=83428.64817643177\n",
            "Iteration 13595, Train Loss=83358.46290034198\n",
            "Iteration 13596, Train Loss=83282.00594631022\n",
            "Iteration 13597, Train Loss=83226.51253706538\n",
            "Iteration 13598, Train Loss=83178.7303896809\n",
            "Iteration 13599, Train Loss=83115.07313045363\n",
            "Iteration 13600, Train Loss=83043.49840074909\n",
            "Iteration 13601, Train Loss=82984.02024841767\n",
            "Iteration 13602, Train Loss=82932.60381739275\n",
            "Iteration 13603, Train Loss=82872.56664181351\n",
            "Iteration 13604, Train Loss=82805.02081930081\n",
            "Iteration 13605, Train Loss=82743.60125556898\n",
            "Iteration 13606, Train Loss=82689.20190735166\n",
            "Iteration 13607, Train Loss=82630.93487398827\n",
            "Iteration 13608, Train Loss=82566.53520825443\n",
            "Iteration 13609, Train Loss=82504.44737322454\n",
            "Iteration 13610, Train Loss=82447.75975626172\n",
            "Iteration 13611, Train Loss=82390.12883209353\n",
            "Iteration 13612, Train Loss=82328.05021048084\n",
            "Iteration 13613, Train Loss=82266.06728638863\n",
            "Iteration 13614, Train Loss=82207.75976012697\n",
            "Iteration 13615, Train Loss=82150.08166186369\n",
            "Iteration 13616, Train Loss=82089.60479912165\n",
            "Iteration 13617, Train Loss=82028.15012917464\n",
            "Iteration 13618, Train Loss=81968.82288171902\n",
            "Iteration 13619, Train Loss=81910.75117034059\n",
            "Iteration 13620, Train Loss=81851.25906039577\n",
            "Iteration 13621, Train Loss=81790.50260909197\n",
            "Iteration 13622, Train Loss=81730.6658338971\n",
            "Iteration 13623, Train Loss=81672.09169892411\n",
            "Iteration 13624, Train Loss=81613.09566853722\n",
            "Iteration 13625, Train Loss=81553.01930107045\n",
            "Iteration 13626, Train Loss=81493.06686030401\n",
            "Iteration 13627, Train Loss=81434.051251895\n",
            "Iteration 13628, Train Loss=81375.20002002423\n",
            "Iteration 13629, Train Loss=81315.66699729822\n",
            "Iteration 13630, Train Loss=81255.85730949802\n",
            "Iteration 13631, Train Loss=81196.55750558712\n",
            "Iteration 13632, Train Loss=81137.64917092798\n",
            "Iteration 13633, Train Loss=81078.46702499171\n",
            "Iteration 13634, Train Loss=81018.92228138118\n",
            "Iteration 13635, Train Loss=80959.52391735304\n",
            "Iteration 13636, Train Loss=80900.49061300085\n",
            "Iteration 13637, Train Loss=80841.47695266185\n",
            "Iteration 13638, Train Loss=80782.20290843862\n",
            "Iteration 13639, Train Loss=80722.86094922625\n",
            "Iteration 13640, Train Loss=80663.73561225235\n",
            "Iteration 13641, Train Loss=80604.76292340491\n",
            "Iteration 13642, Train Loss=80545.69338199736\n",
            "Iteration 13643, Train Loss=80486.49567557665\n",
            "Iteration 13644, Train Loss=80427.36036068981\n",
            "Iteration 13645, Train Loss=80368.3773832818\n",
            "Iteration 13646, Train Loss=80309.42504958986\n",
            "Iteration 13647, Train Loss=80250.388197478\n",
            "Iteration 13648, Train Loss=80191.32285938726\n",
            "Iteration 13649, Train Loss=80132.34256838658\n",
            "Iteration 13650, Train Loss=80073.44439573752\n",
            "Iteration 13651, Train Loss=80014.536137741\n",
            "Iteration 13652, Train Loss=79955.58503100804\n",
            "Iteration 13653, Train Loss=79896.65215523413\n",
            "Iteration 13654, Train Loss=79837.78849986332\n",
            "Iteration 13655, Train Loss=79778.96554557425\n",
            "Iteration 13656, Train Loss=79720.13078004375\n",
            "Iteration 13657, Train Loss=79661.28570621919\n",
            "Iteration 13658, Train Loss=79602.473029624\n",
            "Iteration 13659, Train Loss=79543.71020531165\n",
            "Iteration 13660, Train Loss=79484.97064194253\n",
            "Iteration 13661, Train Loss=79426.22874048099\n",
            "Iteration 13662, Train Loss=79367.49484762349\n",
            "Iteration 13663, Train Loss=79308.7941858201\n",
            "Iteration 13664, Train Loss=79250.13039887587\n",
            "Iteration 13665, Train Loss=79191.48513778397\n",
            "Iteration 13666, Train Loss=79132.84696479174\n",
            "Iteration 13667, Train Loss=79074.22556451608\n",
            "Iteration 13668, Train Loss=79015.63485413787\n",
            "Iteration 13669, Train Loss=78957.074242257\n",
            "Iteration 13670, Train Loss=78898.5325088915\n",
            "Iteration 13671, Train Loss=78840.00463144135\n",
            "Iteration 13672, Train Loss=78781.4974557735\n",
            "Iteration 13673, Train Loss=78723.01863136729\n",
            "Iteration 13674, Train Loss=78664.5669893707\n",
            "Iteration 13675, Train Loss=78606.13602313546\n",
            "Iteration 13676, Train Loss=78547.72333907765\n",
            "Iteration 13677, Train Loss=78489.3332267326\n",
            "Iteration 13678, Train Loss=78430.97005853143\n",
            "Iteration 13679, Train Loss=78372.63311607155\n",
            "Iteration 13680, Train Loss=78314.31871900133\n",
            "Iteration 13681, Train Loss=78256.02549908837\n",
            "Iteration 13682, Train Loss=78197.75594583688\n",
            "Iteration 13683, Train Loss=78139.51269987178\n",
            "Iteration 13684, Train Loss=78081.29557954699\n",
            "Iteration 13685, Train Loss=78023.10255884042\n",
            "Iteration 13686, Train Loss=77964.93270112039\n",
            "Iteration 13687, Train Loss=77906.7873400785\n",
            "Iteration 13688, Train Loss=77848.66813315307\n",
            "Iteration 13689, Train Loss=77790.57527233633\n",
            "Iteration 13690, Train Loss=77732.50771463444\n",
            "Iteration 13691, Train Loss=77674.46477147334\n",
            "Iteration 13692, Train Loss=77616.44707345536\n",
            "Iteration 13693, Train Loss=77558.45565962311\n",
            "Iteration 13694, Train Loss=77500.49091226273\n",
            "Iteration 13695, Train Loss=77442.55238640102\n",
            "Iteration 13696, Train Loss=77384.63960176484\n",
            "Iteration 13697, Train Loss=77326.75279094909\n",
            "Iteration 13698, Train Loss=77268.89257277355\n",
            "Iteration 13699, Train Loss=77211.05937609212\n",
            "Iteration 13700, Train Loss=77153.25311543427\n",
            "Iteration 13701, Train Loss=77095.47351206266\n",
            "Iteration 13702, Train Loss=77037.72060727564\n",
            "Iteration 13703, Train Loss=76979.99473216911\n",
            "Iteration 13704, Train Loss=76922.29626770725\n",
            "Iteration 13705, Train Loss=76864.62532329326\n",
            "Iteration 13706, Train Loss=76806.9818032635\n",
            "Iteration 13707, Train Loss=76749.36569436386\n",
            "Iteration 13708, Train Loss=76691.77714801693\n",
            "Iteration 13709, Train Loss=76634.21645050615\n",
            "Iteration 13710, Train Loss=76576.68378640448\n",
            "Iteration 13711, Train Loss=76519.17920049824\n",
            "Iteration 13712, Train Loss=76461.70270337825\n",
            "Iteration 13713, Train Loss=76404.25435955888\n",
            "Iteration 13714, Train Loss=76346.83435678226\n",
            "Iteration 13715, Train Loss=76289.44287488244\n",
            "Iteration 13716, Train Loss=76232.08003885073\n",
            "Iteration 13717, Train Loss=76174.74591009572\n",
            "Iteration 13718, Train Loss=76117.44053640883\n",
            "Iteration 13719, Train Loss=76060.16403438112\n",
            "Iteration 13720, Train Loss=76002.91654136372\n",
            "Iteration 13721, Train Loss=75945.69820278739\n",
            "Iteration 13722, Train Loss=75888.5091186587\n",
            "Iteration 13723, Train Loss=75831.34935728367\n",
            "Iteration 13724, Train Loss=75774.21900274414\n",
            "Iteration 13725, Train Loss=75717.11815008304\n",
            "Iteration 13726, Train Loss=75660.04692564889\n",
            "Iteration 13727, Train Loss=75603.00543798797\n",
            "Iteration 13728, Train Loss=75545.99377879951\n",
            "Iteration 13729, Train Loss=75489.012027899\n",
            "Iteration 13730, Train Loss=75432.06025921098\n",
            "Iteration 13731, Train Loss=75375.13856944596\n",
            "Iteration 13732, Train Loss=75318.24705181744\n",
            "Iteration 13733, Train Loss=75261.3858033728\n",
            "Iteration 13734, Train Loss=75204.55490634963\n",
            "Iteration 13735, Train Loss=75147.75443279967\n",
            "Iteration 13736, Train Loss=75090.98445940675\n",
            "Iteration 13737, Train Loss=75034.24505939432\n",
            "Iteration 13738, Train Loss=74977.53631748461\n",
            "Iteration 13739, Train Loss=74920.85830997267\n",
            "Iteration 13740, Train Loss=74864.2111105667\n",
            "Iteration 13741, Train Loss=74807.59478713028\n",
            "Iteration 13742, Train Loss=74751.00940173154\n",
            "Iteration 13743, Train Loss=74694.45502240477\n",
            "Iteration 13744, Train Loss=74637.93171219976\n",
            "Iteration 13745, Train Loss=74581.43953866165\n",
            "Iteration 13746, Train Loss=74524.97856256177\n",
            "Iteration 13747, Train Loss=74468.54884179842\n",
            "Iteration 13748, Train Loss=74412.15043254026\n",
            "Iteration 13749, Train Loss=74355.78338642922\n",
            "Iteration 13750, Train Loss=74299.4477589208\n",
            "Iteration 13751, Train Loss=74243.14360040806\n",
            "Iteration 13752, Train Loss=74186.87096293102\n",
            "Iteration 13753, Train Loss=74130.62989385825\n",
            "Iteration 13754, Train Loss=74074.42043813753\n",
            "Iteration 13755, Train Loss=74018.24263991756\n",
            "Iteration 13756, Train Loss=73962.09653940979\n",
            "Iteration 13757, Train Loss=73905.98217875385\n",
            "Iteration 13758, Train Loss=73849.89959568018\n",
            "Iteration 13759, Train Loss=73793.84882845404\n",
            "Iteration 13760, Train Loss=73737.82991181745\n",
            "Iteration 13761, Train Loss=73681.84287853228\n",
            "Iteration 13762, Train Loss=73625.88776036198\n",
            "Iteration 13763, Train Loss=73569.96458571622\n",
            "Iteration 13764, Train Loss=73514.07338365646\n",
            "Iteration 13765, Train Loss=73458.21417950178\n",
            "Iteration 13766, Train Loss=73402.38699859542\n",
            "Iteration 13767, Train Loss=73346.59186323572\n",
            "Iteration 13768, Train Loss=73290.82879415086\n",
            "Iteration 13769, Train Loss=73235.09781061455\n",
            "Iteration 13770, Train Loss=73179.39892914917\n",
            "Iteration 13771, Train Loss=73123.73216602758\n",
            "Iteration 13772, Train Loss=73068.09753432655\n",
            "Iteration 13773, Train Loss=73012.49504681188\n",
            "Iteration 13774, Train Loss=72956.92471340563\n",
            "Iteration 13775, Train Loss=72901.38654280742\n",
            "Iteration 13776, Train Loss=72845.88054183128\n",
            "Iteration 13777, Train Loss=72790.40671512767\n",
            "Iteration 13778, Train Loss=72734.96506638633\n",
            "Iteration 13779, Train Loss=72679.55559660695\n",
            "Iteration 13780, Train Loss=72624.1783061297\n",
            "Iteration 13781, Train Loss=72568.83319262\n",
            "Iteration 13782, Train Loss=72513.5202527309\n",
            "Iteration 13783, Train Loss=72458.23948092472\n",
            "Iteration 13784, Train Loss=72402.99087002287\n",
            "Iteration 13785, Train Loss=72347.77441130807\n",
            "Iteration 13786, Train Loss=72292.59009389432\n",
            "Iteration 13787, Train Loss=72237.43790580524\n",
            "Iteration 13788, Train Loss=72182.31783266936\n",
            "Iteration 13789, Train Loss=72127.22985908319\n",
            "Iteration 13790, Train Loss=72072.17396735582\n",
            "Iteration 13791, Train Loss=72017.15013850699\n",
            "Iteration 13792, Train Loss=71962.15835161394\n",
            "Iteration 13793, Train Loss=71907.19858408412\n",
            "Iteration 13794, Train Loss=71852.270811776\n",
            "Iteration 13795, Train Loss=71797.37500856417\n",
            "Iteration 13796, Train Loss=71742.51114703939\n",
            "Iteration 13797, Train Loss=71687.67919767046\n",
            "Iteration 13798, Train Loss=71632.87912969908\n",
            "Iteration 13799, Train Loss=71578.11091030065\n",
            "Iteration 13800, Train Loss=71523.374505305\n",
            "Iteration 13801, Train Loss=71468.66987866678\n",
            "Iteration 13802, Train Loss=71413.99699279552\n",
            "Iteration 13803, Train Loss=71359.35580846011\n",
            "Iteration 13804, Train Loss=71304.74628469204\n",
            "Iteration 13805, Train Loss=71250.16837908102\n",
            "Iteration 13806, Train Loss=71195.62204735649\n",
            "Iteration 13807, Train Loss=71141.10724391679\n",
            "Iteration 13808, Train Loss=71086.62392127779\n",
            "Iteration 13809, Train Loss=71032.17203063509\n",
            "Iteration 13810, Train Loss=70977.75152137173\n",
            "Iteration 13811, Train Loss=70923.36234149092\n",
            "Iteration 13812, Train Loss=70869.00443731253\n",
            "Iteration 13813, Train Loss=70814.67775368171\n",
            "Iteration 13814, Train Loss=70760.38223390763\n",
            "Iteration 13815, Train Loss=70706.11781974218\n",
            "Iteration 13816, Train Loss=70651.884451527\n",
            "Iteration 13817, Train Loss=70597.68206799976\n",
            "Iteration 13818, Train Loss=70543.5106065794\n",
            "Iteration 13819, Train Loss=70489.3700030772\n",
            "Iteration 13820, Train Loss=70435.26019204155\n",
            "Iteration 13821, Train Loss=70381.18110644816\n",
            "Iteration 13822, Train Loss=70327.13267803626\n",
            "Iteration 13823, Train Loss=70273.11483704341\n",
            "Iteration 13824, Train Loss=70219.12751247596\n",
            "Iteration 13825, Train Loss=70165.17063192728\n",
            "Iteration 13826, Train Loss=70111.24412176176\n",
            "Iteration 13827, Train Loss=70057.34790703017\n",
            "Iteration 13828, Train Loss=70003.48191155857\n",
            "Iteration 13829, Train Loss=69949.64605795895\n",
            "Iteration 13830, Train Loss=69895.84026763488\n",
            "Iteration 13831, Train Loss=69842.06446087288\n",
            "Iteration 13832, Train Loss=69788.31855678163\n",
            "Iteration 13833, Train Loss=69734.60247344233\n",
            "Iteration 13834, Train Loss=69680.91612780365\n",
            "Iteration 13835, Train Loss=69627.25943587648\n",
            "Iteration 13836, Train Loss=69573.63231259622\n",
            "Iteration 13837, Train Loss=69520.03467204321\n",
            "Iteration 13838, Train Loss=69466.4664272982\n",
            "Iteration 13839, Train Loss=69412.92749066865\n",
            "Iteration 13840, Train Loss=69359.41777354272\n",
            "Iteration 13841, Train Loss=69305.93718662394\n",
            "Iteration 13842, Train Loss=69252.48563978328\n",
            "Iteration 13843, Train Loss=69199.06304230176\n",
            "Iteration 13844, Train Loss=69145.66930272062\n",
            "Iteration 13845, Train Loss=69092.3043290891\n",
            "Iteration 13846, Train Loss=69038.96802881692\n",
            "Iteration 13847, Train Loss=68985.6603089268\n",
            "Iteration 13848, Train Loss=68932.38107590338\n",
            "Iteration 13849, Train Loss=68879.13023596397\n",
            "Iteration 13850, Train Loss=68825.90769488562\n",
            "Iteration 13851, Train Loss=68772.71335831036\n",
            "Iteration 13852, Train Loss=68719.54713153825\n",
            "Iteration 13853, Train Loss=68666.40891987819\n",
            "Iteration 13854, Train Loss=68613.29862839315\n",
            "Iteration 13855, Train Loss=68560.2161623172\n",
            "Iteration 13856, Train Loss=68507.16142672228\n",
            "Iteration 13857, Train Loss=68454.13432704002\n",
            "Iteration 13858, Train Loss=68401.13476860836\n",
            "Iteration 13859, Train Loss=68348.16265733918\n",
            "Iteration 13860, Train Loss=68295.21789909869\n",
            "Iteration 13861, Train Loss=68242.30040058373\n",
            "Iteration 13862, Train Loss=68189.41006845026\n",
            "Iteration 13863, Train Loss=68136.54681050542\n",
            "Iteration 13864, Train Loss=68083.71053445456\n",
            "Iteration 13865, Train Loss=68030.90114956573\n",
            "Iteration 13866, Train Loss=67978.11856484483\n",
            "Iteration 13867, Train Loss=67925.36269141838\n",
            "Iteration 13868, Train Loss=67872.63343982091\n",
            "Iteration 13869, Train Loss=67819.93072350707\n",
            "Iteration 13870, Train Loss=67767.25445474505\n",
            "Iteration 13871, Train Loss=67714.60454991806\n",
            "Iteration 13872, Train Loss=67661.98092322265\n",
            "Iteration 13873, Train Loss=67609.38349485745\n",
            "Iteration 13874, Train Loss=67556.81218120742\n",
            "Iteration 13875, Train Loss=67504.26690783056\n",
            "Iteration 13876, Train Loss=67451.7475940008\n",
            "Iteration 13877, Train Loss=67399.25417385834\n",
            "Iteration 13878, Train Loss=67346.78657198248\n",
            "Iteration 13879, Train Loss=67294.3447390066\n",
            "Iteration 13880, Train Loss=67241.92861336353\n",
            "Iteration 13881, Train Loss=67189.538184025\n",
            "Iteration 13882, Train Loss=67137.17343297452\n",
            "Iteration 13883, Train Loss=67084.83445309663\n",
            "Iteration 13884, Train Loss=67032.52137220636\n",
            "Iteration 13885, Train Loss=66980.23459589314\n",
            "Iteration 13886, Train Loss=66927.97475145393\n",
            "Iteration 13887, Train Loss=66875.74325205563\n",
            "Iteration 13888, Train Loss=66823.5424628912\n",
            "Iteration 13889, Train Loss=66771.37720102088\n",
            "Iteration 13890, Train Loss=66719.25598384606\n",
            "Iteration 13891, Train Loss=66667.19555073968\n",
            "Iteration 13892, Train Loss=66615.22662585127\n",
            "Iteration 13893, Train Loss=66563.40896387563\n",
            "Iteration 13894, Train Loss=66511.85504756145\n",
            "Iteration 13895, Train Loss=66460.78357924943\n",
            "Iteration 13896, Train Loss=66410.61276731108\n",
            "Iteration 13897, Train Loss=66362.15595339755\n",
            "Iteration 13898, Train Loss=66316.97112015265\n",
            "Iteration 13899, Train Loss=66278.04090623833\n",
            "Iteration 13900, Train Loss=66250.8588348284\n",
            "Iteration 13901, Train Loss=66245.02022167819\n",
            "Iteration 13902, Train Loss=66274.68634307846\n",
            "Iteration 13903, Train Loss=66353.36972585243\n",
            "Iteration 13904, Train Loss=66469.03643957569\n",
            "Iteration 13905, Train Loss=66541.33380487548\n",
            "Iteration 13906, Train Loss=66427.10959634981\n",
            "Iteration 13907, Train Loss=66105.67542968749\n",
            "Iteration 13908, Train Loss=65813.77562514725\n",
            "Iteration 13909, Train Loss=65777.6762120289\n",
            "Iteration 13910, Train Loss=65906.11146639124\n",
            "Iteration 13911, Train Loss=65926.36593772043\n",
            "Iteration 13912, Train Loss=65737.34763538495\n",
            "Iteration 13913, Train Loss=65540.31307614013\n",
            "Iteration 13914, Train Loss=65525.16218749897\n",
            "Iteration 13915, Train Loss=65588.18621937341\n",
            "Iteration 13916, Train Loss=65529.15523478505\n",
            "Iteration 13917, Train Loss=65367.128582109566\n",
            "Iteration 13918, Train Loss=65280.82263614193\n",
            "Iteration 13919, Train Loss=65300.10388864484\n",
            "Iteration 13920, Train Loss=65282.74611050292\n",
            "Iteration 13921, Train Loss=65171.589029871044\n",
            "Iteration 13922, Train Loss=65073.770430255056\n",
            "Iteration 13923, Train Loss=65056.29194949016\n",
            "Iteration 13924, Train Loss=65042.03811513215\n",
            "Iteration 13925, Train Loss=64963.49661841473\n",
            "Iteration 13926, Train Loss=64873.44451891795\n",
            "Iteration 13927, Train Loss=64835.037920037925\n",
            "Iteration 13928, Train Loss=64813.426818574626\n",
            "Iteration 13929, Train Loss=64752.51783621909\n",
            "Iteration 13930, Train Loss=64673.43708796931\n",
            "Iteration 13931, Train Loss=64624.54812659259\n",
            "Iteration 13932, Train Loss=64594.452536632445\n",
            "Iteration 13933, Train Loss=64542.5009504025\n",
            "Iteration 13934, Train Loss=64472.95658957595\n",
            "Iteration 13935, Train Loss=64419.37342329521\n",
            "Iteration 13936, Train Loss=64382.23566514248\n",
            "Iteration 13937, Train Loss=64334.42147198405\n",
            "Iteration 13938, Train Loss=64272.185310260276\n",
            "Iteration 13939, Train Loss=64216.99940165852\n",
            "Iteration 13940, Train Loss=64174.592994969\n",
            "Iteration 13941, Train Loss=64128.32722737836\n",
            "Iteration 13942, Train Loss=64071.35542045361\n",
            "Iteration 13943, Train Loss=64016.20223718232\n",
            "Iteration 13944, Train Loss=63970.10755030097\n",
            "Iteration 13945, Train Loss=63923.97543846841\n",
            "Iteration 13946, Train Loss=63870.63443300326\n",
            "Iteration 13947, Train Loss=63816.33926842783\n",
            "Iteration 13948, Train Loss=63767.83779567685\n",
            "Iteration 13949, Train Loss=63721.136314810254\n",
            "Iteration 13950, Train Loss=63670.1405057072\n",
            "Iteration 13951, Train Loss=63617.05280574693\n",
            "Iteration 13952, Train Loss=63567.15690306006\n",
            "Iteration 13953, Train Loss=63519.60457432489\n",
            "Iteration 13954, Train Loss=63469.98124631182\n",
            "Iteration 13955, Train Loss=63418.14396756715\n",
            "Iteration 13956, Train Loss=63367.62394288911\n",
            "Iteration 13957, Train Loss=63319.22221826733\n",
            "Iteration 13958, Train Loss=63270.25372987895\n",
            "Iteration 13959, Train Loss=63219.51892961065\n",
            "Iteration 13960, Train Loss=63168.92249572873\n",
            "Iteration 13961, Train Loss=63119.84385639227\n",
            "Iteration 13962, Train Loss=63071.05334350894\n",
            "Iteration 13963, Train Loss=63021.158684717746\n",
            "Iteration 13964, Train Loss=62970.82581242114\n",
            "Iteration 13965, Train Loss=62921.331661377786\n",
            "Iteration 13966, Train Loss=62872.45195510256\n",
            "Iteration 13967, Train Loss=62823.10228427828\n",
            "Iteration 13968, Train Loss=62773.184532424064\n",
            "Iteration 13969, Train Loss=62723.54665964042\n",
            "Iteration 13970, Train Loss=62674.49187522787\n",
            "Iteration 13971, Train Loss=62625.42032952471\n",
            "Iteration 13972, Train Loss=62575.92150700076\n",
            "Iteration 13973, Train Loss=62526.36002723275\n",
            "Iteration 13974, Train Loss=62477.172423473865\n",
            "Iteration 13975, Train Loss=62428.19342142402\n",
            "Iteration 13976, Train Loss=62379.02179285367\n",
            "Iteration 13977, Train Loss=62329.667842875686\n",
            "Iteration 13978, Train Loss=62280.45637152302\n",
            "Iteration 13979, Train Loss=62231.483094064904\n",
            "Iteration 13980, Train Loss=62182.51759269407\n",
            "Iteration 13981, Train Loss=62133.40633305863\n",
            "Iteration 13982, Train Loss=62084.283549309475\n",
            "Iteration 13983, Train Loss=62035.31783151741\n",
            "Iteration 13984, Train Loss=61986.46030485451\n",
            "Iteration 13985, Train Loss=61937.55817584362\n",
            "Iteration 13986, Train Loss=61888.59523752234\n",
            "Iteration 13987, Train Loss=61839.68893672522\n",
            "Iteration 13988, Train Loss=61790.89550376796\n",
            "Iteration 13989, Train Loss=61742.14199123959\n",
            "Iteration 13990, Train Loss=61693.35538958215\n",
            "Iteration 13991, Train Loss=61644.56541040072\n",
            "Iteration 13992, Train Loss=61595.84296627537\n",
            "Iteration 13993, Train Loss=61547.192426231166\n",
            "Iteration 13994, Train Loss=61498.5586805647\n",
            "Iteration 13995, Train Loss=61449.91563173054\n",
            "Iteration 13996, Train Loss=61401.29622456868\n",
            "Iteration 13997, Train Loss=61352.73586617336\n",
            "Iteration 13998, Train Loss=61304.22326932671\n",
            "Iteration 13999, Train Loss=61255.725053737566\n",
            "Iteration 14000, Test Loss=7019.960628969284\n",
            "Iteration 14000, Train Loss=61207.235686153945\n",
            "Iteration 14001, Train Loss=61158.77919969368\n",
            "Iteration 14002, Train Loss=61110.37133351927\n",
            "Iteration 14003, Train Loss=61062.00002115422\n",
            "Iteration 14004, Train Loss=61013.646935951525\n",
            "Iteration 14005, Train Loss=60965.31327627522\n",
            "Iteration 14006, Train Loss=60917.014121648754\n",
            "Iteration 14007, Train Loss=60868.75611182483\n",
            "Iteration 14008, Train Loss=60820.530403062745\n",
            "Iteration 14009, Train Loss=60772.327204276444\n",
            "Iteration 14010, Train Loss=60724.14884899075\n",
            "Iteration 14011, Train Loss=60676.00423084108\n",
            "Iteration 14012, Train Loss=60627.896284713155\n",
            "Iteration 14013, Train Loss=60579.81941279102\n",
            "Iteration 14014, Train Loss=60531.76824846909\n",
            "Iteration 14015, Train Loss=60483.74455822995\n",
            "Iteration 14016, Train Loss=60435.75348499384\n",
            "Iteration 14017, Train Loss=60387.79659309909\n",
            "Iteration 14018, Train Loss=60339.87062797713\n",
            "Iteration 14019, Train Loss=60291.97247441361\n",
            "Iteration 14020, Train Loss=60244.103097221174\n",
            "Iteration 14021, Train Loss=60196.26545186881\n",
            "Iteration 14022, Train Loss=60148.46060562912\n",
            "Iteration 14023, Train Loss=60100.686811954736\n",
            "Iteration 14024, Train Loss=60052.942157939615\n",
            "Iteration 14025, Train Loss=60005.22699181774\n",
            "Iteration 14026, Train Loss=59957.54297850657\n",
            "Iteration 14027, Train Loss=59909.89095813455\n",
            "Iteration 14028, Train Loss=59862.27010500608\n",
            "Iteration 14029, Train Loss=59814.67921967767\n",
            "Iteration 14030, Train Loss=59767.1182746507\n",
            "Iteration 14031, Train Loss=59719.58814729364\n",
            "Iteration 14032, Train Loss=59672.08950442652\n",
            "Iteration 14033, Train Loss=59624.62205698832\n",
            "Iteration 14034, Train Loss=59577.18508043735\n",
            "Iteration 14035, Train Loss=59529.778369998836\n",
            "Iteration 14036, Train Loss=59482.40231373233\n",
            "Iteration 14037, Train Loss=59435.05740008324\n",
            "Iteration 14038, Train Loss=59387.74362374666\n",
            "Iteration 14039, Train Loss=59340.4605968391\n",
            "Iteration 14040, Train Loss=59293.20807724677\n",
            "Iteration 14041, Train Loss=59245.98616093614\n",
            "Iteration 14042, Train Loss=59198.79515537815\n",
            "Iteration 14043, Train Loss=59151.63516946228\n",
            "Iteration 14044, Train Loss=59104.506047316034\n",
            "Iteration 14045, Train Loss=59057.40759373717\n",
            "Iteration 14046, Train Loss=59010.33975577269\n",
            "Iteration 14047, Train Loss=58963.302680435125\n",
            "Iteration 14048, Train Loss=58916.29648246641\n",
            "Iteration 14049, Train Loss=58869.32114402372\n",
            "Iteration 14050, Train Loss=58822.37655026051\n",
            "Iteration 14051, Train Loss=58775.46260286262\n",
            "Iteration 14052, Train Loss=58728.57933266461\n",
            "Iteration 14053, Train Loss=58681.72680475464\n",
            "Iteration 14054, Train Loss=58634.9050571407\n",
            "Iteration 14055, Train Loss=58588.11404691668\n",
            "Iteration 14056, Train Loss=58541.35369345512\n",
            "Iteration 14057, Train Loss=58494.62396702836\n",
            "Iteration 14058, Train Loss=58447.924873420576\n",
            "Iteration 14059, Train Loss=58401.25644762391\n",
            "Iteration 14060, Train Loss=58354.61868524188\n",
            "Iteration 14061, Train Loss=58308.01154483855\n",
            "Iteration 14062, Train Loss=58261.434983736035\n",
            "Iteration 14063, Train Loss=58214.8889699321\n",
            "Iteration 14064, Train Loss=58168.373509248624\n",
            "Iteration 14065, Train Loss=58121.88860012698\n",
            "Iteration 14066, Train Loss=58075.434229422404\n",
            "Iteration 14067, Train Loss=58029.01036560272\n",
            "Iteration 14068, Train Loss=57982.61696875221\n",
            "Iteration 14069, Train Loss=57936.254018942964\n",
            "Iteration 14070, Train Loss=57889.92149874343\n",
            "Iteration 14071, Train Loss=57843.61940015949\n",
            "Iteration 14072, Train Loss=57797.347702067855\n",
            "Iteration 14073, Train Loss=57751.10637404381\n",
            "Iteration 14074, Train Loss=57704.89538669291\n",
            "Iteration 14075, Train Loss=57658.71470919659\n",
            "Iteration 14076, Train Loss=57612.564323813865\n",
            "Iteration 14077, Train Loss=57566.44420818647\n",
            "Iteration 14078, Train Loss=57520.354340163896\n",
            "Iteration 14079, Train Loss=57474.29469146471\n",
            "Iteration 14080, Train Loss=57428.2652294218\n",
            "Iteration 14081, Train Loss=57382.265926695705\n",
            "Iteration 14082, Train Loss=57336.296753702496\n",
            "Iteration 14083, Train Loss=57290.35768714949\n",
            "Iteration 14084, Train Loss=57244.448699037384\n",
            "Iteration 14085, Train Loss=57198.56976038757\n",
            "Iteration 14086, Train Loss=57152.72084062108\n",
            "Iteration 14087, Train Loss=57106.90190652986\n",
            "Iteration 14088, Train Loss=57061.112929231764\n",
            "Iteration 14089, Train Loss=57015.353877118614\n",
            "Iteration 14090, Train Loss=56969.62472174462\n",
            "Iteration 14091, Train Loss=56923.92543152912\n",
            "Iteration 14092, Train Loss=56878.25597418286\n",
            "Iteration 14093, Train Loss=56832.616317307766\n",
            "Iteration 14094, Train Loss=56787.006426507854\n",
            "Iteration 14095, Train Loss=56741.42627023028\n",
            "Iteration 14096, Train Loss=56695.8758145289\n",
            "Iteration 14097, Train Loss=56650.35502737408\n",
            "Iteration 14098, Train Loss=56604.863874728166\n",
            "Iteration 14099, Train Loss=56559.402322213085\n",
            "Iteration 14100, Train Loss=56513.97033553904\n",
            "Iteration 14101, Train Loss=56468.56787891783\n",
            "Iteration 14102, Train Loss=56423.19491834236\n",
            "Iteration 14103, Train Loss=56377.85141794777\n",
            "Iteration 14104, Train Loss=56332.5373432555\n",
            "Iteration 14105, Train Loss=56287.252658367004\n",
            "Iteration 14106, Train Loss=56241.997327399535\n",
            "Iteration 14107, Train Loss=56196.771314384954\n",
            "Iteration 14108, Train Loss=56151.574582384215\n",
            "Iteration 14109, Train Loss=56106.40709552113\n",
            "Iteration 14110, Train Loss=56061.26881656989\n",
            "Iteration 14111, Train Loss=56016.15970939922\n",
            "Iteration 14112, Train Loss=55971.07973677658\n",
            "Iteration 14113, Train Loss=55926.02886180512\n",
            "Iteration 14114, Train Loss=55881.00704729378\n",
            "Iteration 14115, Train Loss=55836.01425561039\n",
            "Iteration 14116, Train Loss=55791.05044966559\n",
            "Iteration 14117, Train Loss=55746.11559150669\n",
            "Iteration 14118, Train Loss=55701.20964402049\n",
            "Iteration 14119, Train Loss=55656.33256925161\n",
            "Iteration 14120, Train Loss=55611.48432980278\n",
            "Iteration 14121, Train Loss=55566.66488785269\n",
            "Iteration 14122, Train Loss=55521.87420562727\n",
            "Iteration 14123, Train Loss=55477.112245504846\n",
            "Iteration 14124, Train Loss=55432.37896949267\n",
            "Iteration 14125, Train Loss=55387.67434015837\n",
            "Iteration 14126, Train Loss=55342.998319534745\n",
            "Iteration 14127, Train Loss=55298.35087027319\n",
            "Iteration 14128, Train Loss=55253.73195461459\n",
            "Iteration 14129, Train Loss=55209.141535207375\n",
            "Iteration 14130, Train Loss=55164.579574606185\n",
            "Iteration 14131, Train Loss=55120.04603547101\n",
            "Iteration 14132, Train Loss=55075.54088072376\n",
            "Iteration 14133, Train Loss=55031.064073152724\n",
            "Iteration 14134, Train Loss=54986.61557604211\n",
            "Iteration 14135, Train Loss=54942.19535246196\n",
            "Iteration 14136, Train Loss=54897.803366023196\n",
            "Iteration 14137, Train Loss=54853.43958020456\n",
            "Iteration 14138, Train Loss=54809.103958924956\n",
            "Iteration 14139, Train Loss=54764.79646616043\n",
            "Iteration 14140, Train Loss=54720.51706616392\n",
            "Iteration 14141, Train Loss=54676.2657234633\n",
            "Iteration 14142, Train Loss=54632.04240271569\n",
            "Iteration 14143, Train Loss=54587.847069021605\n",
            "Iteration 14144, Train Loss=54543.6796875345\n",
            "Iteration 14145, Train Loss=54499.54022393092\n",
            "Iteration 14146, Train Loss=54455.42864394933\n",
            "Iteration 14147, Train Loss=54411.34491384491\n",
            "Iteration 14148, Train Loss=54367.28900001501\n",
            "Iteration 14149, Train Loss=54323.260869308746\n",
            "Iteration 14150, Train Loss=54279.26048883487\n",
            "Iteration 14151, Train Loss=54235.287826069034\n",
            "Iteration 14152, Train Loss=54191.34284886473\n",
            "Iteration 14153, Train Loss=54147.42552536856\n",
            "Iteration 14154, Train Loss=54103.53582419641\n",
            "Iteration 14155, Train Loss=54059.67371421532\n",
            "Iteration 14156, Train Loss=54015.839164812816\n",
            "Iteration 14157, Train Loss=53972.03214561988\n",
            "Iteration 14158, Train Loss=53928.25262680145\n",
            "Iteration 14159, Train Loss=53884.50057879133\n",
            "Iteration 14160, Train Loss=53840.77597253661\n",
            "Iteration 14161, Train Loss=53797.07877929974\n",
            "Iteration 14162, Train Loss=53753.40897081754\n",
            "Iteration 14163, Train Loss=53709.76651919606\n",
            "Iteration 14164, Train Loss=53666.15139697339\n",
            "Iteration 14165, Train Loss=53622.56357710331\n",
            "Iteration 14166, Train Loss=53579.00303293219\n",
            "Iteration 14167, Train Loss=53535.469738259\n",
            "Iteration 14168, Train Loss=53491.96366724452\n",
            "Iteration 14169, Train Loss=53448.48479452366\n",
            "Iteration 14170, Train Loss=53405.0330950716\n",
            "Iteration 14171, Train Loss=53361.608544345734\n",
            "Iteration 14172, Train Loss=53318.21111812916\n",
            "Iteration 14173, Train Loss=53274.84079268489\n",
            "Iteration 14174, Train Loss=53231.49754459174\n",
            "Iteration 14175, Train Loss=53188.181350896855\n",
            "Iteration 14176, Train Loss=53144.89218895689\n",
            "Iteration 14177, Train Loss=53101.63003657807\n",
            "Iteration 14178, Train Loss=53058.394871871125\n",
            "Iteration 14179, Train Loss=53015.18667337351\n",
            "Iteration 14180, Train Loss=52972.00541991921\n",
            "Iteration 14181, Train Loss=52928.85109074555\n",
            "Iteration 14182, Train Loss=52885.723665376\n",
            "Iteration 14183, Train Loss=52842.62312371166\n",
            "Iteration 14184, Train Loss=52799.549445926445\n",
            "Iteration 14185, Train Loss=52756.50261254727\n",
            "Iteration 14186, Train Loss=52713.48260435437\n",
            "Iteration 14187, Train Loss=52670.48940245949\n",
            "Iteration 14188, Train Loss=52627.522988203404\n",
            "Iteration 14189, Train Loss=52584.58334323545\n",
            "Iteration 14190, Train Loss=52541.670449409125\n",
            "Iteration 14191, Train Loss=52498.78428886505\n",
            "Iteration 14192, Train Loss=52455.92484391572\n",
            "Iteration 14193, Train Loss=52413.09209714345\n",
            "Iteration 14194, Train Loss=52370.286031267104\n",
            "Iteration 14195, Train Loss=52327.50662925935\n",
            "Iteration 14196, Train Loss=52284.75387418829\n",
            "Iteration 14197, Train Loss=52242.02774936621\n",
            "Iteration 14198, Train Loss=52199.32823815069\n",
            "Iteration 14199, Train Loss=52156.65532414548\n",
            "Iteration 14200, Train Loss=52114.00899093649\n",
            "Iteration 14201, Train Loss=52071.38922236771\n",
            "Iteration 14202, Train Loss=52028.796002191506\n",
            "Iteration 14203, Train Loss=51986.229314448414\n",
            "Iteration 14204, Train Loss=51943.68914298639\n",
            "Iteration 14205, Train Loss=51901.175472007184\n",
            "Iteration 14206, Train Loss=51858.68828538048\n",
            "Iteration 14207, Train Loss=51816.22756744509\n",
            "Iteration 14208, Train Loss=51773.79330200448\n",
            "Iteration 14209, Train Loss=51731.385473534\n",
            "Iteration 14210, Train Loss=51689.004065672074\n",
            "Iteration 14211, Train Loss=51646.649063067416\n",
            "Iteration 14212, Train Loss=51604.320449073726\n",
            "Iteration 14213, Train Loss=51562.0182086264\n",
            "Iteration 14214, Train Loss=51519.742324663064\n",
            "Iteration 14215, Train Loss=51477.49278268011\n",
            "Iteration 14216, Train Loss=51435.26956509311\n",
            "Iteration 14217, Train Loss=51393.07265861874\n",
            "Iteration 14218, Train Loss=51350.90204526823\n",
            "Iteration 14219, Train Loss=51308.7577146219\n",
            "Iteration 14220, Train Loss=51266.63964936633\n",
            "Iteration 14221, Train Loss=51224.54784635605\n",
            "Iteration 14222, Train Loss=51182.48229360827\n",
            "Iteration 14223, Train Loss=51140.44300806048\n",
            "Iteration 14224, Train Loss=51098.430000435794\n",
            "Iteration 14225, Train Loss=51056.44334745475\n",
            "Iteration 14226, Train Loss=51014.48314483559\n",
            "Iteration 14227, Train Loss=50972.549658875134\n",
            "Iteration 14228, Train Loss=50930.643292238354\n",
            "Iteration 14229, Train Loss=50888.764943674265\n",
            "Iteration 14230, Train Loss=50846.9161210071\n",
            "Iteration 14231, Train Loss=50805.09992082282\n",
            "Iteration 14232, Train Loss=50763.32187458397\n",
            "Iteration 14233, Train Loss=50721.592980298665\n",
            "Iteration 14234, Train Loss=50679.93367746445\n",
            "Iteration 14235, Train Loss=50638.38421367427\n",
            "Iteration 14236, Train Loss=50597.02136869632\n",
            "Iteration 14237, Train Loss=50555.996410445\n",
            "Iteration 14238, Train Loss=50515.602911795606\n",
            "Iteration 14239, Train Loss=50476.420929026914\n",
            "Iteration 14240, Train Loss=50439.583859747545\n",
            "Iteration 14241, Train Loss=50407.31715801038\n",
            "Iteration 14242, Train Loss=50383.88223877756\n",
            "Iteration 14243, Train Loss=50377.212195912245\n",
            "Iteration 14244, Train Loss=50400.65147966069\n",
            "Iteration 14245, Train Loss=50472.48724704243\n",
            "Iteration 14246, Train Loss=50601.91315255256\n",
            "Iteration 14247, Train Loss=50748.32924568854\n",
            "Iteration 14248, Train Loss=50771.18462189502\n",
            "Iteration 14249, Train Loss=50531.764859703355\n",
            "Iteration 14250, Train Loss=50153.76680057273\n",
            "Iteration 14251, Train Loss=49977.12064601137\n",
            "Iteration 14252, Train Loss=50090.68803364873\n",
            "Iteration 14253, Train Loss=50221.50005811976\n",
            "Iteration 14254, Train Loss=50111.53142630273\n",
            "Iteration 14255, Train Loss=49867.134307280576\n",
            "Iteration 14256, Train Loss=49785.22090500551\n",
            "Iteration 14257, Train Loss=49875.48944247623\n",
            "Iteration 14258, Train Loss=49885.572427726336\n",
            "Iteration 14259, Train Loss=49731.48502950984\n",
            "Iteration 14260, Train Loss=49609.0685508451\n",
            "Iteration 14261, Train Loss=49631.55942622121\n",
            "Iteration 14262, Train Loss=49654.3136311996\n",
            "Iteration 14263, Train Loss=49559.14871269638\n",
            "Iteration 14264, Train Loss=49449.51257866348\n",
            "Iteration 14265, Train Loss=49436.45027867305\n",
            "Iteration 14266, Train Loss=49445.90520038274\n",
            "Iteration 14267, Train Loss=49379.551604240594\n",
            "Iteration 14268, Train Loss=49289.41575034506\n",
            "Iteration 14269, Train Loss=49260.46351603804\n",
            "Iteration 14270, Train Loss=49255.422201053974\n",
            "Iteration 14271, Train Loss=49202.65212602912\n",
            "Iteration 14272, Train Loss=49128.3622743324\n",
            "Iteration 14273, Train Loss=49092.36698947\n",
            "Iteration 14274, Train Loss=49076.15867597472\n",
            "Iteration 14275, Train Loss=49030.24420098065\n",
            "Iteration 14276, Train Loss=48967.24580643979\n",
            "Iteration 14277, Train Loss=48927.992073689515\n",
            "Iteration 14278, Train Loss=48903.97340015571\n",
            "Iteration 14279, Train Loss=48861.70642303747\n",
            "Iteration 14280, Train Loss=48806.48879774441\n",
            "Iteration 14281, Train Loss=48765.679320001815\n",
            "Iteration 14282, Train Loss=48736.32019672643\n",
            "Iteration 14283, Train Loss=48696.1160499277\n",
            "Iteration 14284, Train Loss=48646.20902510381\n",
            "Iteration 14285, Train Loss=48604.70119553064\n",
            "Iteration 14286, Train Loss=48571.72525023227\n",
            "Iteration 14287, Train Loss=48532.65807962314\n",
            "Iteration 14288, Train Loss=48486.40518954415\n",
            "Iteration 14289, Train Loss=48444.68725532568\n",
            "Iteration 14290, Train Loss=48409.25252266086\n",
            "Iteration 14291, Train Loss=48370.76913194078\n",
            "Iteration 14292, Train Loss=48327.04469060965\n",
            "Iteration 14293, Train Loss=48285.41966525494\n",
            "Iteration 14294, Train Loss=48248.32131375942\n",
            "Iteration 14295, Train Loss=48210.057356508405\n",
            "Iteration 14296, Train Loss=48168.09619918905\n",
            "Iteration 14297, Train Loss=48126.75500745594\n",
            "Iteration 14298, Train Loss=48088.54846551997\n",
            "Iteration 14299, Train Loss=48050.28018970729\n",
            "Iteration 14300, Train Loss=48009.53644086049\n",
            "Iteration 14301, Train Loss=47968.591947640234\n",
            "Iteration 14302, Train Loss=47929.6830962079\n",
            "Iteration 14303, Train Loss=47891.27731874371\n",
            "Iteration 14304, Train Loss=47851.35640718896\n",
            "Iteration 14305, Train Loss=47810.858260701505\n",
            "Iteration 14306, Train Loss=47771.55053907166\n",
            "Iteration 14307, Train Loss=47732.95087996799\n",
            "Iteration 14308, Train Loss=47693.55631093206\n",
            "Iteration 14309, Train Loss=47653.50451565558\n",
            "Iteration 14310, Train Loss=47614.027368157476\n",
            "Iteration 14311, Train Loss=47575.23284742389\n",
            "Iteration 14312, Train Loss=47536.147893712005\n",
            "Iteration 14313, Train Loss=47496.50043539649\n",
            "Iteration 14314, Train Loss=47457.02143913719\n",
            "Iteration 14315, Train Loss=47418.074606745504\n",
            "Iteration 14316, Train Loss=47379.14596196177\n",
            "Iteration 14317, Train Loss=47339.83407842029\n",
            "Iteration 14318, Train Loss=47300.46447384821\n",
            "Iteration 14319, Train Loss=47261.4330344042\n",
            "Iteration 14320, Train Loss=47222.56728548042\n",
            "Iteration 14321, Train Loss=47183.507878041004\n",
            "Iteration 14322, Train Loss=47144.3077821612\n",
            "Iteration 14323, Train Loss=47105.26823660177\n",
            "Iteration 14324, Train Loss=47066.42244811729\n",
            "Iteration 14325, Train Loss=47027.53631419056\n",
            "Iteration 14326, Train Loss=46988.52113064474\n",
            "Iteration 14327, Train Loss=46949.5417661924\n",
            "Iteration 14328, Train Loss=46910.71418494086\n",
            "Iteration 14329, Train Loss=46871.93829273355\n",
            "Iteration 14330, Train Loss=46833.092207473426\n",
            "Iteration 14331, Train Loss=46794.22040234541\n",
            "Iteration 14332, Train Loss=46755.43444815655\n",
            "Iteration 14333, Train Loss=46716.732079964655\n",
            "Iteration 14334, Train Loss=46678.02281829713\n",
            "Iteration 14335, Train Loss=46639.28010481958\n",
            "Iteration 14336, Train Loss=46600.56775268121\n",
            "Iteration 14337, Train Loss=46561.9282273818\n",
            "Iteration 14338, Train Loss=46523.32420487392\n",
            "Iteration 14339, Train Loss=46484.708757347245\n",
            "Iteration 14340, Train Loss=46446.09588861455\n",
            "Iteration 14341, Train Loss=46407.52793163298\n",
            "Iteration 14342, Train Loss=46369.008897727894\n",
            "Iteration 14343, Train Loss=46330.50616832514\n",
            "Iteration 14344, Train Loss=46292.00432913863\n",
            "Iteration 14345, Train Loss=46253.52399987138\n",
            "Iteration 14346, Train Loss=46215.085208259\n",
            "Iteration 14347, Train Loss=46176.67923372837\n",
            "Iteration 14348, Train Loss=46138.286571639685\n",
            "Iteration 14349, Train Loss=46099.90628932513\n",
            "Iteration 14350, Train Loss=46061.55384555872\n",
            "Iteration 14351, Train Loss=46023.23620715727\n",
            "Iteration 14352, Train Loss=45984.943913494775\n",
            "Iteration 14353, Train Loss=45946.66741023677\n",
            "Iteration 14354, Train Loss=45908.410059681904\n",
            "Iteration 14355, Train Loss=45870.18113957387\n",
            "Iteration 14356, Train Loss=45831.981960934754\n",
            "Iteration 14357, Train Loss=45793.80568434279\n",
            "Iteration 14358, Train Loss=45755.64821079343\n",
            "Iteration 14359, Train Loss=45717.513038829704\n",
            "Iteration 14360, Train Loss=45679.40520721122\n",
            "Iteration 14361, Train Loss=45641.32423538445\n",
            "Iteration 14362, Train Loss=45603.26585825075\n",
            "Iteration 14363, Train Loss=45565.228455325145\n",
            "Iteration 14364, Train Loss=45527.21457271768\n",
            "Iteration 14365, Train Loss=45489.22687253663\n",
            "Iteration 14366, Train Loss=45451.26462201285\n",
            "Iteration 14367, Train Loss=45413.325317412535\n",
            "Iteration 14368, Train Loss=45375.40831314375\n",
            "Iteration 14369, Train Loss=45337.5152215844\n",
            "Iteration 14370, Train Loss=45299.647492657554\n",
            "Iteration 14371, Train Loss=45261.80459198016\n",
            "Iteration 14372, Train Loss=45223.985075175\n",
            "Iteration 14373, Train Loss=45186.18863347621\n",
            "Iteration 14374, Train Loss=45148.41621266526\n",
            "Iteration 14375, Train Loss=45110.668658308365\n",
            "Iteration 14376, Train Loss=45072.9456832245\n",
            "Iteration 14377, Train Loss=45035.24645704457\n",
            "Iteration 14378, Train Loss=44997.570776289656\n",
            "Iteration 14379, Train Loss=44959.91916047292\n",
            "Iteration 14380, Train Loss=44922.29213999247\n",
            "Iteration 14381, Train Loss=44884.68961315778\n",
            "Iteration 14382, Train Loss=44847.11111100797\n",
            "Iteration 14383, Train Loss=44809.556469435134\n",
            "Iteration 14384, Train Loss=44772.025947610244\n",
            "Iteration 14385, Train Loss=44734.51989232887\n",
            "Iteration 14386, Train Loss=44697.03831525349\n",
            "Iteration 14387, Train Loss=44659.58096888568\n",
            "Iteration 14388, Train Loss=44622.14771988935\n",
            "Iteration 14389, Train Loss=44584.73867449905\n",
            "Iteration 14390, Train Loss=44547.354057608805\n",
            "Iteration 14391, Train Loss=44509.99393630389\n",
            "Iteration 14392, Train Loss=44472.65820294052\n",
            "Iteration 14393, Train Loss=44435.34676095867\n",
            "Iteration 14394, Train Loss=44398.05963381247\n",
            "Iteration 14395, Train Loss=44360.796958683735\n",
            "Iteration 14396, Train Loss=44323.55881741024\n",
            "Iteration 14397, Train Loss=44286.34518869573\n",
            "Iteration 14398, Train Loss=44249.15601721245\n",
            "Iteration 14399, Train Loss=44211.99129095745\n",
            "Iteration 14400, Train Loss=44174.85108467471\n",
            "Iteration 14401, Train Loss=44137.73546995929\n",
            "Iteration 14402, Train Loss=44100.644472798114\n",
            "Iteration 14403, Train Loss=44063.57807653058\n",
            "Iteration 14404, Train Loss=44026.53626474984\n",
            "Iteration 14405, Train Loss=43989.5190728625\n",
            "Iteration 14406, Train Loss=43952.52655127264\n",
            "Iteration 14407, Train Loss=43915.55874414204\n",
            "Iteration 14408, Train Loss=43878.61566375049\n",
            "Iteration 14409, Train Loss=43841.69730631058\n",
            "Iteration 14410, Train Loss=43804.803688827975\n",
            "Iteration 14411, Train Loss=43767.93484140341\n",
            "Iteration 14412, Train Loss=43731.09080736912\n",
            "Iteration 14413, Train Loss=43694.27161422026\n",
            "Iteration 14414, Train Loss=43657.4772752721\n",
            "Iteration 14415, Train Loss=43620.70780486744\n",
            "Iteration 14416, Train Loss=43583.963221293925\n",
            "Iteration 14417, Train Loss=43547.24355883892\n",
            "Iteration 14418, Train Loss=43510.54884771397\n",
            "Iteration 14419, Train Loss=43473.87911357866\n",
            "Iteration 14420, Train Loss=43437.234375825545\n",
            "Iteration 14421, Train Loss=43400.61465098618\n",
            "Iteration 14422, Train Loss=43364.01996511784\n",
            "Iteration 14423, Train Loss=43327.45034442318\n",
            "Iteration 14424, Train Loss=43290.905818661755\n",
            "Iteration 14425, Train Loss=43254.38641218133\n",
            "Iteration 14426, Train Loss=43217.89214579806\n",
            "Iteration 14427, Train Loss=43181.42304235918\n",
            "Iteration 14428, Train Loss=43144.97912410294\n",
            "Iteration 14429, Train Loss=43108.56041897838\n",
            "Iteration 14430, Train Loss=43072.16695238834\n",
            "Iteration 14431, Train Loss=43035.79874949342\n",
            "Iteration 14432, Train Loss=42999.455833701475\n",
            "Iteration 14433, Train Loss=42963.138226773845\n",
            "Iteration 14434, Train Loss=42926.84595372704\n",
            "Iteration 14435, Train Loss=42890.579038318574\n",
            "Iteration 14436, Train Loss=42854.33750678015\n",
            "Iteration 14437, Train Loss=42818.12138327952\n",
            "Iteration 14438, Train Loss=42781.93069134557\n",
            "Iteration 14439, Train Loss=42745.76545469729\n",
            "Iteration 14440, Train Loss=42709.62569588713\n",
            "Iteration 14441, Train Loss=42673.511439805785\n",
            "Iteration 14442, Train Loss=42637.422709919985\n",
            "Iteration 14443, Train Loss=42601.35953078668\n",
            "Iteration 14444, Train Loss=42565.32192581451\n",
            "Iteration 14445, Train Loss=42529.30991786303\n",
            "Iteration 14446, Train Loss=42493.323530391615\n",
            "Iteration 14447, Train Loss=42457.36278582204\n",
            "Iteration 14448, Train Loss=42421.42770802522\n",
            "Iteration 14449, Train Loss=42385.518319681214\n",
            "Iteration 14450, Train Loss=42349.63464402965\n",
            "Iteration 14451, Train Loss=42313.77670366057\n",
            "Iteration 14452, Train Loss=42277.94452071392\n",
            "Iteration 14453, Train Loss=42242.138117794624\n",
            "Iteration 14454, Train Loss=42206.357516613374\n",
            "Iteration 14455, Train Loss=42170.60273976249\n",
            "Iteration 14456, Train Loss=42134.87380889082\n",
            "Iteration 14457, Train Loss=42099.1707459955\n",
            "Iteration 14458, Train Loss=42063.49357258534\n",
            "Iteration 14459, Train Loss=42027.842309831845\n",
            "Iteration 14460, Train Loss=41992.216979125296\n",
            "Iteration 14461, Train Loss=41956.61760113522\n",
            "Iteration 14462, Train Loss=41921.04419704679\n",
            "Iteration 14463, Train Loss=41885.49678727703\n",
            "Iteration 14464, Train Loss=41849.97539248911\n",
            "Iteration 14465, Train Loss=41814.48003287501\n",
            "Iteration 14466, Train Loss=41779.010728411726\n",
            "Iteration 14467, Train Loss=41743.567499060104\n",
            "Iteration 14468, Train Loss=41708.1503642382\n",
            "Iteration 14469, Train Loss=41672.75934361379\n",
            "Iteration 14470, Train Loss=41637.39445621536\n",
            "Iteration 14471, Train Loss=41602.055721244506\n",
            "Iteration 14472, Train Loss=41566.74315741198\n",
            "Iteration 14473, Train Loss=41531.456783321046\n",
            "Iteration 14474, Train Loss=41496.19661737305\n",
            "Iteration 14475, Train Loss=41460.96267759326\n",
            "Iteration 14476, Train Loss=41425.75498203807\n",
            "Iteration 14477, Train Loss=41390.573548251006\n",
            "Iteration 14478, Train Loss=41355.41839385653\n",
            "Iteration 14479, Train Loss=41320.28953599265\n",
            "Iteration 14480, Train Loss=41285.1869917584\n",
            "Iteration 14481, Train Loss=41250.110777916634\n",
            "Iteration 14482, Train Loss=41215.06091100829\n",
            "Iteration 14483, Train Loss=41180.03740741797\n",
            "Iteration 14484, Train Loss=41145.04028315728\n",
            "Iteration 14485, Train Loss=41110.0695541971\n",
            "Iteration 14486, Train Loss=41075.12523607527\n",
            "Iteration 14487, Train Loss=41040.20734429357\n",
            "Iteration 14488, Train Loss=41005.31589395924\n",
            "Iteration 14489, Train Loss=40970.450900061114\n",
            "Iteration 14490, Train Loss=40935.6123772969\n",
            "Iteration 14491, Train Loss=40900.800340132744\n",
            "Iteration 14492, Train Loss=40866.01480285271\n",
            "Iteration 14493, Train Loss=40831.255779420295\n",
            "Iteration 14494, Train Loss=40796.5232836867\n",
            "Iteration 14495, Train Loss=40761.817329143385\n",
            "Iteration 14496, Train Loss=40727.137929183074\n",
            "Iteration 14497, Train Loss=40692.48509685391\n",
            "Iteration 14498, Train Loss=40657.85884506871\n",
            "Iteration 14499, Train Loss=40623.259186450545\n",
            "Iteration 14500, Train Loss=40588.686133426876\n",
            "Iteration 14501, Train Loss=40554.13969820122\n",
            "Iteration 14502, Train Loss=40519.61989272541\n",
            "Iteration 14503, Train Loss=40485.12672878009\n",
            "Iteration 14504, Train Loss=40450.66021785723\n",
            "Iteration 14505, Train Loss=40416.22037130767\n",
            "Iteration 14506, Train Loss=40381.8072001833\n",
            "Iteration 14507, Train Loss=40347.42071539718\n",
            "Iteration 14508, Train Loss=40313.06092757815\n",
            "Iteration 14509, Train Loss=40278.72784719806\n",
            "Iteration 14510, Train Loss=40244.42148447666\n",
            "Iteration 14511, Train Loss=40210.141849445354\n",
            "Iteration 14512, Train Loss=40175.88895192338\n",
            "Iteration 14513, Train Loss=40141.66280151287\n",
            "Iteration 14514, Train Loss=40107.46340763262\n",
            "Iteration 14515, Train Loss=40073.290779468065\n",
            "Iteration 14516, Train Loss=40039.14492604251\n",
            "Iteration 14517, Train Loss=40005.02585613576\n",
            "Iteration 14518, Train Loss=39970.933578377415\n",
            "Iteration 14519, Train Loss=39936.868101155414\n",
            "Iteration 14520, Train Loss=39902.829432709215\n",
            "Iteration 14521, Train Loss=39868.817581046096\n",
            "Iteration 14522, Train Loss=39834.83255402015\n",
            "Iteration 14523, Train Loss=39800.87435926861\n",
            "Iteration 14524, Train Loss=39766.94300426786\n",
            "Iteration 14525, Train Loss=39733.0384962928\n",
            "Iteration 14526, Train Loss=39699.160842450045\n",
            "Iteration 14527, Train Loss=39665.3100496604\n",
            "Iteration 14528, Train Loss=39631.48612466936\n",
            "Iteration 14529, Train Loss=39597.68907405193\n",
            "Iteration 14530, Train Loss=39563.91890420097\n",
            "Iteration 14531, Train Loss=39530.17562135284\n",
            "Iteration 14532, Train Loss=39496.459231557215\n",
            "Iteration 14533, Train Loss=39462.76974071781\n",
            "Iteration 14534, Train Loss=39429.1071545522\n",
            "Iteration 14535, Train Loss=39395.47147863686\n",
            "Iteration 14536, Train Loss=39361.86271836491\n",
            "Iteration 14537, Train Loss=39328.280878995654\n",
            "Iteration 14538, Train Loss=39294.72596560486\n",
            "Iteration 14539, Train Loss=39261.19798314035\n",
            "Iteration 14540, Train Loss=39227.6969363704\n",
            "Iteration 14541, Train Loss=39194.22282993784\n",
            "Iteration 14542, Train Loss=39160.7756683092\n",
            "Iteration 14543, Train Loss=39127.35545583219\n",
            "Iteration 14544, Train Loss=39093.96219667727\n",
            "Iteration 14545, Train Loss=39060.5958949031\n",
            "Iteration 14546, Train Loss=39027.2565543919\n",
            "Iteration 14547, Train Loss=38993.94417892039\n",
            "Iteration 14548, Train Loss=38960.65877208626\n",
            "Iteration 14549, Train Loss=38927.400337392995\n",
            "Iteration 14550, Train Loss=38894.16887815917\n",
            "Iteration 14551, Train Loss=38860.9643976212\n",
            "Iteration 14552, Train Loss=38827.78689882245\n",
            "Iteration 14553, Train Loss=38794.63638474036\n",
            "Iteration 14554, Train Loss=38761.51285814562\n",
            "Iteration 14555, Train Loss=38728.4163217649\n",
            "Iteration 14556, Train Loss=38695.34677809766\n",
            "Iteration 14557, Train Loss=38662.30422962947\n",
            "Iteration 14558, Train Loss=38629.28867858582\n",
            "Iteration 14559, Train Loss=38596.300127223985\n",
            "Iteration 14560, Train Loss=38563.338577491435\n",
            "Iteration 14561, Train Loss=38530.40403143155\n",
            "Iteration 14562, Train Loss=38497.49649070442\n",
            "Iteration 14563, Train Loss=38464.61595716233\n",
            "Iteration 14564, Train Loss=38431.76243216024\n",
            "Iteration 14565, Train Loss=38398.9359173953\n",
            "Iteration 14566, Train Loss=38366.13641388771\n",
            "Iteration 14567, Train Loss=38333.363923238845\n",
            "Iteration 14568, Train Loss=38300.6184460928\n",
            "Iteration 14569, Train Loss=38267.89998405685\n",
            "Iteration 14570, Train Loss=38235.20853734754\n",
            "Iteration 14571, Train Loss=38202.54410777779\n",
            "Iteration 14572, Train Loss=38169.906695097474\n",
            "Iteration 14573, Train Loss=38137.296301747054\n",
            "Iteration 14574, Train Loss=38104.71292709897\n",
            "Iteration 14575, Train Loss=38072.156575211106\n",
            "Iteration 14576, Train Loss=38039.62724570248\n",
            "Iteration 14577, Train Loss=38007.12494680168\n",
            "Iteration 14578, Train Loss=37974.64968096466\n",
            "Iteration 14579, Train Loss=37942.201467763414\n",
            "Iteration 14580, Train Loss=37909.78032194225\n",
            "Iteration 14581, Train Loss=37877.38629602761\n",
            "Iteration 14582, Train Loss=37845.01945018874\n",
            "Iteration 14583, Train Loss=37812.67993870693\n",
            "Iteration 14584, Train Loss=37780.36798262837\n",
            "Iteration 14585, Train Loss=37748.084066783944\n",
            "Iteration 14586, Train Loss=37715.828978850805\n",
            "Iteration 14587, Train Loss=37683.604322798434\n",
            "Iteration 14588, Train Loss=37651.41290210266\n",
            "Iteration 14589, Train Loss=37619.26024300955\n",
            "Iteration 14590, Train Loss=37587.15644373231\n",
            "Iteration 14591, Train Loss=37555.12119482526\n",
            "Iteration 14592, Train Loss=37523.19152815852\n",
            "Iteration 14593, Train Loss=37491.439655095455\n",
            "Iteration 14594, Train Loss=37460.00407435881\n",
            "Iteration 14595, Train Loss=37429.15605108865\n",
            "Iteration 14596, Train Loss=37399.422188653276\n",
            "Iteration 14597, Train Loss=37371.83594804543\n",
            "Iteration 14598, Train Loss=37348.40037191295\n",
            "Iteration 14599, Train Loss=37332.97857211078\n",
            "Iteration 14600, Train Loss=37332.71296555206\n",
            "Iteration 14601, Train Loss=37359.99062108002\n",
            "Iteration 14602, Train Loss=37432.352445397024\n",
            "Iteration 14603, Train Loss=37563.401751642115\n",
            "Iteration 14604, Train Loss=37725.800305730605\n",
            "Iteration 14605, Train Loss=37801.31987597047\n",
            "Iteration 14606, Train Loss=37627.01868337805\n",
            "Iteration 14607, Train Loss=37256.80796568494\n",
            "Iteration 14608, Train Loss=37017.34735107831\n",
            "Iteration 14609, Train Loss=37088.28504645554\n",
            "Iteration 14610, Train Loss=37259.200209010305\n",
            "Iteration 14611, Train Loss=37224.87639024506\n",
            "Iteration 14612, Train Loss=36992.67563236847\n",
            "Iteration 14613, Train Loss=36858.34629983538\n",
            "Iteration 14614, Train Loss=36933.47365320589\n",
            "Iteration 14615, Train Loss=36998.95392724483\n",
            "Iteration 14616, Train Loss=36888.189459656896\n",
            "Iteration 14617, Train Loss=36741.82202960919\n",
            "Iteration 14618, Train Loss=36736.89770499951\n",
            "Iteration 14619, Train Loss=36790.57295027983\n",
            "Iteration 14620, Train Loss=36740.057914326375\n",
            "Iteration 14621, Train Loss=36625.66917050215\n",
            "Iteration 14622, Train Loss=36589.81674584906\n",
            "Iteration 14623, Train Loss=36616.92923552614\n",
            "Iteration 14624, Train Loss=36588.10715949774\n",
            "Iteration 14625, Train Loss=36502.798857193746\n",
            "Iteration 14626, Train Loss=36458.74077834453\n",
            "Iteration 14627, Train Loss=36465.0286736212\n",
            "Iteration 14628, Train Loss=36442.56716568822\n",
            "Iteration 14629, Train Loss=36377.40181664855\n",
            "Iteration 14630, Train Loss=36333.11120767373\n",
            "Iteration 14631, Train Loss=36325.46028339795\n",
            "Iteration 14632, Train Loss=36303.83240739085\n",
            "Iteration 14633, Train Loss=36251.90629263154\n",
            "Iteration 14634, Train Loss=36209.654447827415\n",
            "Iteration 14635, Train Loss=36192.97941193919\n",
            "Iteration 14636, Train Loss=36170.50269754101\n",
            "Iteration 14637, Train Loss=36127.15208272658\n",
            "Iteration 14638, Train Loss=36087.29824450889\n",
            "Iteration 14639, Train Loss=36064.84035356514\n",
            "Iteration 14640, Train Loss=36041.06080658926\n",
            "Iteration 14641, Train Loss=36003.29111181743\n",
            "Iteration 14642, Train Loss=35965.64553486787\n",
            "Iteration 14643, Train Loss=35939.51132801331\n",
            "Iteration 14644, Train Loss=35914.42365992877\n",
            "Iteration 14645, Train Loss=35880.25388559423\n",
            "Iteration 14646, Train Loss=35844.520948805766\n",
            "Iteration 14647, Train Loss=35816.1072454728\n",
            "Iteration 14648, Train Loss=35789.81675902144\n",
            "Iteration 14649, Train Loss=35757.940224002305\n",
            "Iteration 14650, Train Loss=35723.82866222283\n",
            "Iteration 14651, Train Loss=35694.07719289706\n",
            "Iteration 14652, Train Loss=35666.74093890622\n",
            "Iteration 14653, Train Loss=35636.262322654205\n",
            "Iteration 14654, Train Loss=35603.50817975194\n",
            "Iteration 14655, Train Loss=35573.06690571601\n",
            "Iteration 14656, Train Loss=35544.85818644661\n",
            "Iteration 14657, Train Loss=35515.1695267278\n",
            "Iteration 14658, Train Loss=35483.522089788305\n",
            "Iteration 14659, Train Loss=35452.83576246166\n",
            "Iteration 14660, Train Loss=35423.9459237415\n",
            "Iteration 14661, Train Loss=35394.63244682911\n",
            "Iteration 14662, Train Loss=35363.85347722902\n",
            "Iteration 14663, Train Loss=35333.216611839154\n",
            "Iteration 14664, Train Loss=35303.84404204935\n",
            "Iteration 14665, Train Loss=35274.64269620072\n",
            "Iteration 14666, Train Loss=35244.503112891805\n",
            "Iteration 14667, Train Loss=35214.09213657961\n",
            "Iteration 14668, Train Loss=35184.433093495485\n",
            "Iteration 14669, Train Loss=35155.196521587684\n",
            "Iteration 14670, Train Loss=35125.489157271295\n",
            "Iteration 14671, Train Loss=35095.38365873883\n",
            "Iteration 14672, Train Loss=35065.61530089889\n",
            "Iteration 14673, Train Loss=35036.289998399756\n",
            "Iteration 14674, Train Loss=35006.83993056606\n",
            "Iteration 14675, Train Loss=34977.045568566464\n",
            "Iteration 14676, Train Loss=34947.309812862084\n",
            "Iteration 14677, Train Loss=34917.90921580374\n",
            "Iteration 14678, Train Loss=34888.58901622051\n",
            "Iteration 14679, Train Loss=34859.060281067235\n",
            "Iteration 14680, Train Loss=34829.451694282834\n",
            "Iteration 14681, Train Loss=34800.029792583824\n",
            "Iteration 14682, Train Loss=34770.76466693802\n",
            "Iteration 14683, Train Loss=34741.43327323357\n",
            "Iteration 14684, Train Loss=34711.99509540049\n",
            "Iteration 14685, Train Loss=34682.61787613415\n",
            "Iteration 14686, Train Loss=34653.38390025048\n",
            "Iteration 14687, Train Loss=34624.18330069981\n",
            "Iteration 14688, Train Loss=34594.915770670246\n",
            "Iteration 14689, Train Loss=34565.63753358185\n",
            "Iteration 14690, Train Loss=34536.447546228876\n",
            "Iteration 14691, Train Loss=34507.33279681803\n",
            "Iteration 14692, Train Loss=34478.20928517153\n",
            "Iteration 14693, Train Loss=34449.0590030135\n",
            "Iteration 14694, Train Loss=34419.94293834131\n",
            "Iteration 14695, Train Loss=34390.89714423821\n",
            "Iteration 14696, Train Loss=34361.88534313826\n",
            "Iteration 14697, Train Loss=34332.865244136374\n",
            "Iteration 14698, Train Loss=34303.850490657875\n",
            "Iteration 14699, Train Loss=34274.87995042347\n",
            "Iteration 14700, Train Loss=34245.95755709535\n",
            "Iteration 14701, Train Loss=34217.05353703364\n",
            "Iteration 14702, Train Loss=34188.15288844707\n",
            "Iteration 14703, Train Loss=34159.273646036185\n",
            "Iteration 14704, Train Loss=34130.43499229378\n",
            "Iteration 14705, Train Loss=34101.63036764551\n",
            "Iteration 14706, Train Loss=34072.84160286975\n",
            "Iteration 14707, Train Loss=34044.066149785955\n",
            "Iteration 14708, Train Loss=34015.317594634456\n",
            "Iteration 14709, Train Loss=33986.603768267116\n",
            "Iteration 14710, Train Loss=33957.91722861926\n",
            "Iteration 14711, Train Loss=33929.24835541216\n",
            "Iteration 14712, Train Loss=33900.59864809793\n",
            "Iteration 14713, Train Loss=33871.97658709157\n",
            "Iteration 14714, Train Loss=33843.384822417655\n",
            "Iteration 14715, Train Loss=33814.81779178872\n",
            "Iteration 14716, Train Loss=33786.270713842314\n",
            "Iteration 14717, Train Loss=33757.74564196427\n",
            "Iteration 14718, Train Loss=33729.24747019521\n",
            "Iteration 14719, Train Loss=33700.776906019055\n",
            "Iteration 14720, Train Loss=33672.330363939516\n",
            "Iteration 14721, Train Loss=33643.90545710655\n",
            "Iteration 14722, Train Loss=33615.50375144096\n",
            "Iteration 14723, Train Loss=33587.128012978625\n",
            "Iteration 14724, Train Loss=33558.77838470262\n",
            "Iteration 14725, Train Loss=33530.45270653781\n",
            "Iteration 14726, Train Loss=33502.149699353824\n",
            "Iteration 14727, Train Loss=33473.87032852889\n",
            "Iteration 14728, Train Loss=33445.616176286516\n",
            "Iteration 14729, Train Loss=33417.387301587594\n",
            "Iteration 14730, Train Loss=33389.182451152265\n",
            "Iteration 14731, Train Loss=33361.00085884127\n",
            "Iteration 14732, Train Loss=33332.843029072894\n",
            "Iteration 14733, Train Loss=33304.709884693366\n",
            "Iteration 14734, Train Loss=33276.60152087622\n",
            "Iteration 14735, Train Loss=33248.51723783673\n",
            "Iteration 14736, Train Loss=33220.45652952858\n",
            "Iteration 14737, Train Loss=33192.41959958433\n",
            "Iteration 14738, Train Loss=33164.40698588542\n",
            "Iteration 14739, Train Loss=33136.41881842156\n",
            "Iteration 14740, Train Loss=33108.454732877166\n",
            "Iteration 14741, Train Loss=33080.51438116997\n",
            "Iteration 14742, Train Loss=33052.59779064435\n",
            "Iteration 14743, Train Loss=33024.70525918922\n",
            "Iteration 14744, Train Loss=32996.83692110008\n",
            "Iteration 14745, Train Loss=32968.99261323873\n",
            "Iteration 14746, Train Loss=32941.17210241482\n",
            "Iteration 14747, Train Loss=32913.37532743205\n",
            "Iteration 14748, Train Loss=32885.60242992126\n",
            "Iteration 14749, Train Loss=32857.85351997798\n",
            "Iteration 14750, Train Loss=32830.12854866333\n",
            "Iteration 14751, Train Loss=32802.42737328941\n",
            "Iteration 14752, Train Loss=32774.749903232856\n",
            "Iteration 14753, Train Loss=32747.09618040968\n",
            "Iteration 14754, Train Loss=32719.46627398328\n",
            "Iteration 14755, Train Loss=32691.8601895669\n",
            "Iteration 14756, Train Loss=32664.27785299048\n",
            "Iteration 14757, Train Loss=32636.719180836157\n",
            "Iteration 14758, Train Loss=32609.184158736338\n",
            "Iteration 14759, Train Loss=32581.67281210022\n",
            "Iteration 14760, Train Loss=32554.185160711826\n",
            "Iteration 14761, Train Loss=32526.721174816274\n",
            "Iteration 14762, Train Loss=32499.280795602\n",
            "Iteration 14763, Train Loss=32471.863986166834\n",
            "Iteration 14764, Train Loss=32444.470737392818\n",
            "Iteration 14765, Train Loss=32417.101059883087\n",
            "Iteration 14766, Train Loss=32389.754944282246\n",
            "Iteration 14767, Train Loss=32362.432357680005\n",
            "Iteration 14768, Train Loss=32335.133263120555\n",
            "Iteration 14769, Train Loss=32307.857633040832\n",
            "Iteration 14770, Train Loss=32280.605461199313\n",
            "Iteration 14771, Train Loss=32253.37673991844\n",
            "Iteration 14772, Train Loss=32226.17145286132\n",
            "Iteration 14773, Train Loss=32198.98957261736\n",
            "Iteration 14774, Train Loss=32171.83106870508\n",
            "Iteration 14775, Train Loss=32144.695922081824\n",
            "Iteration 14776, Train Loss=32117.58411773144\n",
            "Iteration 14777, Train Loss=32090.495643748378\n",
            "Iteration 14778, Train Loss=32063.430480935647\n",
            "Iteration 14779, Train Loss=32036.388604539796\n",
            "Iteration 14780, Train Loss=32009.369991303167\n",
            "Iteration 14781, Train Loss=31982.374619553484\n",
            "Iteration 14782, Train Loss=31955.40247407021\n",
            "Iteration 14783, Train Loss=31928.4535377039\n",
            "Iteration 14784, Train Loss=31901.52779161836\n",
            "Iteration 14785, Train Loss=31874.625214271084\n",
            "Iteration 14786, Train Loss=31847.74578267812\n",
            "Iteration 14787, Train Loss=31820.88947748837\n",
            "Iteration 14788, Train Loss=31794.05627940673\n",
            "Iteration 14789, Train Loss=31767.24617125131\n",
            "Iteration 14790, Train Loss=31740.459133682736\n",
            "Iteration 14791, Train Loss=31713.695146073074\n",
            "Iteration 14792, Train Loss=31686.954187896852\n",
            "Iteration 14793, Train Loss=31660.236238121455\n",
            "Iteration 14794, Train Loss=31633.54127819197\n",
            "Iteration 14795, Train Loss=31606.8692887682\n",
            "Iteration 14796, Train Loss=31580.220251154456\n",
            "Iteration 14797, Train Loss=31553.59414556476\n",
            "Iteration 14798, Train Loss=31526.990951553315\n",
            "Iteration 14799, Train Loss=31500.410649430793\n",
            "Iteration 14800, Train Loss=31473.853219023404\n",
            "Iteration 14801, Train Loss=31447.31864160347\n",
            "Iteration 14802, Train Loss=31420.8068976852\n",
            "Iteration 14803, Train Loss=31394.31796808917\n",
            "Iteration 14804, Train Loss=31367.85183321055\n",
            "Iteration 14805, Train Loss=31341.40847307602\n",
            "Iteration 14806, Train Loss=31314.98786838365\n",
            "Iteration 14807, Train Loss=31288.589999371885\n",
            "Iteration 14808, Train Loss=31262.214847174368\n",
            "Iteration 14809, Train Loss=31235.8623923845\n",
            "Iteration 14810, Train Loss=31209.532615850527\n",
            "Iteration 14811, Train Loss=31183.22549824969\n",
            "Iteration 14812, Train Loss=31156.941020052367\n",
            "Iteration 14813, Train Loss=31130.679162193963\n",
            "Iteration 14814, Train Loss=31104.439905249405\n",
            "Iteration 14815, Train Loss=31078.223230401007\n",
            "Iteration 14816, Train Loss=31052.029118462593\n",
            "Iteration 14817, Train Loss=31025.85755051193\n",
            "Iteration 14818, Train Loss=30999.70850752228\n",
            "Iteration 14819, Train Loss=30973.58197039064\n",
            "Iteration 14820, Train Loss=30947.477920299854\n",
            "Iteration 14821, Train Loss=30921.396338191098\n",
            "Iteration 14822, Train Loss=30895.33720543833\n",
            "Iteration 14823, Train Loss=30869.300503159175\n",
            "Iteration 14824, Train Loss=30843.286212747473\n",
            "Iteration 14825, Train Loss=30817.294315498577\n",
            "Iteration 14826, Train Loss=30791.324792748077\n",
            "Iteration 14827, Train Loss=30765.37762597663\n",
            "Iteration 14828, Train Loss=30739.452796545916\n",
            "Iteration 14829, Train Loss=30713.550286114136\n",
            "Iteration 14830, Train Loss=30687.67007616956\n",
            "Iteration 14831, Train Loss=30661.81214847156\n",
            "Iteration 14832, Train Loss=30635.976484675077\n",
            "Iteration 14833, Train Loss=30610.16306657077\n",
            "Iteration 14834, Train Loss=30584.371875990728\n",
            "Iteration 14835, Train Loss=30558.602894765452\n",
            "Iteration 14836, Train Loss=30532.856104902115\n",
            "Iteration 14837, Train Loss=30507.13148832626\n",
            "Iteration 14838, Train Loss=30481.429027188726\n",
            "Iteration 14839, Train Loss=30455.748703557976\n",
            "Iteration 14840, Train Loss=30430.090499684553\n",
            "Iteration 14841, Train Loss=30404.45439780056\n",
            "Iteration 14842, Train Loss=30378.840380234986\n",
            "Iteration 14843, Train Loss=30353.248429391184\n",
            "Iteration 14844, Train Loss=30327.67852768681\n",
            "Iteration 14845, Train Loss=30302.1306576891\n",
            "Iteration 14846, Train Loss=30276.604801933758\n",
            "Iteration 14847, Train Loss=30251.100943130314\n",
            "Iteration 14848, Train Loss=30225.619063958962\n",
            "Iteration 14849, Train Loss=30200.159147249284\n",
            "Iteration 14850, Train Loss=30174.721175840525\n",
            "Iteration 14851, Train Loss=30149.30513267182\n",
            "Iteration 14852, Train Loss=30123.91100074639\n",
            "Iteration 14853, Train Loss=30098.53876311729\n",
            "Iteration 14854, Train Loss=30073.18840294633\n",
            "Iteration 14855, Train Loss=30047.85990341343\n",
            "Iteration 14856, Train Loss=30022.553247829994\n",
            "Iteration 14857, Train Loss=29997.268419515596\n",
            "Iteration 14858, Train Loss=29972.005401919112\n",
            "Iteration 14859, Train Loss=29946.764178510093\n",
            "Iteration 14860, Train Loss=29921.54473286711\n",
            "Iteration 14861, Train Loss=29896.347048614745\n",
            "Iteration 14862, Train Loss=29871.171109458683\n",
            "Iteration 14863, Train Loss=29846.01689917869\n",
            "Iteration 14864, Train Loss=29820.884401609354\n",
            "Iteration 14865, Train Loss=29795.773600682267\n",
            "Iteration 14866, Train Loss=29770.684480366675\n",
            "Iteration 14867, Train Loss=29745.617024738993\n",
            "Iteration 14868, Train Loss=29720.571217907727\n",
            "Iteration 14869, Train Loss=29695.54704408956\n",
            "Iteration 14870, Train Loss=29670.544487535786\n",
            "Iteration 14871, Train Loss=29645.563532597014\n",
            "Iteration 14872, Train Loss=29620.604163669886\n",
            "Iteration 14873, Train Loss=29595.666365236288\n",
            "Iteration 14874, Train Loss=29570.750121836994\n",
            "Iteration 14875, Train Loss=29545.855418084284\n",
            "Iteration 14876, Train Loss=29520.98223866108\n",
            "Iteration 14877, Train Loss=29496.13056830923\n",
            "Iteration 14878, Train Loss=29471.300391849643\n",
            "Iteration 14879, Train Loss=29446.49169415408\n",
            "Iteration 14880, Train Loss=29421.704460179106\n",
            "Iteration 14881, Train Loss=29396.938674925088\n",
            "Iteration 14882, Train Loss=29372.194323479234\n",
            "Iteration 14883, Train Loss=29347.47139097083\n",
            "Iteration 14884, Train Loss=29322.769862613583\n",
            "Iteration 14885, Train Loss=29298.08972366421\n",
            "Iteration 14886, Train Loss=29273.43095945993\n",
            "Iteration 14887, Train Loss=29248.793555381977\n",
            "Iteration 14888, Train Loss=29224.177496888577\n",
            "Iteration 14889, Train Loss=29199.582769482502\n",
            "Iteration 14890, Train Loss=29175.009358738713\n",
            "Iteration 14891, Train Loss=29150.45725027984\n",
            "Iteration 14892, Train Loss=29125.926429793995\n",
            "Iteration 14893, Train Loss=29101.416883019127\n",
            "Iteration 14894, Train Loss=29076.928595755144\n",
            "Iteration 14895, Train Loss=29052.46155385113\n",
            "Iteration 14896, Train Loss=29028.01574321384\n",
            "Iteration 14897, Train Loss=29003.591149800544\n",
            "Iteration 14898, Train Loss=28979.187759622087\n",
            "Iteration 14899, Train Loss=28954.805558739423\n",
            "Iteration 14900, Train Loss=28930.444533263413\n",
            "Iteration 14901, Train Loss=28906.104669354176\n",
            "Iteration 14902, Train Loss=28881.78595322005\n",
            "Iteration 14903, Train Loss=28857.488371115593\n",
            "Iteration 14904, Train Loss=28833.211909340964\n",
            "Iteration 14905, Train Loss=28808.956554243436\n",
            "Iteration 14906, Train Loss=28784.72229221017\n",
            "Iteration 14907, Train Loss=28760.50910967507\n",
            "Iteration 14908, Train Loss=28736.316993108787\n",
            "Iteration 14909, Train Loss=28712.145929028164\n",
            "Iteration 14910, Train Loss=28687.995903982654\n",
            "Iteration 14911, Train Loss=28663.866904567367\n",
            "Iteration 14912, Train Loss=28639.758917406452\n",
            "Iteration 14913, Train Loss=28615.671929167274\n",
            "Iteration 14914, Train Loss=28591.605926544657\n",
            "Iteration 14915, Train Loss=28567.560896275856\n",
            "Iteration 14916, Train Loss=28543.53682511815\n",
            "Iteration 14917, Train Loss=28519.533699876163\n",
            "Iteration 14918, Train Loss=28495.551507364777\n",
            "Iteration 14919, Train Loss=28471.590234451123\n",
            "Iteration 14920, Train Loss=28447.64986800254\n",
            "Iteration 14921, Train Loss=28423.730394944385\n",
            "Iteration 14922, Train Loss=28399.831802189954\n",
            "Iteration 14923, Train Loss=28375.954076723232\n",
            "Iteration 14924, Train Loss=28352.09720549353\n",
            "Iteration 14925, Train Loss=28328.261175542953\n",
            "Iteration 14926, Train Loss=28304.44597384713\n",
            "Iteration 14927, Train Loss=28280.651587511366\n",
            "Iteration 14928, Train Loss=28256.878003523492\n",
            "Iteration 14929, Train Loss=28233.125209064387\n",
            "Iteration 14930, Train Loss=28209.393191115676\n",
            "Iteration 14931, Train Loss=28185.68193696126\n",
            "Iteration 14932, Train Loss=28161.991433556286\n",
            "Iteration 14933, Train Loss=28138.321668348763\n",
            "Iteration 14934, Train Loss=28114.672628256358\n",
            "Iteration 14935, Train Loss=28091.044301037044\n",
            "Iteration 14936, Train Loss=28067.436673615844\n",
            "Iteration 14937, Train Loss=28043.849734439988\n",
            "Iteration 14938, Train Loss=28020.28347073934\n",
            "Iteration 14939, Train Loss=27996.737872716203\n",
            "Iteration 14940, Train Loss=27973.21292915625\n",
            "Iteration 14941, Train Loss=27949.708635275332\n",
            "Iteration 14942, Train Loss=27926.224986169662\n",
            "Iteration 14943, Train Loss=27902.761992687047\n",
            "Iteration 14944, Train Loss=27879.319673779635\n",
            "Iteration 14945, Train Loss=27855.898092314055\n",
            "Iteration 14946, Train Loss=27832.497355749736\n",
            "Iteration 14947, Train Loss=27809.11770870823\n",
            "Iteration 14948, Train Loss=27785.759588105237\n",
            "Iteration 14949, Train Loss=27762.423898253393\n",
            "Iteration 14950, Train Loss=27739.112319197426\n",
            "Iteration 14951, Train Loss=27715.828226715716\n",
            "Iteration 14952, Train Loss=27692.578079234074\n",
            "Iteration 14953, Train Loss=27669.374772993975\n",
            "Iteration 14954, Train Loss=27646.24351810893\n",
            "Iteration 14955, Train Loss=27623.234822759714\n",
            "Iteration 14956, Train Loss=27600.4490379832\n",
            "Iteration 14957, Train Loss=27578.0886132519\n",
            "Iteration 14958, Train Loss=27556.560432580754\n",
            "Iteration 14959, Train Loss=27536.689987122438\n",
            "Iteration 14960, Train Loss=27520.140335236512\n",
            "Iteration 14961, Train Loss=27510.261155816766\n",
            "Iteration 14962, Train Loss=27513.60699820267\n",
            "Iteration 14963, Train Loss=27542.513397632843\n",
            "Iteration 14964, Train Loss=27617.236760796273\n",
            "Iteration 14965, Train Loss=27762.26796739882\n",
            "Iteration 14966, Train Loss=27973.813478416843\n",
            "Iteration 14967, Train Loss=28149.044976027817\n",
            "Iteration 14968, Train Loss=28062.812108779548\n",
            "Iteration 14969, Train Loss=27658.610731425044\n",
            "Iteration 14970, Train Loss=27303.322308831324\n",
            "Iteration 14971, Train Loss=27341.272129196277\n",
            "Iteration 14972, Train Loss=27581.574717299773\n",
            "Iteration 14973, Train Loss=27592.411843256017\n",
            "Iteration 14974, Train Loss=27323.37828426714\n",
            "Iteration 14975, Train Loss=27165.504377512807\n",
            "Iteration 14976, Train Loss=27282.54264875488\n",
            "Iteration 14977, Train Loss=27370.735482009408\n",
            "Iteration 14978, Train Loss=27225.908025393077\n",
            "Iteration 14979, Train Loss=27076.304066371238\n",
            "Iteration 14980, Train Loss=27123.03427987634\n",
            "Iteration 14981, Train Loss=27189.579679316703\n",
            "Iteration 14982, Train Loss=27097.628566475505\n",
            "Iteration 14983, Train Loss=26986.742586490254\n",
            "Iteration 14984, Train Loss=27008.069699396518\n",
            "Iteration 14985, Train Loss=27044.757180833614\n",
            "Iteration 14986, Train Loss=26974.242280531045\n",
            "Iteration 14987, Train Loss=26896.11484942332\n",
            "Iteration 14988, Train Loss=26906.932023235386\n",
            "Iteration 14989, Train Loss=26920.09507879244\n",
            "Iteration 14990, Train Loss=26861.594806942732\n",
            "Iteration 14991, Train Loss=26806.59428500321\n",
            "Iteration 14992, Train Loss=26810.415750980224\n",
            "Iteration 14993, Train Loss=26807.652557231766\n",
            "Iteration 14994, Train Loss=26758.129012809623\n",
            "Iteration 14995, Train Loss=26718.16157289411\n",
            "Iteration 14996, Train Loss=26715.843175418246\n",
            "Iteration 14997, Train Loss=26703.17256459652\n",
            "Iteration 14998, Train Loss=26661.175437470538\n",
            "Iteration 14999, Train Loss=26630.245023247197\n",
            "Iteration 15000, Test Loss=2135.464760787778\n",
            "Iteration 15000, Train Loss=26622.54672749964\n",
            "Iteration 15001, Train Loss=26604.373151172418\n",
            "Iteration 15002, Train Loss=26568.405241315755\n",
            "Iteration 15003, Train Loss=26542.485378202808\n",
            "Iteration 15004, Train Loss=26530.475714954897\n",
            "Iteration 15005, Train Loss=26509.573238780384\n",
            "Iteration 15006, Train Loss=26478.171467720655\n",
            "Iteration 15007, Train Loss=26454.792646851136\n",
            "Iteration 15008, Train Loss=26439.59492957095\n",
            "Iteration 15009, Train Loss=26417.590118838387\n",
            "Iteration 15010, Train Loss=26389.44264757151\n",
            "Iteration 15011, Train Loss=26367.227288642527\n",
            "Iteration 15012, Train Loss=26349.83012313295\n",
            "Iteration 15013, Train Loss=26327.54193701701\n",
            "Iteration 15014, Train Loss=26301.629457680654\n",
            "Iteration 15015, Train Loss=26279.879962423933\n",
            "Iteration 15016, Train Loss=26261.03389211527\n",
            "Iteration 15017, Train Loss=26238.83348776824\n",
            "Iteration 15018, Train Loss=26214.419640770706\n",
            "Iteration 15019, Train Loss=26192.822410961555\n",
            "Iteration 15020, Train Loss=26173.0560963448\n",
            "Iteration 15021, Train Loss=26151.066421401836\n",
            "Iteration 15022, Train Loss=26127.656249978365\n",
            "Iteration 15023, Train Loss=26106.094438803488\n",
            "Iteration 15024, Train Loss=26085.755279178295\n",
            "Iteration 15025, Train Loss=26063.99261494613\n",
            "Iteration 15026, Train Loss=26041.26313475206\n",
            "Iteration 15027, Train Loss=26019.71027995389\n",
            "Iteration 15028, Train Loss=25999.0214114915\n",
            "Iteration 15029, Train Loss=25977.456840698836\n",
            "Iteration 15030, Train Loss=25955.203207864593\n",
            "Iteration 15031, Train Loss=25933.669293542265\n",
            "Iteration 15032, Train Loss=25912.7707549371\n",
            "Iteration 15033, Train Loss=25891.366579094385\n",
            "Iteration 15034, Train Loss=25869.457640013792\n",
            "Iteration 15035, Train Loss=25847.96345551384\n",
            "Iteration 15036, Train Loss=25826.94501078639\n",
            "Iteration 15037, Train Loss=25805.665124186613\n",
            "Iteration 15038, Train Loss=25784.016271289536\n",
            "Iteration 15039, Train Loss=25762.58269278729\n",
            "Iteration 15040, Train Loss=25741.503037023966\n",
            "Iteration 15041, Train Loss=25720.319167593905\n",
            "Iteration 15042, Train Loss=25698.873025527886\n",
            "Iteration 15043, Train Loss=25677.517034552806\n",
            "Iteration 15044, Train Loss=25656.41704451871\n",
            "Iteration 15045, Train Loss=25635.308173528774\n",
            "Iteration 15046, Train Loss=25614.024474823847\n",
            "Iteration 15047, Train Loss=25592.757844720945\n",
            "Iteration 15048, Train Loss=25571.667338527892\n",
            "Iteration 15049, Train Loss=25550.620048024015\n",
            "Iteration 15050, Train Loss=25529.4685939074\n",
            "Iteration 15051, Train Loss=25508.298076715484\n",
            "Iteration 15052, Train Loss=25487.240033213697\n",
            "Iteration 15053, Train Loss=25466.24690458869\n",
            "Iteration 15054, Train Loss=25445.204802078748\n",
            "Iteration 15055, Train Loss=25424.13235169188\n",
            "Iteration 15056, Train Loss=25403.124574326932\n",
            "Iteration 15057, Train Loss=25382.1835852999\n",
            "Iteration 15058, Train Loss=25361.23319592671\n",
            "Iteration 15059, Train Loss=25340.256980990867\n",
            "Iteration 15060, Train Loss=25319.312841220144\n",
            "Iteration 15061, Train Loss=25298.425841766282\n",
            "Iteration 15062, Train Loss=25277.55454845837\n",
            "Iteration 15063, Train Loss=25256.66975854645\n",
            "Iteration 15064, Train Loss=25235.798383089877\n",
            "Iteration 15065, Train Loss=25214.96990495138\n",
            "Iteration 15066, Train Loss=25194.16951185337\n",
            "Iteration 15067, Train Loss=25173.3699167042\n",
            "Iteration 15068, Train Loss=25152.57628646975\n",
            "Iteration 15069, Train Loss=25131.811893119284\n",
            "Iteration 15070, Train Loss=25111.078437860713\n",
            "Iteration 15071, Train Loss=25090.357607817634\n",
            "Iteration 15072, Train Loss=25069.643146682138\n",
            "Iteration 15073, Train Loss=25048.947997077576\n",
            "Iteration 15074, Train Loss=25028.28080593919\n",
            "Iteration 15075, Train Loss=25007.63356867247\n",
            "Iteration 15076, Train Loss=24986.9969660961\n",
            "Iteration 15077, Train Loss=24966.374689180673\n",
            "Iteration 15078, Train Loss=24945.775276201854\n",
            "Iteration 15079, Train Loss=24925.198381430448\n",
            "Iteration 15080, Train Loss=24904.63700256894\n",
            "Iteration 15081, Train Loss=24884.089135883904\n",
            "Iteration 15082, Train Loss=24863.559769501047\n",
            "Iteration 15083, Train Loss=24843.05211010635\n",
            "Iteration 15084, Train Loss=24822.56321387282\n",
            "Iteration 15085, Train Loss=24802.089479784492\n",
            "Iteration 15086, Train Loss=24781.631973304342\n",
            "Iteration 15087, Train Loss=24761.193970095326\n",
            "Iteration 15088, Train Loss=24740.77575263314\n",
            "Iteration 15089, Train Loss=24720.374771905095\n",
            "Iteration 15090, Train Loss=24699.989843264542\n",
            "Iteration 15091, Train Loss=24679.62252460546\n",
            "Iteration 15092, Train Loss=24659.27436433681\n",
            "Iteration 15093, Train Loss=24638.944682982605\n",
            "Iteration 15094, Train Loss=24618.631943670323\n",
            "Iteration 15095, Train Loss=24598.33606724714\n",
            "Iteration 15096, Train Loss=24578.05821820384\n",
            "Iteration 15097, Train Loss=24557.79893550136\n",
            "Iteration 15098, Train Loss=24537.55748500873\n",
            "Iteration 15099, Train Loss=24517.33309596429\n",
            "Iteration 15100, Train Loss=24497.126017194478\n",
            "Iteration 15101, Train Loss=24476.93694341135\n",
            "Iteration 15102, Train Loss=24456.765973251888\n",
            "Iteration 15103, Train Loss=24436.612572027745\n",
            "Iteration 15104, Train Loss=24416.476391459608\n",
            "Iteration 15105, Train Loss=24396.357690902623\n",
            "Iteration 15106, Train Loss=24376.256842062143\n",
            "Iteration 15107, Train Loss=24356.17380065465\n",
            "Iteration 15108, Train Loss=24336.108228030156\n",
            "Iteration 15109, Train Loss=24316.059970680122\n",
            "Iteration 15110, Train Loss=24296.029214490798\n",
            "Iteration 15111, Train Loss=24276.016149134444\n",
            "Iteration 15112, Train Loss=24256.020707423544\n",
            "Iteration 15113, Train Loss=24236.042684976535\n",
            "Iteration 15114, Train Loss=24216.08200701795\n",
            "Iteration 15115, Train Loss=24196.138786990075\n",
            "Iteration 15116, Train Loss=24176.21312104968\n",
            "Iteration 15117, Train Loss=24156.304955214626\n",
            "Iteration 15118, Train Loss=24136.414165888185\n",
            "Iteration 15119, Train Loss=24116.54070706746\n",
            "Iteration 15120, Train Loss=24096.684640058986\n",
            "Iteration 15121, Train Loss=24076.846014028823\n",
            "Iteration 15122, Train Loss=24057.024792968634\n",
            "Iteration 15123, Train Loss=24037.220899926742\n",
            "Iteration 15124, Train Loss=24017.43429905314\n",
            "Iteration 15125, Train Loss=23997.66501805648\n",
            "Iteration 15126, Train Loss=23977.913081395443\n",
            "Iteration 15127, Train Loss=23958.178466850335\n",
            "Iteration 15128, Train Loss=23938.461124699526\n",
            "Iteration 15129, Train Loss=23918.76102356095\n",
            "Iteration 15130, Train Loss=23899.078170447036\n",
            "Iteration 15131, Train Loss=23879.41257538624\n",
            "Iteration 15132, Train Loss=23859.7642246647\n",
            "Iteration 15133, Train Loss=23840.133085011028\n",
            "Iteration 15134, Train Loss=23820.519128438962\n",
            "Iteration 15135, Train Loss=23800.922349655917\n",
            "Iteration 15136, Train Loss=23781.34274916849\n",
            "Iteration 15137, Train Loss=23761.780317440745\n",
            "Iteration 15138, Train Loss=23742.235031388063\n",
            "Iteration 15139, Train Loss=23722.70686659889\n",
            "Iteration 15140, Train Loss=23703.195810955836\n",
            "Iteration 15141, Train Loss=23683.701858217206\n",
            "Iteration 15142, Train Loss=23664.224999952843\n",
            "Iteration 15143, Train Loss=23644.76521921409\n",
            "Iteration 15144, Train Loss=23625.322495310735\n",
            "Iteration 15145, Train Loss=23605.896812974\n",
            "Iteration 15146, Train Loss=23586.48816127354\n",
            "Iteration 15147, Train Loss=23567.09653097354\n",
            "Iteration 15148, Train Loss=23547.721908366962\n",
            "Iteration 15149, Train Loss=23528.364276157063\n",
            "Iteration 15150, Train Loss=23509.023618322804\n",
            "Iteration 15151, Train Loss=23489.69992110059\n",
            "Iteration 15152, Train Loss=23470.3931735802\n",
            "Iteration 15153, Train Loss=23451.103363199687\n",
            "Iteration 15154, Train Loss=23431.83047521383\n",
            "Iteration 15155, Train Loss=23412.574494187123\n",
            "Iteration 15156, Train Loss=23393.335405175763\n",
            "Iteration 15157, Train Loss=23374.113195608024\n",
            "Iteration 15158, Train Loss=23354.90785275236\n",
            "Iteration 15159, Train Loss=23335.71936328506\n",
            "Iteration 15160, Train Loss=23316.54771276418\n",
            "Iteration 15161, Train Loss=23297.39288629574\n",
            "Iteration 15162, Train Loss=23278.254870237848\n",
            "Iteration 15163, Train Loss=23259.133651183893\n",
            "Iteration 15164, Train Loss=23240.029216189607\n",
            "Iteration 15165, Train Loss=23220.941551545395\n",
            "Iteration 15166, Train Loss=23201.870643003458\n",
            "Iteration 15167, Train Loss=23182.81647658835\n",
            "Iteration 15168, Train Loss=23163.779038375716\n",
            "Iteration 15169, Train Loss=23144.75831518782\n",
            "Iteration 15170, Train Loss=23125.754293538525\n",
            "Iteration 15171, Train Loss=23106.766959801418\n",
            "Iteration 15172, Train Loss=23087.796300145586\n",
            "Iteration 15173, Train Loss=23068.842300580727\n",
            "Iteration 15174, Train Loss=23049.90494760338\n",
            "Iteration 15175, Train Loss=23030.984227616456\n",
            "Iteration 15176, Train Loss=23012.080127257555\n",
            "Iteration 15177, Train Loss=22993.19263292163\n",
            "Iteration 15178, Train Loss=22974.321730873522\n",
            "Iteration 15179, Train Loss=22955.467407496482\n",
            "Iteration 15180, Train Loss=22936.629649086746\n",
            "Iteration 15181, Train Loss=22917.80844225624\n",
            "Iteration 15182, Train Loss=22899.003773474025\n",
            "Iteration 15183, Train Loss=22880.21562927051\n",
            "Iteration 15184, Train Loss=22861.44399609765\n",
            "Iteration 15185, Train Loss=22842.688860330712\n",
            "Iteration 15186, Train Loss=22823.950208509814\n",
            "Iteration 15187, Train Loss=22805.22802708938\n",
            "Iteration 15188, Train Loss=22786.522302701032\n",
            "Iteration 15189, Train Loss=22767.83302187449\n",
            "Iteration 15190, Train Loss=22749.160171161755\n",
            "Iteration 15191, Train Loss=22730.503737118455\n",
            "Iteration 15192, Train Loss=22711.863706246877\n",
            "Iteration 15193, Train Loss=22693.24006518109\n",
            "Iteration 15194, Train Loss=22674.632800481366\n",
            "Iteration 15195, Train Loss=22656.041898816788\n",
            "Iteration 15196, Train Loss=22637.467346798494\n",
            "Iteration 15197, Train Loss=22618.909131053864\n",
            "Iteration 15198, Train Loss=22600.36723823456\n",
            "Iteration 15199, Train Loss=22581.841654956\n",
            "Iteration 15200, Train Loss=22563.33236792699\n",
            "Iteration 15201, Train Loss=22544.839363802566\n",
            "Iteration 15202, Train Loss=22526.362629314048\n",
            "Iteration 15203, Train Loss=22507.902151159884\n",
            "Iteration 15204, Train Loss=22489.45791605689\n",
            "Iteration 15205, Train Loss=22471.029910744954\n",
            "Iteration 15206, Train Loss=22452.61812194179\n",
            "Iteration 15207, Train Loss=22434.22253643118\n",
            "Iteration 15208, Train Loss=22415.843140960227\n",
            "Iteration 15209, Train Loss=22397.47992233506\n",
            "Iteration 15210, Train Loss=22379.132867341083\n",
            "Iteration 15211, Train Loss=22360.801962786638\n",
            "Iteration 15212, Train Loss=22342.487195495203\n",
            "Iteration 15213, Train Loss=22324.188552281714\n",
            "Iteration 15214, Train Loss=22305.906020006743\n",
            "Iteration 15215, Train Loss=22287.63958550839\n",
            "Iteration 15216, Train Loss=22269.38923567164\n",
            "Iteration 15217, Train Loss=22251.1549573669\n",
            "Iteration 15218, Train Loss=22232.936737492353\n",
            "Iteration 15219, Train Loss=22214.734562951817\n",
            "Iteration 15220, Train Loss=22196.548420654588\n",
            "Iteration 15221, Train Loss=22178.37829753763\n",
            "Iteration 15222, Train Loss=22160.22418052872\n",
            "Iteration 15223, Train Loss=22142.086056591965\n",
            "Iteration 15224, Train Loss=22123.963912680636\n",
            "Iteration 15225, Train Loss=22105.857735777958\n",
            "Iteration 15226, Train Loss=22087.76751286708\n",
            "Iteration 15227, Train Loss=22069.693230946497\n",
            "Iteration 15228, Train Loss=22051.634877028162\n",
            "Iteration 15229, Train Loss=22033.592438127664\n",
            "Iteration 15230, Train Loss=22015.565901284106\n",
            "Iteration 15231, Train Loss=21997.555253533872\n",
            "Iteration 15232, Train Loss=21979.5604819388\n",
            "Iteration 15233, Train Loss=21961.581573559128\n",
            "Iteration 15234, Train Loss=21943.618515476446\n",
            "Iteration 15235, Train Loss=21925.671294775944\n",
            "Iteration 15236, Train Loss=21907.739898556036\n",
            "Iteration 15237, Train Loss=21889.824313928217\n",
            "Iteration 15238, Train Loss=21871.924528008545\n",
            "Iteration 15239, Train Loss=21854.040527931273\n",
            "Iteration 15240, Train Loss=21836.1723008328\n",
            "Iteration 15241, Train Loss=21818.319833868663\n",
            "Iteration 15242, Train Loss=21800.483114195893\n",
            "Iteration 15243, Train Loss=21782.662128988894\n",
            "Iteration 15244, Train Loss=21764.856865426857\n",
            "Iteration 15245, Train Loss=21747.06731070176\n",
            "Iteration 15246, Train Loss=21729.293452014674\n",
            "Iteration 15247, Train Loss=21711.535276574177\n",
            "Iteration 15248, Train Loss=21693.792771602653\n",
            "Iteration 15249, Train Loss=21676.065924325874\n",
            "Iteration 15250, Train Loss=21658.354721985575\n",
            "Iteration 15251, Train Loss=21640.65915182657\n",
            "Iteration 15252, Train Loss=21622.97920110762\n",
            "Iteration 15253, Train Loss=21605.3148570925\n",
            "Iteration 15254, Train Loss=21587.66610705606\n",
            "Iteration 15255, Train Loss=21570.032938280485\n",
            "Iteration 15256, Train Loss=21552.41533805696\n",
            "Iteration 15257, Train Loss=21534.81329368548\n",
            "Iteration 15258, Train Loss=21517.226792472775\n",
            "Iteration 15259, Train Loss=21499.65582173533\n",
            "Iteration 15260, Train Loss=21482.100368796047\n",
            "Iteration 15261, Train Loss=21464.560420987098\n",
            "Iteration 15262, Train Loss=21447.035965646493\n",
            "Iteration 15263, Train Loss=21429.52699012223\n",
            "Iteration 15264, Train Loss=21412.03348176633\n",
            "Iteration 15265, Train Loss=21394.555427941486\n",
            "Iteration 15266, Train Loss=21377.092816014378\n",
            "Iteration 15267, Train Loss=21359.645633361604\n",
            "Iteration 15268, Train Loss=21342.213867364128\n",
            "Iteration 15269, Train Loss=21324.797505410108\n",
            "Iteration 15270, Train Loss=21307.39653489553\n",
            "Iteration 15271, Train Loss=21290.010943220794\n",
            "Iteration 15272, Train Loss=21272.640717794056\n",
            "Iteration 15273, Train Loss=21255.28584602786\n",
            "Iteration 15274, Train Loss=21237.94631534337\n",
            "Iteration 15275, Train Loss=21220.62211316388\n",
            "Iteration 15276, Train Loss=21203.31322692192\n",
            "Iteration 15277, Train Loss=21186.01964405227\n",
            "Iteration 15278, Train Loss=21168.741351997283\n",
            "Iteration 15279, Train Loss=21151.47833820369\n",
            "Iteration 15280, Train Loss=21134.230590123207\n",
            "Iteration 15281, Train Loss=21116.998095211973\n",
            "Iteration 15282, Train Loss=21099.780840932075\n",
            "Iteration 15283, Train Loss=21082.578814748493\n",
            "Iteration 15284, Train Loss=21065.39200413267\n",
            "Iteration 15285, Train Loss=21048.22039655801\n",
            "Iteration 15286, Train Loss=21031.0639795041\n",
            "Iteration 15287, Train Loss=21013.922740453214\n",
            "Iteration 15288, Train Loss=20996.79666689206\n",
            "Iteration 15289, Train Loss=20979.685746310955\n",
            "Iteration 15290, Train Loss=20962.589966203566\n",
            "Iteration 15291, Train Loss=20945.509314066694\n",
            "Iteration 15292, Train Loss=20928.443777401528\n",
            "Iteration 15293, Train Loss=20911.393343710668\n",
            "Iteration 15294, Train Loss=20894.35800050195\n",
            "Iteration 15295, Train Loss=20877.33773528358\n",
            "Iteration 15296, Train Loss=20860.332535568523\n",
            "Iteration 15297, Train Loss=20843.34238887039\n",
            "Iteration 15298, Train Loss=20826.36728270764\n",
            "Iteration 15299, Train Loss=20809.407204597963\n",
            "Iteration 15300, Train Loss=20792.462142064713\n",
            "Iteration 15301, Train Loss=20775.532082629506\n",
            "Iteration 15302, Train Loss=20758.61701381936\n",
            "Iteration 15303, Train Loss=20741.716923160322\n",
            "Iteration 15304, Train Loss=20724.83179818266\n",
            "Iteration 15305, Train Loss=20707.9616264145\n",
            "Iteration 15306, Train Loss=20691.106395389957\n",
            "Iteration 15307, Train Loss=20674.266092638853\n",
            "Iteration 15308, Train Loss=20657.440705699264\n",
            "Iteration 15309, Train Loss=20640.6302221018\n",
            "Iteration 15310, Train Loss=20623.834629387584\n",
            "Iteration 15311, Train Loss=20607.053915086864\n",
            "Iteration 15312, Train Loss=20590.288066744797\n",
            "Iteration 15313, Train Loss=20573.53707189022\n",
            "Iteration 15314, Train Loss=20556.8009180734\n",
            "Iteration 15315, Train Loss=20540.07959281876\n",
            "Iteration 15316, Train Loss=20523.373083684113\n",
            "Iteration 15317, Train Loss=20506.681378188026\n",
            "Iteration 15318, Train Loss=20490.004463899117\n",
            "Iteration 15319, Train Loss=20473.34232832698\n",
            "Iteration 15320, Train Loss=20456.694959060344\n",
            "Iteration 15321, Train Loss=20440.062343598016\n",
            "Iteration 15322, Train Loss=20423.444469567105\n",
            "Iteration 15323, Train Loss=20406.841324459772\n",
            "Iteration 15324, Train Loss=20390.252895988033\n",
            "Iteration 15325, Train Loss=20373.67917166717\n",
            "Iteration 15326, Train Loss=20357.120139416023\n",
            "Iteration 15327, Train Loss=20340.575786899644\n",
            "Iteration 15328, Train Loss=20324.046102590375\n",
            "Iteration 15329, Train Loss=20307.53107477048\n",
            "Iteration 15330, Train Loss=20291.030693525972\n",
            "Iteration 15331, Train Loss=20274.544949409556\n",
            "Iteration 15332, Train Loss=20258.07383754669\n",
            "Iteration 15333, Train Loss=20241.61735657755\n",
            "Iteration 15334, Train Loss=20225.175518207085\n",
            "Iteration 15335, Train Loss=20208.748349827285\n",
            "Iteration 15336, Train Loss=20192.335919993297\n",
            "Iteration 15337, Train Loss=20175.938359451837\n",
            "Iteration 15338, Train Loss=20159.555938473357\n",
            "Iteration 15339, Train Loss=20143.18916594212\n",
            "Iteration 15340, Train Loss=20126.839049474387\n",
            "Iteration 15341, Train Loss=20110.507510494142\n",
            "Iteration 15342, Train Loss=20094.198325942816\n",
            "Iteration 15343, Train Loss=20077.91881237209\n",
            "Iteration 15344, Train Loss=20061.683415501924\n",
            "Iteration 15345, Train Loss=20045.520528958794\n",
            "Iteration 15346, Train Loss=20029.4866971726\n",
            "Iteration 15347, Train Loss=20013.694466686604\n",
            "Iteration 15348, Train Loss=19998.370118710918\n",
            "Iteration 15349, Train Loss=19983.968279054006\n",
            "Iteration 15350, Train Loss=19971.409258753516\n",
            "Iteration 15351, Train Loss=19962.542754228423\n",
            "Iteration 15352, Train Loss=19961.085219360542\n",
            "Iteration 15353, Train Loss=19974.25975697417\n",
            "Iteration 15354, Train Loss=20015.562342671263\n",
            "Iteration 15355, Train Loss=20106.59031158054\n",
            "Iteration 15356, Train Loss=20272.06183267257\n",
            "Iteration 15357, Train Loss=20501.35203717912\n",
            "Iteration 15358, Train Loss=20675.630375569788\n",
            "Iteration 15359, Train Loss=20559.85032085724\n",
            "Iteration 15360, Train Loss=20133.808993956576\n",
            "Iteration 15361, Train Loss=19801.968855745577\n",
            "Iteration 15362, Train Loss=19881.63638286025\n",
            "Iteration 15363, Train Loss=20133.156918556822\n",
            "Iteration 15364, Train Loss=20122.11864065215\n",
            "Iteration 15365, Train Loss=19844.39500063854\n",
            "Iteration 15366, Train Loss=19711.79819817652\n",
            "Iteration 15367, Train Loss=19851.50674462159\n",
            "Iteration 15368, Train Loss=19934.597101746225\n",
            "Iteration 15369, Train Loss=19781.223356842627\n",
            "Iteration 15370, Train Loss=19645.45192517973\n",
            "Iteration 15371, Train Loss=19710.826697552584\n",
            "Iteration 15372, Train Loss=19779.066939501052\n",
            "Iteration 15373, Train Loss=19684.073239196412\n",
            "Iteration 15374, Train Loss=19583.06896935478\n",
            "Iteration 15375, Train Loss=19618.20109416483\n",
            "Iteration 15376, Train Loss=19659.00050726207\n",
            "Iteration 15377, Train Loss=19589.601891131897\n",
            "Iteration 15378, Train Loss=19519.935093393928\n",
            "Iteration 15379, Train Loss=19541.278905841617\n",
            "Iteration 15380, Train Loss=19559.552436396105\n",
            "Iteration 15381, Train Loss=19504.799354468494\n",
            "Iteration 15382, Train Loss=19457.493950019933\n",
            "Iteration 15383, Train Loss=19469.903839997212\n",
            "Iteration 15384, Train Loss=19472.923344705858\n",
            "Iteration 15385, Train Loss=19428.583811012817\n",
            "Iteration 15386, Train Loss=19395.759835880825\n",
            "Iteration 15387, Train Loss=19400.993989105184\n",
            "Iteration 15388, Train Loss=19394.561145940068\n",
            "Iteration 15389, Train Loss=19358.48106808168\n",
            "Iteration 15390, Train Loss=19334.304682324153\n",
            "Iteration 15391, Train Loss=19333.65048817012\n",
            "Iteration 15392, Train Loss=19322.014288016904\n",
            "Iteration 15393, Train Loss=19292.28442048358\n",
            "Iteration 15394, Train Loss=19272.876253553615\n",
            "Iteration 15395, Train Loss=19267.680274955124\n",
            "Iteration 15396, Train Loss=19253.463779247653\n",
            "Iteration 15397, Train Loss=19228.427553281028\n",
            "Iteration 15398, Train Loss=19211.434311437726\n",
            "Iteration 15399, Train Loss=19202.942669005497\n",
            "Iteration 15400, Train Loss=19187.675725574296\n",
            "Iteration 15401, Train Loss=19165.936247855327\n",
            "Iteration 15402, Train Loss=19150.05379797964\n",
            "Iteration 15403, Train Loss=19139.301560916887\n",
            "Iteration 15404, Train Loss=19123.742417885234\n",
            "Iteration 15405, Train Loss=19104.25185128965\n",
            "Iteration 15406, Train Loss=19088.82122980612\n",
            "Iteration 15407, Train Loss=19076.57190989743\n",
            "Iteration 15408, Train Loss=19061.06400045966\n",
            "Iteration 15409, Train Loss=19043.077299633693\n",
            "Iteration 15410, Train Loss=19027.801355149517\n",
            "Iteration 15411, Train Loss=19014.586181964423\n",
            "Iteration 15412, Train Loss=18999.241055713774\n",
            "Iteration 15413, Train Loss=18982.260433278123\n",
            "Iteration 15414, Train Loss=18967.026728656732\n",
            "Iteration 15415, Train Loss=18953.194391027337\n",
            "Iteration 15416, Train Loss=18938.0259683955\n",
            "Iteration 15417, Train Loss=18921.72595149368\n",
            "Iteration 15418, Train Loss=18906.50741012304\n",
            "Iteration 15419, Train Loss=18892.283086083004\n",
            "Iteration 15420, Train Loss=18877.263671659897\n",
            "Iteration 15421, Train Loss=18861.435827928806\n",
            "Iteration 15422, Train Loss=18846.239886460844\n",
            "Iteration 15423, Train Loss=18831.76719767098\n",
            "Iteration 15424, Train Loss=18816.861346086276\n",
            "Iteration 15425, Train Loss=18801.370327997196\n",
            "Iteration 15426, Train Loss=18786.214837746087\n",
            "Iteration 15427, Train Loss=18771.587624148444\n",
            "Iteration 15428, Train Loss=18756.762051882768\n",
            "Iteration 15429, Train Loss=18741.518504837488\n",
            "Iteration 15430, Train Loss=18726.421381781845\n",
            "Iteration 15431, Train Loss=18711.702932085\n",
            "Iteration 15432, Train Loss=18696.932047811297\n",
            "Iteration 15433, Train Loss=18681.874087017077\n",
            "Iteration 15434, Train Loss=18666.849298692752\n",
            "Iteration 15435, Train Loss=18652.084721976767\n",
            "Iteration 15436, Train Loss=18637.35059085115\n",
            "Iteration 15437, Train Loss=18622.433745783997\n",
            "Iteration 15438, Train Loss=18607.48993739766\n",
            "Iteration 15439, Train Loss=18592.71280626108\n",
            "Iteration 15440, Train Loss=18578.00514909104\n",
            "Iteration 15441, Train Loss=18563.195996473805\n",
            "Iteration 15442, Train Loss=18548.336517875756\n",
            "Iteration 15443, Train Loss=18533.57256594513\n",
            "Iteration 15444, Train Loss=18518.8875010343\n",
            "Iteration 15445, Train Loss=18504.1608584335\n",
            "Iteration 15446, Train Loss=18489.384195862964\n",
            "Iteration 15447, Train Loss=18474.652887954806\n",
            "Iteration 15448, Train Loss=18459.991847758807\n",
            "Iteration 15449, Train Loss=18445.32912321855\n",
            "Iteration 15450, Train Loss=18430.630036258546\n",
            "Iteration 15451, Train Loss=18415.945190612303\n",
            "Iteration 15452, Train Loss=18401.313280475122\n",
            "Iteration 15453, Train Loss=18386.701983551844\n",
            "Iteration 15454, Train Loss=18372.07282115401\n",
            "Iteration 15455, Train Loss=18357.44291160895\n",
            "Iteration 15456, Train Loss=18342.847191411012\n",
            "Iteration 15457, Train Loss=18328.280304357635\n",
            "Iteration 15458, Train Loss=18313.712769870377\n",
            "Iteration 15459, Train Loss=18299.14137823065\n",
            "Iteration 15460, Train Loss=18284.589002863257\n",
            "Iteration 15461, Train Loss=18270.064227482762\n",
            "Iteration 15462, Train Loss=18255.550943028175\n",
            "Iteration 15463, Train Loss=18241.037772011176\n",
            "Iteration 15464, Train Loss=18226.53441994457\n",
            "Iteration 15465, Train Loss=18212.052817973905\n",
            "Iteration 15466, Train Loss=18197.588624273532\n",
            "Iteration 15467, Train Loss=18183.13091471338\n",
            "Iteration 15468, Train Loss=18168.679834910155\n",
            "Iteration 15469, Train Loss=18154.244178140205\n",
            "Iteration 15470, Train Loss=18139.826597417887\n",
            "Iteration 15471, Train Loss=18125.42085166387\n",
            "Iteration 15472, Train Loss=18111.022731910554\n",
            "Iteration 15473, Train Loss=18096.635838259772\n",
            "Iteration 15474, Train Loss=18082.264747560275\n",
            "Iteration 15475, Train Loss=18067.90813374732\n",
            "Iteration 15476, Train Loss=18053.56183208556\n",
            "Iteration 15477, Train Loss=18039.225393316505\n",
            "Iteration 15478, Train Loss=18024.901990865368\n",
            "Iteration 15479, Train Loss=18010.59313291884\n",
            "Iteration 15480, Train Loss=17996.296819030995\n",
            "Iteration 15481, Train Loss=17982.01105878444\n",
            "Iteration 15482, Train Loss=17967.736665351968\n",
            "Iteration 15483, Train Loss=17953.47554660706\n",
            "Iteration 15484, Train Loss=17939.227789187502\n",
            "Iteration 15485, Train Loss=17924.991878885707\n",
            "Iteration 15486, Train Loss=17910.767109721513\n",
            "Iteration 15487, Train Loss=17896.554381817004\n",
            "Iteration 15488, Train Loss=17882.354631405986\n",
            "Iteration 15489, Train Loss=17868.167518564045\n",
            "Iteration 15490, Train Loss=17853.992126768244\n",
            "Iteration 15491, Train Loss=17839.82831150996\n",
            "Iteration 15492, Train Loss=17825.67672430302\n",
            "Iteration 15493, Train Loss=17811.537764233093\n",
            "Iteration 15494, Train Loss=17797.41107455415\n",
            "Iteration 15495, Train Loss=17783.29615249458\n",
            "Iteration 15496, Train Loss=17769.193034565476\n",
            "Iteration 15497, Train Loss=17755.102123425466\n",
            "Iteration 15498, Train Loss=17741.023571963433\n",
            "Iteration 15499, Train Loss=17726.957116163096\n",
            "Iteration 15500, Train Loss=17712.902486488885\n",
            "Iteration 15501, Train Loss=17698.859746782742\n",
            "Iteration 15502, Train Loss=17684.829128708087\n",
            "Iteration 15503, Train Loss=17670.81068746708\n",
            "Iteration 15504, Train Loss=17656.80425099904\n",
            "Iteration 15505, Train Loss=17642.80966911939\n",
            "Iteration 15506, Train Loss=17628.826987175267\n",
            "Iteration 15507, Train Loss=17614.856334019434\n",
            "Iteration 15508, Train Loss=17600.89773084111\n",
            "Iteration 15509, Train Loss=17586.951071659387\n",
            "Iteration 15510, Train Loss=17573.016265710867\n",
            "Iteration 15511, Train Loss=17559.09333421809\n",
            "Iteration 15512, Train Loss=17545.18234686373\n",
            "Iteration 15513, Train Loss=17531.283314216835\n",
            "Iteration 15514, Train Loss=17517.396172711487\n",
            "Iteration 15515, Train Loss=17503.52086241198\n",
            "Iteration 15516, Train Loss=17489.657386115658\n",
            "Iteration 15517, Train Loss=17475.805779072387\n",
            "Iteration 15518, Train Loss=17461.966048092632\n",
            "Iteration 15519, Train Loss=17448.138155657656\n",
            "Iteration 15520, Train Loss=17434.32205960302\n",
            "Iteration 15521, Train Loss=17420.51775122696\n",
            "Iteration 15522, Train Loss=17406.72524498431\n",
            "Iteration 15523, Train Loss=17392.944544957467\n",
            "Iteration 15524, Train Loss=17379.17562913035\n",
            "Iteration 15525, Train Loss=17365.418467023213\n",
            "Iteration 15526, Train Loss=17351.67304403649\n",
            "Iteration 15527, Train Loss=17337.93936165168\n",
            "Iteration 15528, Train Loss=17324.2174206953\n",
            "Iteration 15529, Train Loss=17310.507207842093\n",
            "Iteration 15530, Train Loss=17296.808700998656\n",
            "Iteration 15531, Train Loss=17283.121883631433\n",
            "Iteration 15532, Train Loss=17269.446749251645\n",
            "Iteration 15533, Train Loss=17255.783294946283\n",
            "Iteration 15534, Train Loss=17242.131511525666\n",
            "Iteration 15535, Train Loss=17228.491382927186\n",
            "Iteration 15536, Train Loss=17214.86289319194\n",
            "Iteration 15537, Train Loss=17201.2460314358\n",
            "Iteration 15538, Train Loss=17187.640791115937\n",
            "Iteration 15539, Train Loss=17174.04716417071\n",
            "Iteration 15540, Train Loss=17160.465138447984\n",
            "Iteration 15541, Train Loss=17146.894699792127\n",
            "Iteration 15542, Train Loss=17133.33583557528\n",
            "Iteration 15543, Train Loss=17119.7885363939\n",
            "Iteration 15544, Train Loss=17106.252793573774\n",
            "Iteration 15545, Train Loss=17092.72859692641\n",
            "Iteration 15546, Train Loss=17079.215934239834\n",
            "Iteration 15547, Train Loss=17065.714792909286\n",
            "Iteration 15548, Train Loss=17052.225161863033\n",
            "Iteration 15549, Train Loss=17038.74703119753\n",
            "Iteration 15550, Train Loss=17025.280391151773\n",
            "Iteration 15551, Train Loss=17011.82523085071\n",
            "Iteration 15552, Train Loss=16998.38153853226\n",
            "Iteration 15553, Train Loss=16984.949302639627\n",
            "Iteration 15554, Train Loss=16971.528512284538\n",
            "Iteration 15555, Train Loss=16958.119157282094\n",
            "Iteration 15556, Train Loss=16944.72122722905\n",
            "Iteration 15557, Train Loss=16931.334711196185\n",
            "Iteration 15558, Train Loss=16917.95959789446\n",
            "Iteration 15559, Train Loss=16904.595876088657\n",
            "Iteration 15560, Train Loss=16891.243535044043\n",
            "Iteration 15561, Train Loss=16877.90256421363\n",
            "Iteration 15562, Train Loss=16864.57295303737\n",
            "Iteration 15563, Train Loss=16851.254690651207\n",
            "Iteration 15564, Train Loss=16837.947765988465\n",
            "Iteration 15565, Train Loss=16824.652168084194\n",
            "Iteration 15566, Train Loss=16811.36788611282\n",
            "Iteration 15567, Train Loss=16798.094909462678\n",
            "Iteration 15568, Train Loss=16784.833227459407\n",
            "Iteration 15569, Train Loss=16771.58282932286\n",
            "Iteration 15570, Train Loss=16758.34370417409\n",
            "Iteration 15571, Train Loss=16745.11584111343\n",
            "Iteration 15572, Train Loss=16731.89922937684\n",
            "Iteration 15573, Train Loss=16718.693858245308\n",
            "Iteration 15574, Train Loss=16705.499717056293\n",
            "Iteration 15575, Train Loss=16692.316795076324\n",
            "Iteration 15576, Train Loss=16679.14508151983\n",
            "Iteration 15577, Train Loss=16665.98456560133\n",
            "Iteration 15578, Train Loss=16652.835236543448\n",
            "Iteration 15579, Train Loss=16639.697083653242\n",
            "Iteration 15580, Train Loss=16626.570096236428\n",
            "Iteration 15581, Train Loss=16613.454263615327\n",
            "Iteration 15582, Train Loss=16600.3495750773\n",
            "Iteration 15583, Train Loss=16587.256019891167\n",
            "Iteration 15584, Train Loss=16574.17358734737\n",
            "Iteration 15585, Train Loss=16561.102266740658\n",
            "Iteration 15586, Train Loss=16548.04204741256\n",
            "Iteration 15587, Train Loss=16534.992918696564\n",
            "Iteration 15588, Train Loss=16521.95486993833\n",
            "Iteration 15589, Train Loss=16508.92789046997\n",
            "Iteration 15590, Train Loss=16495.911969619097\n",
            "Iteration 15591, Train Loss=16482.90709673238\n",
            "Iteration 15592, Train Loss=16469.913261157057\n",
            "Iteration 15593, Train Loss=16456.930452269957\n",
            "Iteration 15594, Train Loss=16443.958659443462\n",
            "Iteration 15595, Train Loss=16430.997872061016\n",
            "Iteration 15596, Train Loss=16418.04807950387\n",
            "Iteration 15597, Train Loss=16405.109271154266\n",
            "Iteration 15598, Train Loss=16392.18143640773\n",
            "Iteration 15599, Train Loss=16379.264564660829\n",
            "Iteration 15600, Train Loss=16366.358645330352\n",
            "Iteration 15601, Train Loss=16353.463667832624\n",
            "Iteration 15602, Train Loss=16340.579621595545\n",
            "Iteration 15603, Train Loss=16327.706496048882\n",
            "Iteration 15604, Train Loss=16314.844280626934\n",
            "Iteration 15605, Train Loss=16301.992964773259\n",
            "Iteration 15606, Train Loss=16289.152537934413\n",
            "Iteration 15607, Train Loss=16276.322989571205\n",
            "Iteration 15608, Train Loss=16263.504309146252\n",
            "Iteration 15609, Train Loss=16250.696486135357\n",
            "Iteration 15610, Train Loss=16237.89951001568\n",
            "Iteration 15611, Train Loss=16225.113370273863\n",
            "Iteration 15612, Train Loss=16212.338056401484\n",
            "Iteration 15613, Train Loss=16199.573557896243\n",
            "Iteration 15614, Train Loss=16186.819864266721\n",
            "Iteration 15615, Train Loss=16174.076965024438\n",
            "Iteration 15616, Train Loss=16161.34484969321\n",
            "Iteration 15617, Train Loss=16148.623507800123\n",
            "Iteration 15618, Train Loss=16135.912928881933\n",
            "Iteration 15619, Train Loss=16123.213102481619\n",
            "Iteration 15620, Train Loss=16110.524018148659\n",
            "Iteration 15621, Train Loss=16097.845665440434\n",
            "Iteration 15622, Train Loss=16085.178033920669\n",
            "Iteration 15623, Train Loss=16072.521113162977\n",
            "Iteration 15624, Train Loss=16059.874892745522\n",
            "Iteration 15625, Train Loss=16047.239362256913\n",
            "Iteration 15626, Train Loss=16034.61451129035\n",
            "Iteration 15627, Train Loss=16022.000329448794\n",
            "Iteration 15628, Train Loss=16009.396806341529\n",
            "Iteration 15629, Train Loss=15996.80393158564\n",
            "Iteration 15630, Train Loss=15984.221694805485\n",
            "Iteration 15631, Train Loss=15971.650085633266\n",
            "Iteration 15632, Train Loss=15959.089093709574\n",
            "Iteration 15633, Train Loss=15946.53870868077\n",
            "Iteration 15634, Train Loss=15933.998920203821\n",
            "Iteration 15635, Train Loss=15921.469717940607\n",
            "Iteration 15636, Train Loss=15908.95109156246\n",
            "Iteration 15637, Train Loss=15896.443030748418\n",
            "Iteration 15638, Train Loss=15883.945525184425\n",
            "Iteration 15639, Train Loss=15871.458564565628\n",
            "Iteration 15640, Train Loss=15858.982138593605\n",
            "Iteration 15641, Train Loss=15846.51623697934\n",
            "Iteration 15642, Train Loss=15834.060849439873\n",
            "Iteration 15643, Train Loss=15821.615965702837\n",
            "Iteration 15644, Train Loss=15809.181575501618\n",
            "Iteration 15645, Train Loss=15796.757668578657\n",
            "Iteration 15646, Train Loss=15784.344234684995\n",
            "Iteration 15647, Train Loss=15771.9412635783\n",
            "Iteration 15648, Train Loss=15759.548745025964\n",
            "Iteration 15649, Train Loss=15747.16666880294\n",
            "Iteration 15650, Train Loss=15734.795024692192\n",
            "Iteration 15651, Train Loss=15722.433802484858\n",
            "Iteration 15652, Train Loss=15710.082991981177\n",
            "Iteration 15653, Train Loss=15697.742582988978\n",
            "Iteration 15654, Train Loss=15685.412565324601\n",
            "Iteration 15655, Train Loss=15673.092928812734\n",
            "Iteration 15656, Train Loss=15660.783663286851\n",
            "Iteration 15657, Train Loss=15648.48475858803\n",
            "Iteration 15658, Train Loss=15636.19620456736\n",
            "Iteration 15659, Train Loss=15623.917991082586\n",
            "Iteration 15660, Train Loss=15611.650108001504\n",
            "Iteration 15661, Train Loss=15599.392545199511\n",
            "Iteration 15662, Train Loss=15587.145292561236\n",
            "Iteration 15663, Train Loss=15574.908339979873\n",
            "Iteration 15664, Train Loss=15562.681677356803\n",
            "Iteration 15665, Train Loss=15550.465294602682\n",
            "Iteration 15666, Train Loss=15538.259181636453\n",
            "Iteration 15667, Train Loss=15526.063328386601\n",
            "Iteration 15668, Train Loss=15513.877724789127\n",
            "Iteration 15669, Train Loss=15501.702360790066\n",
            "Iteration 15670, Train Loss=15489.537226343735\n",
            "Iteration 15671, Train Loss=15477.382311413687\n",
            "Iteration 15672, Train Loss=15465.237605971928\n",
            "Iteration 15673, Train Loss=15453.103099999793\n",
            "Iteration 15674, Train Loss=15440.978783487804\n",
            "Iteration 15675, Train Loss=15428.864646434839\n",
            "Iteration 15676, Train Loss=15416.760678849201\n",
            "Iteration 15677, Train Loss=15404.666870748391\n",
            "Iteration 15678, Train Loss=15392.583212158954\n",
            "Iteration 15679, Train Loss=15380.509693116805\n",
            "Iteration 15680, Train Loss=15368.446303666427\n",
            "Iteration 15681, Train Loss=15356.393033862125\n",
            "Iteration 15682, Train Loss=15344.349873766967\n",
            "Iteration 15683, Train Loss=15332.31681345356\n",
            "Iteration 15684, Train Loss=15320.293843003863\n",
            "Iteration 15685, Train Loss=15308.280952508925\n",
            "Iteration 15686, Train Loss=15296.278132069332\n",
            "Iteration 15687, Train Loss=15284.285371795175\n",
            "Iteration 15688, Train Loss=15272.30266180552\n",
            "Iteration 15689, Train Loss=15260.329992229419\n",
            "Iteration 15690, Train Loss=15248.367353205025\n",
            "Iteration 15691, Train Loss=15236.414734880247\n",
            "Iteration 15692, Train Loss=15224.472127412097\n",
            "Iteration 15693, Train Loss=15212.539520967799\n",
            "Iteration 15694, Train Loss=15200.616905723617\n",
            "Iteration 15695, Train Loss=15188.7042718658\n",
            "Iteration 15696, Train Loss=15176.801609589882\n",
            "Iteration 15697, Train Loss=15164.908909101989\n",
            "Iteration 15698, Train Loss=15153.026160616218\n",
            "Iteration 15699, Train Loss=15141.15335435871\n",
            "Iteration 15700, Train Loss=15129.290480563068\n",
            "Iteration 15701, Train Loss=15117.437529474486\n",
            "Iteration 15702, Train Loss=15105.594491346816\n",
            "Iteration 15703, Train Loss=15093.76135644581\n",
            "Iteration 15704, Train Loss=15081.938115044137\n",
            "Iteration 15705, Train Loss=15070.124757428513\n",
            "Iteration 15706, Train Loss=15058.321273891885\n",
            "Iteration 15707, Train Loss=15046.527654743175\n",
            "Iteration 15708, Train Loss=15034.743890296004\n",
            "Iteration 15709, Train Loss=15022.969970885064\n",
            "Iteration 15710, Train Loss=15011.205886848498\n",
            "Iteration 15711, Train Loss=14999.451628553981\n",
            "Iteration 15712, Train Loss=14987.707186375366\n",
            "Iteration 15713, Train Loss=14975.972550735436\n",
            "Iteration 15714, Train Loss=14964.247712076703\n",
            "Iteration 15715, Train Loss=14952.532660940255\n",
            "Iteration 15716, Train Loss=14940.827387941352\n",
            "Iteration 15717, Train Loss=14929.131883930924\n",
            "Iteration 15718, Train Loss=14917.446140024515\n",
            "Iteration 15719, Train Loss=14905.770147984153\n",
            "Iteration 15720, Train Loss=14894.103900481918\n",
            "Iteration 15721, Train Loss=14882.447392128019\n",
            "Iteration 15722, Train Loss=14870.800620637907\n",
            "Iteration 15723, Train Loss=14859.163589898259\n",
            "Iteration 15724, Train Loss=14847.536314438748\n",
            "Iteration 15725, Train Loss=14835.918829320994\n",
            "Iteration 15726, Train Loss=14824.311206595108\n",
            "Iteration 15727, Train Loss=14812.71358901825\n",
            "Iteration 15728, Train Loss=14801.12625035375\n",
            "Iteration 15729, Train Loss=14789.549715171155\n",
            "Iteration 15730, Train Loss=14777.984982104623\n",
            "Iteration 15731, Train Loss=14766.433963674179\n",
            "Iteration 15732, Train Loss=14754.900327100555\n",
            "Iteration 15733, Train Loss=14743.391155110929\n",
            "Iteration 15734, Train Loss=14731.920175598318\n",
            "Iteration 15735, Train Loss=14720.51419433689\n",
            "Iteration 15736, Train Loss=14709.225750443953\n",
            "Iteration 15737, Train Loss=14698.15860674007\n",
            "Iteration 15738, Train Loss=14687.518182364449\n",
            "Iteration 15739, Train Loss=14677.71427656039\n",
            "Iteration 15740, Train Loss=14669.562879493495\n",
            "Iteration 15741, Train Loss=14664.698141329462\n",
            "Iteration 15742, Train Loss=14666.343681632785\n",
            "Iteration 15743, Train Loss=14680.812005353004\n",
            "Iteration 15744, Train Loss=14719.703442387847\n",
            "Iteration 15745, Train Loss=14802.642619829849\n",
            "Iteration 15746, Train Loss=14953.196449040315\n",
            "Iteration 15747, Train Loss=15175.583242890809\n",
            "Iteration 15748, Train Loss=15380.856199322612\n",
            "Iteration 15749, Train Loss=15361.929108522847\n",
            "Iteration 15750, Train Loss=15000.057353573531\n",
            "Iteration 15751, Train Loss=14603.206628018397\n",
            "Iteration 15752, Train Loss=14570.07468951554\n",
            "Iteration 15753, Train Loss=14819.13041030872\n",
            "Iteration 15754, Train Loss=14923.311051630255\n",
            "Iteration 15755, Train Loss=14704.529190123023\n",
            "Iteration 15756, Train Loss=14488.6394972238\n",
            "Iteration 15757, Train Loss=14557.486279488792\n",
            "Iteration 15758, Train Loss=14705.109497899077\n",
            "Iteration 15759, Train Loss=14637.34094066195\n",
            "Iteration 15760, Train Loss=14462.529321565129\n",
            "Iteration 15761, Train Loss=14455.300584200415\n",
            "Iteration 15762, Train Loss=14559.010084384368\n",
            "Iteration 15763, Train Loss=14538.263863964437\n",
            "Iteration 15764, Train Loss=14417.653812384873\n",
            "Iteration 15765, Train Loss=14395.219569891511\n",
            "Iteration 15766, Train Loss=14460.343184867326\n",
            "Iteration 15767, Train Loss=14449.467124597188\n",
            "Iteration 15768, Train Loss=14366.278658694247\n",
            "Iteration 15769, Train Loss=14345.886768508633\n",
            "Iteration 15770, Train Loss=14385.279254534616\n",
            "Iteration 15771, Train Loss=14374.067766557826\n",
            "Iteration 15772, Train Loss=14315.508604603741\n",
            "Iteration 15773, Train Loss=14299.582316475633\n",
            "Iteration 15774, Train Loss=14321.931679261823\n",
            "Iteration 15775, Train Loss=14309.410500963702\n",
            "Iteration 15776, Train Loss=14267.043243236247\n",
            "Iteration 15777, Train Loss=14254.272708682092\n",
            "Iteration 15778, Train Loss=14265.323931253422\n",
            "Iteration 15779, Train Loss=14252.124016585343\n",
            "Iteration 15780, Train Loss=14220.551323189726\n",
            "Iteration 15781, Train Loss=14209.383190417986\n",
            "Iteration 15782, Train Loss=14212.913980418336\n",
            "Iteration 15783, Train Loss=14199.858557512885\n",
            "Iteration 15784, Train Loss=14175.407939071743\n",
            "Iteration 15785, Train Loss=14164.806268849532\n",
            "Iteration 15786, Train Loss=14163.410217544326\n",
            "Iteration 15787, Train Loss=14150.85975630656\n",
            "Iteration 15788, Train Loss=14131.099325666053\n",
            "Iteration 15789, Train Loss=14120.53869670603\n",
            "Iteration 15790, Train Loss=14115.927313929042\n",
            "Iteration 15791, Train Loss=14103.99986937966\n",
            "Iteration 15792, Train Loss=14087.310658605798\n",
            "Iteration 15793, Train Loss=14076.594160962064\n",
            "Iteration 15794, Train Loss=14069.898400465443\n",
            "Iteration 15795, Train Loss=14058.52213276115\n",
            "Iteration 15796, Train Loss=14043.861347069027\n",
            "Iteration 15797, Train Loss=14032.964336136643\n",
            "Iteration 15798, Train Loss=14024.904648981179\n",
            "Iteration 15799, Train Loss=14013.960367435584\n",
            "Iteration 15800, Train Loss=14000.655285406983\n",
            "Iteration 15801, Train Loss=13989.626554409846\n",
            "Iteration 15802, Train Loss=13980.665542031978\n",
            "Iteration 15803, Train Loss=13970.019096308077\n",
            "Iteration 15804, Train Loss=13957.63998420958\n",
            "Iteration 15805, Train Loss=13946.548510545044\n",
            "Iteration 15806, Train Loss=13936.982535433892\n",
            "Iteration 15807, Train Loss=13926.521796421654\n",
            "Iteration 15808, Train Loss=13914.786930790684\n",
            "Iteration 15809, Train Loss=13903.697321191547\n",
            "Iteration 15810, Train Loss=13893.723053699385\n",
            "Iteration 15811, Train Loss=13883.35900455122\n",
            "Iteration 15812, Train Loss=13872.079588756655\n",
            "Iteration 15813, Train Loss=13861.042956340418\n",
            "Iteration 15814, Train Loss=13850.796107706958\n",
            "Iteration 15815, Train Loss=13840.466206563506\n",
            "Iteration 15816, Train Loss=13829.50903398143\n",
            "Iteration 15817, Train Loss=13818.560564713913\n",
            "Iteration 15818, Train Loss=13808.139948751475\n",
            "Iteration 15819, Train Loss=13797.803568316382\n",
            "Iteration 15820, Train Loss=13787.071222619426\n",
            "Iteration 15821, Train Loss=13776.230581601796\n",
            "Iteration 15822, Train Loss=13765.711120270329\n",
            "Iteration 15823, Train Loss=13755.347137013432\n",
            "Iteration 15824, Train Loss=13744.765858758366\n",
            "Iteration 15825, Train Loss=13734.038809607437\n",
            "Iteration 15826, Train Loss=13723.478120025346\n",
            "Iteration 15827, Train Loss=13713.081007144927\n",
            "Iteration 15828, Train Loss=13702.595374154946\n",
            "Iteration 15829, Train Loss=13691.976063997325\n",
            "Iteration 15830, Train Loss=13681.417234027427\n",
            "Iteration 15831, Train Loss=13670.993513456453\n",
            "Iteration 15832, Train Loss=13660.563627582465\n",
            "Iteration 15833, Train Loss=13650.037838031421\n",
            "Iteration 15834, Train Loss=13639.51049095891\n",
            "Iteration 15835, Train Loss=13629.074492013508\n",
            "Iteration 15836, Train Loss=13618.674748936868\n",
            "Iteration 15837, Train Loss=13608.22365544195\n",
            "Iteration 15838, Train Loss=13597.744738928373\n",
            "Iteration 15839, Train Loss=13587.314123687804\n",
            "Iteration 15840, Train Loss=13576.931685560226\n",
            "Iteration 15841, Train Loss=13566.536067761084\n",
            "Iteration 15842, Train Loss=13556.111409790068\n",
            "Iteration 15843, Train Loss=13545.702837962157\n",
            "Iteration 15844, Train Loss=13535.335262544173\n",
            "Iteration 15845, Train Loss=13524.979205177759\n",
            "Iteration 15846, Train Loss=13514.60628118533\n",
            "Iteration 15847, Train Loss=13504.2320123073\n",
            "Iteration 15848, Train Loss=13493.883747133335\n",
            "Iteration 15849, Train Loss=13483.55713123707\n",
            "Iteration 15850, Train Loss=13473.228811138044\n",
            "Iteration 15851, Train Loss=13462.895021625282\n",
            "Iteration 15852, Train Loss=13452.573327276687\n",
            "Iteration 15853, Train Loss=13442.272378754138\n",
            "Iteration 15854, Train Loss=13431.980858080036\n",
            "Iteration 15855, Train Loss=13421.688017481869\n",
            "Iteration 15856, Train Loss=13411.399295909589\n",
            "Iteration 15857, Train Loss=13401.125188658381\n",
            "Iteration 15858, Train Loss=13390.864980655262\n",
            "Iteration 15859, Train Loss=13380.609960882386\n",
            "Iteration 15860, Train Loss=13370.357534417348\n",
            "Iteration 15861, Train Loss=13360.113733551547\n",
            "Iteration 15862, Train Loss=13349.88288069726\n",
            "Iteration 15863, Train Loss=13339.661751665164\n",
            "Iteration 15864, Train Loss=13329.44561612545\n",
            "Iteration 15865, Train Loss=13319.235201059593\n",
            "Iteration 15866, Train Loss=13309.03458226755\n",
            "Iteration 15867, Train Loss=13298.84480394533\n",
            "Iteration 15868, Train Loss=13288.663005093502\n",
            "Iteration 15869, Train Loss=13278.48714771712\n",
            "Iteration 15870, Train Loss=13268.318661064026\n",
            "Iteration 15871, Train Loss=13258.159779685675\n",
            "Iteration 15872, Train Loss=13248.010309727886\n",
            "Iteration 15873, Train Loss=13237.868359731921\n",
            "Iteration 15874, Train Loss=13227.733233822151\n",
            "Iteration 15875, Train Loss=13217.606126802133\n",
            "Iteration 15876, Train Loss=13207.488131299358\n",
            "Iteration 15877, Train Loss=13197.378782178199\n",
            "Iteration 15878, Train Loss=13187.276973869788\n",
            "Iteration 15879, Train Loss=13177.182544973339\n",
            "Iteration 15880, Train Loss=13167.096295417043\n",
            "Iteration 15881, Train Loss=13157.018733396899\n",
            "Iteration 15882, Train Loss=13146.949456961995\n",
            "Iteration 15883, Train Loss=13136.887848034716\n",
            "Iteration 15884, Train Loss=13126.833907561926\n",
            "Iteration 15885, Train Loss=13116.788122561891\n",
            "Iteration 15886, Train Loss=13106.75073400184\n",
            "Iteration 15887, Train Loss=13096.72146759857\n",
            "Iteration 15888, Train Loss=13086.699975709056\n",
            "Iteration 15889, Train Loss=13076.686282965205\n",
            "Iteration 15890, Train Loss=13066.680672526447\n",
            "Iteration 15891, Train Loss=13056.683270246578\n",
            "Iteration 15892, Train Loss=13046.693909630741\n",
            "Iteration 15893, Train Loss=13036.712387684394\n",
            "Iteration 15894, Train Loss=13026.738714081801\n",
            "Iteration 15895, Train Loss=13016.773048484112\n",
            "Iteration 15896, Train Loss=13006.815466466718\n",
            "Iteration 15897, Train Loss=12996.865875537489\n",
            "Iteration 15898, Train Loss=12986.924151957934\n",
            "Iteration 15899, Train Loss=12976.99028736352\n",
            "Iteration 15900, Train Loss=12967.064367562794\n",
            "Iteration 15901, Train Loss=12957.146442955955\n",
            "Iteration 15902, Train Loss=12947.236467492887\n",
            "Iteration 15903, Train Loss=12937.33436390532\n",
            "Iteration 15904, Train Loss=12927.44011224357\n",
            "Iteration 15905, Train Loss=12917.553753743277\n",
            "Iteration 15906, Train Loss=12907.675322538113\n",
            "Iteration 15907, Train Loss=12897.804799697562\n",
            "Iteration 15908, Train Loss=12887.942137552129\n",
            "Iteration 15909, Train Loss=12878.087311820784\n",
            "Iteration 15910, Train Loss=12868.240336768189\n",
            "Iteration 15911, Train Loss=12858.401233484818\n",
            "Iteration 15912, Train Loss=12848.569997185747\n",
            "Iteration 15913, Train Loss=12838.746600005188\n",
            "Iteration 15914, Train Loss=12828.931018827443\n",
            "Iteration 15915, Train Loss=12819.123252356167\n",
            "Iteration 15916, Train Loss=12809.323310538754\n",
            "Iteration 15917, Train Loss=12799.531194098625\n",
            "Iteration 15918, Train Loss=12789.746888155887\n",
            "Iteration 15919, Train Loss=12779.970373995577\n",
            "Iteration 15920, Train Loss=12770.201642644057\n",
            "Iteration 15921, Train Loss=12760.4406951301\n",
            "Iteration 15922, Train Loss=12750.687532296333\n",
            "Iteration 15923, Train Loss=12740.942146770398\n",
            "Iteration 15924, Train Loss=12731.20452524224\n",
            "Iteration 15925, Train Loss=12721.474656492206\n",
            "Iteration 15926, Train Loss=12711.752535501335\n",
            "Iteration 15927, Train Loss=12702.038160520131\n",
            "Iteration 15928, Train Loss=12692.331527279293\n",
            "Iteration 15929, Train Loss=12682.632627042936\n",
            "Iteration 15930, Train Loss=12672.941449553624\n",
            "Iteration 15931, Train Loss=12663.257986838169\n",
            "Iteration 15932, Train Loss=12653.58223408242\n",
            "Iteration 15933, Train Loss=12643.914187091485\n",
            "Iteration 15934, Train Loss=12634.25383978627\n",
            "Iteration 15935, Train Loss=12624.601184004667\n",
            "Iteration 15936, Train Loss=12614.95621135722\n",
            "Iteration 15937, Train Loss=12605.31891498914\n",
            "Iteration 15938, Train Loss=12595.689289425047\n",
            "Iteration 15939, Train Loss=12586.067329255591\n",
            "Iteration 15940, Train Loss=12576.453027962962\n",
            "Iteration 15941, Train Loss=12566.846378070291\n",
            "Iteration 15942, Train Loss=12557.247372169357\n",
            "Iteration 15943, Train Loss=12547.65600362248\n",
            "Iteration 15944, Train Loss=12538.072266506977\n",
            "Iteration 15945, Train Loss=12528.496154847113\n",
            "Iteration 15946, Train Loss=12518.927662128546\n",
            "Iteration 15947, Train Loss=12509.366781372197\n",
            "Iteration 15948, Train Loss=12499.813505601767\n",
            "Iteration 15949, Train Loss=12490.267828263526\n",
            "Iteration 15950, Train Loss=12480.729743132733\n",
            "Iteration 15951, Train Loss=12471.19924403178\n",
            "Iteration 15952, Train Loss=12461.676324519874\n",
            "Iteration 15953, Train Loss=12452.160977909607\n",
            "Iteration 15954, Train Loss=12442.653197496216\n",
            "Iteration 15955, Train Loss=12433.152976730253\n",
            "Iteration 15956, Train Loss=12423.660309283292\n",
            "Iteration 15957, Train Loss=12414.175188867433\n",
            "Iteration 15958, Train Loss=12404.697609117637\n",
            "Iteration 15959, Train Loss=12395.227563536411\n",
            "Iteration 15960, Train Loss=12385.76504557153\n",
            "Iteration 15961, Train Loss=12376.310048742987\n",
            "Iteration 15962, Train Loss=12366.86256665947\n",
            "Iteration 15963, Train Loss=12357.422593011263\n",
            "Iteration 15964, Train Loss=12347.9901214669\n",
            "Iteration 15965, Train Loss=12338.565145646202\n",
            "Iteration 15966, Train Loss=12329.147659127659\n",
            "Iteration 15967, Train Loss=12319.737655488048\n",
            "Iteration 15968, Train Loss=12310.33512836064\n",
            "Iteration 15969, Train Loss=12300.940071415493\n",
            "Iteration 15970, Train Loss=12291.552478356503\n",
            "Iteration 15971, Train Loss=12282.172342870315\n",
            "Iteration 15972, Train Loss=12272.799658625452\n",
            "Iteration 15973, Train Loss=12263.434419282754\n",
            "Iteration 15974, Train Loss=12254.076618507406\n",
            "Iteration 15975, Train Loss=12244.726250000522\n",
            "Iteration 15976, Train Loss=12235.383307477678\n",
            "Iteration 15977, Train Loss=12226.04778467539\n",
            "Iteration 15978, Train Loss=12216.719675324479\n",
            "Iteration 15979, Train Loss=12207.39897315253\n",
            "Iteration 15980, Train Loss=12198.085671888644\n",
            "Iteration 15981, Train Loss=12188.779765266152\n",
            "Iteration 15982, Train Loss=12179.481247039252\n",
            "Iteration 15983, Train Loss=12170.190110970569\n",
            "Iteration 15984, Train Loss=12160.906350839366\n",
            "Iteration 15985, Train Loss=12151.629960425154\n",
            "Iteration 15986, Train Loss=12142.360933511805\n",
            "Iteration 15987, Train Loss=12133.09926388663\n",
            "Iteration 15988, Train Loss=12123.844945340477\n",
            "Iteration 15989, Train Loss=12114.597971678262\n",
            "Iteration 15990, Train Loss=12105.358336709778\n",
            "Iteration 15991, Train Loss=12096.126034258958\n",
            "Iteration 15992, Train Loss=12086.9010581527\n",
            "Iteration 15993, Train Loss=12077.683402226012\n",
            "Iteration 15994, Train Loss=12068.473060317841\n",
            "Iteration 15995, Train Loss=12059.270026271422\n",
            "Iteration 15996, Train Loss=12050.074293938946\n",
            "Iteration 15997, Train Loss=12040.885857177229\n",
            "Iteration 15998, Train Loss=12031.704709853913\n",
            "Iteration 15999, Train Loss=12022.530845840933\n",
            "Iteration 16000, Test Loss=1524.2511168257508\n",
            "Iteration 16000, Train Loss=12013.364259019192\n",
            "Iteration 16001, Train Loss=12004.204943273531\n",
            "Iteration 16002, Train Loss=11995.05289249447\n",
            "Iteration 16003, Train Loss=11985.908100579272\n",
            "Iteration 16004, Train Loss=11976.770561429556\n",
            "Iteration 16005, Train Loss=11967.640268954932\n",
            "Iteration 16006, Train Loss=11958.517217069224\n",
            "Iteration 16007, Train Loss=11949.401399695547\n",
            "Iteration 16008, Train Loss=11940.292810759154\n",
            "Iteration 16009, Train Loss=11931.191444193832\n",
            "Iteration 16010, Train Loss=11922.097293937471\n",
            "Iteration 16011, Train Loss=11913.010353933287\n",
            "Iteration 16012, Train Loss=11903.930618130613\n",
            "Iteration 16013, Train Loss=11894.858080482523\n",
            "Iteration 16014, Train Loss=11885.792734949355\n",
            "Iteration 16015, Train Loss=11876.734575495551\n",
            "Iteration 16016, Train Loss=11867.683596091601\n",
            "Iteration 16017, Train Loss=11858.63979071207\n",
            "Iteration 16018, Train Loss=11849.60315333789\n",
            "Iteration 16019, Train Loss=11840.573677953571\n",
            "Iteration 16020, Train Loss=11831.551358549086\n",
            "Iteration 16021, Train Loss=11822.53618911879\n",
            "Iteration 16022, Train Loss=11813.52816366187\n",
            "Iteration 16023, Train Loss=11804.527276182029\n",
            "Iteration 16024, Train Loss=11795.533520687235\n",
            "Iteration 16025, Train Loss=11786.546891190947\n",
            "Iteration 16026, Train Loss=11777.567381709308\n",
            "Iteration 16027, Train Loss=11768.594986264357\n",
            "Iteration 16028, Train Loss=11759.62969888098\n",
            "Iteration 16029, Train Loss=11750.671513589436\n",
            "Iteration 16030, Train Loss=11741.720424422676\n",
            "Iteration 16031, Train Loss=11732.776425418419\n",
            "Iteration 16032, Train Loss=11723.839510618434\n",
            "Iteration 16033, Train Loss=11714.909674067214\n",
            "Iteration 16034, Train Loss=11705.986909813966\n",
            "Iteration 16035, Train Loss=11697.07121191078\n",
            "Iteration 16036, Train Loss=11688.162574413815\n",
            "Iteration 16037, Train Loss=11679.260991382305\n",
            "Iteration 16038, Train Loss=11670.366456879037\n",
            "Iteration 16039, Train Loss=11661.478964970074\n",
            "Iteration 16040, Train Loss=11652.59850972442\n",
            "Iteration 16041, Train Loss=11643.725085214344\n",
            "Iteration 16042, Train Loss=11634.858685515344\n",
            "Iteration 16043, Train Loss=11625.999304705361\n",
            "Iteration 16044, Train Loss=11617.146936865782\n",
            "Iteration 16045, Train Loss=11608.301576080266\n",
            "Iteration 16046, Train Loss=11599.463216435377\n",
            "Iteration 16047, Train Loss=11590.631852020277\n",
            "Iteration 16048, Train Loss=11581.807476926477\n",
            "Iteration 16049, Train Loss=11572.990085248271\n",
            "Iteration 16050, Train Loss=11564.179671082054\n",
            "Iteration 16051, Train Loss=11555.376228526737\n",
            "Iteration 16052, Train Loss=11546.579751682755\n",
            "Iteration 16053, Train Loss=11537.79023465356\n",
            "Iteration 16054, Train Loss=11529.00767154393\n",
            "Iteration 16055, Train Loss=11520.232056461013\n",
            "Iteration 16056, Train Loss=11511.463383513319\n",
            "Iteration 16057, Train Loss=11502.701646811722\n",
            "Iteration 16058, Train Loss=11493.946840468623\n",
            "Iteration 16059, Train Loss=11485.198958597615\n",
            "Iteration 16060, Train Loss=11476.457995314073\n",
            "Iteration 16061, Train Loss=11467.723944735204\n",
            "Iteration 16062, Train Loss=11458.996800978854\n",
            "Iteration 16063, Train Loss=11450.276558164627\n",
            "Iteration 16064, Train Loss=11441.56321041314\n",
            "Iteration 16065, Train Loss=11432.856751846204\n",
            "Iteration 16066, Train Loss=11424.157176586612\n",
            "Iteration 16067, Train Loss=11415.464478757855\n",
            "Iteration 16068, Train Loss=11406.778652485005\n",
            "Iteration 16069, Train Loss=11398.099691892852\n",
            "Iteration 16070, Train Loss=11389.427591107737\n",
            "Iteration 16071, Train Loss=11380.762344256424\n",
            "Iteration 16072, Train Loss=11372.103945465657\n",
            "Iteration 16073, Train Loss=11363.452388863494\n",
            "Iteration 16074, Train Loss=11354.807668577643\n",
            "Iteration 16075, Train Loss=11346.1697787364\n",
            "Iteration 16076, Train Loss=11337.538713468046\n",
            "Iteration 16077, Train Loss=11328.914466901293\n",
            "Iteration 16078, Train Loss=11320.297033164656\n",
            "Iteration 16079, Train Loss=11311.686406386774\n",
            "Iteration 16080, Train Loss=11303.082580695516\n",
            "Iteration 16081, Train Loss=11294.485550219284\n",
            "Iteration 16082, Train Loss=11285.895309085969\n",
            "Iteration 16083, Train Loss=11277.311851422799\n",
            "Iteration 16084, Train Loss=11268.735171356428\n",
            "Iteration 16085, Train Loss=11260.165263013821\n",
            "Iteration 16086, Train Loss=11251.602120520032\n",
            "Iteration 16087, Train Loss=11243.045738000199\n",
            "Iteration 16088, Train Loss=11234.496109578431\n",
            "Iteration 16089, Train Loss=11225.953229377945\n",
            "Iteration 16090, Train Loss=11217.417091520558\n",
            "Iteration 16091, Train Loss=11208.887690128362\n",
            "Iteration 16092, Train Loss=11200.365019319814\n",
            "Iteration 16093, Train Loss=11191.849073215126\n",
            "Iteration 16094, Train Loss=11183.339845930246\n",
            "Iteration 16095, Train Loss=11174.837331583156\n",
            "Iteration 16096, Train Loss=11166.341524286254\n",
            "Iteration 16097, Train Loss=11157.852418155826\n",
            "Iteration 16098, Train Loss=11149.370007300065\n",
            "Iteration 16099, Train Loss=11140.89428583564\n",
            "Iteration 16100, Train Loss=11132.42524786648\n",
            "Iteration 16101, Train Loss=11123.962887512791\n",
            "Iteration 16102, Train Loss=11115.507198877047\n",
            "Iteration 16103, Train Loss=11107.058176092489\n",
            "Iteration 16104, Train Loss=11098.615813271565\n",
            "Iteration 16105, Train Loss=11090.180104589634\n",
            "Iteration 16106, Train Loss=11081.751044207282\n",
            "Iteration 16107, Train Loss=11073.328626429358\n",
            "Iteration 16108, Train Loss=11064.912845600857\n",
            "Iteration 16109, Train Loss=11056.503696432675\n",
            "Iteration 16110, Train Loss=11048.101173919209\n",
            "Iteration 16111, Train Loss=11039.705274090218\n",
            "Iteration 16112, Train Loss=11031.315994202601\n",
            "Iteration 16113, Train Loss=11022.93333471303\n",
            "Iteration 16114, Train Loss=11014.557300823859\n",
            "Iteration 16115, Train Loss=11006.187908322903\n",
            "Iteration 16116, Train Loss=10997.825190770169\n",
            "Iteration 16117, Train Loss=10989.469218584907\n",
            "Iteration 16118, Train Loss=10981.120128581537\n",
            "Iteration 16119, Train Loss=10972.778191059722\n",
            "Iteration 16120, Train Loss=10964.443927095364\n",
            "Iteration 16121, Train Loss=10956.118356821404\n",
            "Iteration 16122, Train Loss=10947.803463841477\n",
            "Iteration 16123, Train Loss=10939.50314975066\n",
            "Iteration 16124, Train Loss=10931.22509224758\n",
            "Iteration 16125, Train Loss=10922.984526687873\n",
            "Iteration 16126, Train Loss=10914.811802967695\n",
            "Iteration 16127, Train Loss=10906.767741195337\n",
            "Iteration 16128, Train Loss=10898.974924080383\n",
            "Iteration 16129, Train Loss=10891.681225112989\n",
            "Iteration 16130, Train Loss=10885.391206401104\n",
            "Iteration 16131, Train Loss=10881.129925389137\n",
            "Iteration 16132, Train Loss=10880.989970617044\n",
            "Iteration 16133, Train Loss=10889.170676379808\n",
            "Iteration 16134, Train Loss=10914.011089344393\n",
            "Iteration 16135, Train Loss=10970.876933369815\n",
            "Iteration 16136, Train Loss=11085.243534262563\n",
            "Iteration 16137, Train Loss=11283.741871829174\n",
            "Iteration 16138, Train Loss=11554.448307006434\n",
            "Iteration 16139, Train Loss=11745.70831678657\n",
            "Iteration 16140, Train Loss=11600.761024084692\n",
            "Iteration 16141, Train Loss=11117.91639418965\n",
            "Iteration 16142, Train Loss=10789.628923773611\n",
            "Iteration 16143, Train Loss=10929.03874415115\n",
            "Iteration 16144, Train Loss=11209.761563751503\n",
            "Iteration 16145, Train Loss=11160.138205683283\n",
            "Iteration 16146, Train Loss=10849.490300641137\n",
            "Iteration 16147, Train Loss=10758.740687248\n",
            "Iteration 16148, Train Loss=10945.640032501538\n",
            "Iteration 16149, Train Loss=11006.11942044951\n",
            "Iteration 16150, Train Loss=10818.390957087215\n",
            "Iteration 16151, Train Loss=10713.301604376215\n",
            "Iteration 16152, Train Loss=10823.538906681393\n",
            "Iteration 16153, Train Loss=10877.311524387682\n",
            "Iteration 16154, Train Loss=10754.215961440192\n",
            "Iteration 16155, Train Loss=10680.948048757442\n",
            "Iteration 16156, Train Loss=10753.238107870717\n",
            "Iteration 16157, Train Loss=10780.738600683026\n",
            "Iteration 16158, Train Loss=10693.08651455726\n",
            "Iteration 16159, Train Loss=10650.611168823234\n",
            "Iteration 16160, Train Loss=10700.040091385448\n",
            "Iteration 16161, Train Loss=10706.229609622704\n",
            "Iteration 16162, Train Loss=10642.164814447096\n",
            "Iteration 16163, Train Loss=10620.980546076888\n",
            "Iteration 16164, Train Loss=10653.335891348626\n",
            "Iteration 16165, Train Loss=10646.027664489098\n",
            "Iteration 16166, Train Loss=10600.03775684479\n",
            "Iteration 16167, Train Loss=10590.969142693673\n",
            "Iteration 16168, Train Loss=10609.789650499357\n",
            "Iteration 16169, Train Loss=10596.080203268124\n",
            "Iteration 16170, Train Loss=10563.664609671967\n",
            "Iteration 16171, Train Loss=10560.09538747164\n",
            "Iteration 16172, Train Loss=10568.866266511535\n",
            "Iteration 16173, Train Loss=10553.218496580721\n",
            "Iteration 16174, Train Loss=10530.430825010844\n",
            "Iteration 16175, Train Loss=10528.41012262636\n",
            "Iteration 16176, Train Loss=10530.278287018655\n",
            "Iteration 16177, Train Loss=10515.144917263757\n",
            "Iteration 16178, Train Loss=10498.675488588384\n",
            "Iteration 16179, Train Loss=10496.273848230328\n",
            "Iteration 16180, Train Loss=10493.819331207333\n",
            "Iteration 16181, Train Loss=10480.10662408562\n",
            "Iteration 16182, Train Loss=10467.510605963796\n",
            "Iteration 16183, Train Loss=10464.044880784253\n",
            "Iteration 16184, Train Loss=10459.0719617025\n",
            "Iteration 16185, Train Loss=10446.912459078998\n",
            "Iteration 16186, Train Loss=10436.550396107545\n",
            "Iteration 16187, Train Loss=10431.964895036775\n",
            "Iteration 16188, Train Loss=10425.641065079439\n",
            "Iteration 16189, Train Loss=10414.802507376191\n",
            "Iteration 16190, Train Loss=10405.667254378914\n",
            "Iteration 16191, Train Loss=10400.154595296586\n",
            "Iteration 16192, Train Loss=10393.153547933132\n",
            "Iteration 16193, Train Loss=10383.325659325532\n",
            "Iteration 16194, Train Loss=10374.84096637603\n",
            "Iteration 16195, Train Loss=10368.641250436818\n",
            "Iteration 16196, Train Loss=10361.33186879886\n",
            "Iteration 16197, Train Loss=10352.22962968198\n",
            "Iteration 16198, Train Loss=10344.085287933829\n",
            "Iteration 16199, Train Loss=10337.408224398332\n",
            "Iteration 16200, Train Loss=10329.972986485172\n",
            "Iteration 16201, Train Loss=10321.377663348645\n",
            "Iteration 16202, Train Loss=10313.415500257901\n",
            "Iteration 16203, Train Loss=10306.418722913999\n",
            "Iteration 16204, Train Loss=10298.94298837025\n",
            "Iteration 16205, Train Loss=10290.697785224012\n",
            "Iteration 16206, Train Loss=10282.841552191207\n",
            "Iteration 16207, Train Loss=10275.636013753961\n",
            "Iteration 16208, Train Loss=10268.154078550733\n",
            "Iteration 16209, Train Loss=10260.15204000861\n",
            "Iteration 16210, Train Loss=10252.3670442653\n",
            "Iteration 16211, Train Loss=10245.027993891867\n",
            "Iteration 16212, Train Loss=10237.551393988686\n",
            "Iteration 16213, Train Loss=10229.720323647014\n",
            "Iteration 16214, Train Loss=10221.991868250718\n",
            "Iteration 16215, Train Loss=10214.57009631436\n",
            "Iteration 16216, Train Loss=10207.100510222279\n",
            "Iteration 16217, Train Loss=10199.391735364567\n",
            "Iteration 16218, Train Loss=10191.713879490377\n",
            "Iteration 16219, Train Loss=10184.244001852267\n",
            "Iteration 16220, Train Loss=10176.780176850945\n",
            "Iteration 16221, Train Loss=10169.160207553225\n",
            "Iteration 16222, Train Loss=10161.530311889854\n",
            "Iteration 16223, Train Loss=10154.036524617939\n",
            "Iteration 16224, Train Loss=10146.577095820907\n",
            "Iteration 16225, Train Loss=10139.022374968661\n",
            "Iteration 16226, Train Loss=10131.438475847759\n",
            "Iteration 16227, Train Loss=10123.938131164896\n",
            "Iteration 16228, Train Loss=10116.48284977694\n",
            "Iteration 16229, Train Loss=10108.976434834083\n",
            "Iteration 16230, Train Loss=10101.436100359828\n",
            "Iteration 16231, Train Loss=10093.941816598086\n",
            "Iteration 16232, Train Loss=10086.49188074096\n",
            "Iteration 16233, Train Loss=10079.021541147325\n",
            "Iteration 16234, Train Loss=10071.521520131795\n",
            "Iteration 16235, Train Loss=10064.042340223783\n",
            "Iteration 16236, Train Loss=10056.600236310964\n",
            "Iteration 16237, Train Loss=10049.157388803738\n",
            "Iteration 16238, Train Loss=10041.693679517959\n",
            "Iteration 16239, Train Loss=10034.23570357585\n",
            "Iteration 16240, Train Loss=10026.804822944581\n",
            "Iteration 16241, Train Loss=10019.383886282561\n",
            "Iteration 16242, Train Loss=10011.952121322556\n",
            "Iteration 16243, Train Loss=10004.518899784813\n",
            "Iteration 16244, Train Loss=9997.102962109842\n",
            "Iteration 16245, Train Loss=9989.700904371515\n",
            "Iteration 16246, Train Loss=9982.296836411178\n",
            "Iteration 16247, Train Loss=9974.889769111998\n",
            "Iteration 16248, Train Loss=9967.492227720617\n",
            "Iteration 16249, Train Loss=9960.108052479629\n",
            "Iteration 16250, Train Loss=9952.728098874317\n",
            "Iteration 16251, Train Loss=9945.346917768316\n",
            "Iteration 16252, Train Loss=9937.970433537712\n",
            "Iteration 16253, Train Loss=9930.604584486693\n",
            "Iteration 16254, Train Loss=9923.246201506647\n",
            "Iteration 16255, Train Loss=9915.88961047504\n",
            "Iteration 16256, Train Loss=9908.535734322653\n",
            "Iteration 16257, Train Loss=9901.189392793449\n",
            "Iteration 16258, Train Loss=9893.851239498941\n",
            "Iteration 16259, Train Loss=9886.517576254388\n",
            "Iteration 16260, Train Loss=9879.186735325256\n",
            "Iteration 16261, Train Loss=9871.861153306596\n",
            "Iteration 16262, Train Loss=9864.542930183594\n",
            "Iteration 16263, Train Loss=9857.230763793521\n",
            "Iteration 16264, Train Loss=9849.922522075587\n",
            "Iteration 16265, Train Loss=9842.618530120086\n",
            "Iteration 16266, Train Loss=9835.320593180375\n",
            "Iteration 16267, Train Loss=9828.029058303286\n",
            "Iteration 16268, Train Loss=9820.742579234913\n",
            "Iteration 16269, Train Loss=9813.460381075764\n",
            "Iteration 16270, Train Loss=9806.18325407122\n",
            "Iteration 16271, Train Loss=9798.912092501821\n",
            "Iteration 16272, Train Loss=9791.646580244033\n",
            "Iteration 16273, Train Loss=9784.385863589665\n",
            "Iteration 16274, Train Loss=9777.129851213309\n",
            "Iteration 16275, Train Loss=9769.879180935639\n",
            "Iteration 16276, Train Loss=9762.634160166692\n",
            "Iteration 16277, Train Loss=9755.39438442103\n",
            "Iteration 16278, Train Loss=9748.159427096736\n",
            "Iteration 16279, Train Loss=9740.929419154998\n",
            "Iteration 16280, Train Loss=9733.704744351764\n",
            "Iteration 16281, Train Loss=9726.485442204767\n",
            "Iteration 16282, Train Loss=9719.271205216919\n",
            "Iteration 16283, Train Loss=9712.06185233853\n",
            "Iteration 16284, Train Loss=9704.857532072177\n",
            "Iteration 16285, Train Loss=9697.658441368232\n",
            "Iteration 16286, Train Loss=9690.464532035272\n",
            "Iteration 16287, Train Loss=9683.275606568073\n",
            "Iteration 16288, Train Loss=9676.091595852466\n",
            "Iteration 16289, Train Loss=9668.91260685029\n",
            "Iteration 16290, Train Loss=9661.738729942284\n",
            "Iteration 16291, Train Loss=9654.56990422795\n",
            "Iteration 16292, Train Loss=9647.406009224502\n",
            "Iteration 16293, Train Loss=9640.247017344203\n",
            "Iteration 16294, Train Loss=9633.092991705867\n",
            "Iteration 16295, Train Loss=9625.943969111704\n",
            "Iteration 16296, Train Loss=9618.799898444215\n",
            "Iteration 16297, Train Loss=9611.660704500126\n",
            "Iteration 16298, Train Loss=9604.526370866026\n",
            "Iteration 16299, Train Loss=9597.396928609576\n",
            "Iteration 16300, Train Loss=9590.272389043903\n",
            "Iteration 16301, Train Loss=9583.15271317195\n",
            "Iteration 16302, Train Loss=9576.037850327772\n",
            "Iteration 16303, Train Loss=9568.927784491696\n",
            "Iteration 16304, Train Loss=9561.822525846957\n",
            "Iteration 16305, Train Loss=9554.722073252846\n",
            "Iteration 16306, Train Loss=9547.626396642236\n",
            "Iteration 16307, Train Loss=9540.53545824475\n",
            "Iteration 16308, Train Loss=9533.449239571015\n",
            "Iteration 16309, Train Loss=9526.367737507757\n",
            "Iteration 16310, Train Loss=9519.2909440081\n",
            "Iteration 16311, Train Loss=9512.218834306279\n",
            "Iteration 16312, Train Loss=9505.15137736579\n",
            "Iteration 16313, Train Loss=9498.08855207873\n",
            "Iteration 16314, Train Loss=9491.030346665513\n",
            "Iteration 16315, Train Loss=9483.97674836263\n",
            "Iteration 16316, Train Loss=9476.92773489234\n",
            "Iteration 16317, Train Loss=9469.883278701815\n",
            "Iteration 16318, Train Loss=9462.843356608033\n",
            "Iteration 16319, Train Loss=9455.807950940147\n",
            "Iteration 16320, Train Loss=9448.777045083034\n",
            "Iteration 16321, Train Loss=9441.750617290574\n",
            "Iteration 16322, Train Loss=9434.728641656868\n",
            "Iteration 16323, Train Loss=9427.711093487767\n",
            "Iteration 16324, Train Loss=9420.697950983547\n",
            "Iteration 16325, Train Loss=9413.689194117484\n",
            "Iteration 16326, Train Loss=9406.684800389161\n",
            "Iteration 16327, Train Loss=9399.684744389295\n",
            "Iteration 16328, Train Loss=9392.68900031612\n",
            "Iteration 16329, Train Loss=9385.697543461622\n",
            "Iteration 16330, Train Loss=9378.710350794387\n",
            "Iteration 16331, Train Loss=9371.727398313342\n",
            "Iteration 16332, Train Loss=9364.74866032974\n",
            "Iteration 16333, Train Loss=9357.774110165683\n",
            "Iteration 16334, Train Loss=9350.803721132843\n",
            "Iteration 16335, Train Loss=9343.837467715772\n",
            "Iteration 16336, Train Loss=9336.875324138817\n",
            "Iteration 16337, Train Loss=9329.917263951816\n",
            "Iteration 16338, Train Loss=9322.963259730584\n",
            "Iteration 16339, Train Loss=9316.013283552544\n",
            "Iteration 16340, Train Loss=9309.06730808795\n",
            "Iteration 16341, Train Loss=9302.125305958676\n",
            "Iteration 16342, Train Loss=9295.187249802173\n",
            "Iteration 16343, Train Loss=9288.253111589032\n",
            "Iteration 16344, Train Loss=9281.322862802763\n",
            "Iteration 16345, Train Loss=9274.39647509626\n",
            "Iteration 16346, Train Loss=9267.473920073182\n",
            "Iteration 16347, Train Loss=9260.555169709967\n",
            "Iteration 16348, Train Loss=9253.640195685095\n",
            "Iteration 16349, Train Loss=9246.728969515674\n",
            "Iteration 16350, Train Loss=9239.821462734006\n",
            "Iteration 16351, Train Loss=9232.917646852684\n",
            "Iteration 16352, Train Loss=9226.017493885141\n",
            "Iteration 16353, Train Loss=9219.12097586405\n",
            "Iteration 16354, Train Loss=9212.228065098103\n",
            "Iteration 16355, Train Loss=9205.33873401056\n",
            "Iteration 16356, Train Loss=9198.452955208526\n",
            "Iteration 16357, Train Loss=9191.570701862336\n",
            "Iteration 16358, Train Loss=9184.691947439793\n",
            "Iteration 16359, Train Loss=9177.816666083785\n",
            "Iteration 16360, Train Loss=9170.94483232064\n",
            "Iteration 16361, Train Loss=9164.076421242582\n",
            "Iteration 16362, Train Loss=9157.211408639356\n",
            "Iteration 16363, Train Loss=9150.349770920513\n",
            "Iteration 16364, Train Loss=9143.491485491075\n",
            "Iteration 16365, Train Loss=9136.636530493999\n",
            "Iteration 16366, Train Loss=9129.784885105313\n",
            "Iteration 16367, Train Loss=9122.936529458191\n",
            "Iteration 16368, Train Loss=9116.091444730211\n",
            "Iteration 16369, Train Loss=9109.249613379545\n",
            "Iteration 16370, Train Loss=9102.411019009756\n",
            "Iteration 16371, Train Loss=9095.57564670484\n",
            "Iteration 16372, Train Loss=9088.743482866686\n",
            "Iteration 16373, Train Loss=9081.914515435576\n",
            "Iteration 16374, Train Loss=9075.088733936918\n",
            "Iteration 16375, Train Loss=9068.266129498708\n",
            "Iteration 16376, Train Loss=9061.446695103632\n",
            "Iteration 16377, Train Loss=9054.630425458769\n",
            "Iteration 16378, Train Loss=9047.81731727537\n",
            "Iteration 16379, Train Loss=9041.007369179639\n",
            "Iteration 16380, Train Loss=9034.20058186221\n",
            "Iteration 16381, Train Loss=9027.396958164667\n",
            "Iteration 16382, Train Loss=9020.596503055072\n",
            "Iteration 16383, Train Loss=9013.799223843333\n",
            "Iteration 16384, Train Loss=9007.005130063853\n",
            "Iteration 16385, Train Loss=9000.214233680666\n",
            "Iteration 16386, Train Loss=8993.426549021784\n",
            "Iteration 16387, Train Loss=8986.642092859714\n",
            "Iteration 16388, Train Loss=8979.860884473504\n",
            "Iteration 16389, Train Loss=8973.082945585433\n",
            "Iteration 16390, Train Loss=8966.308300505563\n",
            "Iteration 16391, Train Loss=8959.536975998606\n",
            "Iteration 16392, Train Loss=8952.769001403192\n",
            "Iteration 16393, Train Loss=8946.00440853601\n",
            "Iteration 16394, Train Loss=8939.24323170202\n",
            "Iteration 16395, Train Loss=8932.485507685993\n",
            "Iteration 16396, Train Loss=8925.73127564397\n",
            "Iteration 16397, Train Loss=8918.980577147711\n",
            "Iteration 16398, Train Loss=8912.233456014455\n",
            "Iteration 16399, Train Loss=8905.489958330416\n",
            "Iteration 16400, Train Loss=8898.750132294579\n",
            "Iteration 16401, Train Loss=8892.014028158184\n",
            "Iteration 16402, Train Loss=8885.281698126253\n",
            "Iteration 16403, Train Loss=8878.553196200308\n",
            "Iteration 16404, Train Loss=8871.828578123\n",
            "Iteration 16405, Train Loss=8865.107901163869\n",
            "Iteration 16406, Train Loss=8858.391224055993\n",
            "Iteration 16407, Train Loss=8851.678606782347\n",
            "Iteration 16408, Train Loss=8844.970110459222\n",
            "Iteration 16409, Train Loss=8838.26579716095\n",
            "Iteration 16410, Train Loss=8831.56572973813\n",
            "Iteration 16411, Train Loss=8824.869971679645\n",
            "Iteration 16412, Train Loss=8818.178586887498\n",
            "Iteration 16413, Train Loss=8811.49163955369\n",
            "Iteration 16414, Train Loss=8804.809193926452\n",
            "Iteration 16415, Train Loss=8798.13131417272\n",
            "Iteration 16416, Train Loss=8791.458064171147\n",
            "Iteration 16417, Train Loss=8784.789507341677\n",
            "Iteration 16418, Train Loss=8778.125706480761\n",
            "Iteration 16419, Train Loss=8771.46672356277\n",
            "Iteration 16420, Train Loss=8764.812619609806\n",
            "Iteration 16421, Train Loss=8758.163454489175\n",
            "Iteration 16422, Train Loss=8751.519286797262\n",
            "Iteration 16423, Train Loss=8744.88017367765\n",
            "Iteration 16424, Train Loss=8738.246170706512\n",
            "Iteration 16425, Train Loss=8731.617331747195\n",
            "Iteration 16426, Train Loss=8724.99370882939\n",
            "Iteration 16427, Train Loss=8718.375352047162\n",
            "Iteration 16428, Train Loss=8711.76230943731\n",
            "Iteration 16429, Train Loss=8705.154626911899\n",
            "Iteration 16430, Train Loss=8698.552348150697\n",
            "Iteration 16431, Train Loss=8691.955514555459\n",
            "Iteration 16432, Train Loss=8685.36416516961\n",
            "Iteration 16433, Train Loss=8678.778336643592\n",
            "Iteration 16434, Train Loss=8672.198063188634\n",
            "Iteration 16435, Train Loss=8665.623376550548\n",
            "Iteration 16436, Train Loss=8659.054305995962\n",
            "Iteration 16437, Train Loss=8652.490878296085\n",
            "Iteration 16438, Train Loss=8645.933117739285\n",
            "Iteration 16439, Train Loss=8639.381046129161\n",
            "Iteration 16440, Train Loss=8632.834682817147\n",
            "Iteration 16441, Train Loss=8626.294044717313\n",
            "Iteration 16442, Train Loss=8619.759146350723\n",
            "Iteration 16443, Train Loss=8613.22999988149\n",
            "Iteration 16444, Train Loss=8606.706615167432\n",
            "Iteration 16445, Train Loss=8600.18899981365\n",
            "Iteration 16446, Train Loss=8593.6771592298\n",
            "Iteration 16447, Train Loss=8587.171096698967\n",
            "Iteration 16448, Train Loss=8580.670813437368\n",
            "Iteration 16449, Train Loss=8574.176308675476\n",
            "Iteration 16450, Train Loss=8567.687579722327\n",
            "Iteration 16451, Train Loss=8561.204622051035\n",
            "Iteration 16452, Train Loss=8554.727429369737\n",
            "Iteration 16453, Train Loss=8548.255993707491\n",
            "Iteration 16454, Train Loss=8541.79030549094\n",
            "Iteration 16455, Train Loss=8535.330353627523\n",
            "Iteration 16456, Train Loss=8528.876125586947\n",
            "Iteration 16457, Train Loss=8522.427607480164\n",
            "Iteration 16458, Train Loss=8515.984784143075\n",
            "Iteration 16459, Train Loss=8509.547639212475\n",
            "Iteration 16460, Train Loss=8503.1161552074\n",
            "Iteration 16461, Train Loss=8496.69031360259\n",
            "Iteration 16462, Train Loss=8490.270094906757\n",
            "Iteration 16463, Train Loss=8483.855478732441\n",
            "Iteration 16464, Train Loss=8477.44644386954\n",
            "Iteration 16465, Train Loss=8471.042968351849\n",
            "Iteration 16466, Train Loss=8464.645029525065\n",
            "Iteration 16467, Train Loss=8458.252604109037\n",
            "Iteration 16468, Train Loss=8451.86566826035\n",
            "Iteration 16469, Train Loss=8445.484197629969\n",
            "Iteration 16470, Train Loss=8439.108167419834\n",
            "Iteration 16471, Train Loss=8432.73755243629\n",
            "Iteration 16472, Train Loss=8426.372327140129\n",
            "Iteration 16473, Train Loss=8420.012465695505\n",
            "Iteration 16474, Train Loss=8413.657942014479\n",
            "Iteration 16475, Train Loss=8407.30872980124\n",
            "Iteration 16476, Train Loss=8400.964802590717\n",
            "Iteration 16477, Train Loss=8394.626133788051\n",
            "Iteration 16478, Train Loss=8388.29269670245\n",
            "Iteration 16479, Train Loss=8381.964464581944\n",
            "Iteration 16480, Train Loss=8375.64141064305\n",
            "Iteration 16481, Train Loss=8369.32350810042\n",
            "Iteration 16482, Train Loss=8363.010730191461\n",
            "Iteration 16483, Train Loss=8356.703050203747\n",
            "Iteration 16484, Train Loss=8350.400441494614\n",
            "Iteration 16485, Train Loss=8344.102877514999\n",
            "Iteration 16486, Train Loss=8337.81033182508\n",
            "Iteration 16487, Train Loss=8331.522778114931\n",
            "Iteration 16488, Train Loss=8325.240190217612\n",
            "Iteration 16489, Train Loss=8318.962542125591\n",
            "Iteration 16490, Train Loss=8312.689808001975\n",
            "Iteration 16491, Train Loss=8306.421962193377\n",
            "Iteration 16492, Train Loss=8300.158979239279\n",
            "Iteration 16493, Train Loss=8293.900833882046\n",
            "Iteration 16494, Train Loss=8287.647501074192\n",
            "Iteration 16495, Train Loss=8281.398955986639\n",
            "Iteration 16496, Train Loss=8275.155174013702\n",
            "Iteration 16497, Train Loss=8268.916130779748\n",
            "Iteration 16498, Train Loss=8262.68180214167\n",
            "Iteration 16499, Train Loss=8256.452164195667\n",
            "Iteration 16500, Train Loss=8250.227193276814\n",
            "Iteration 16501, Train Loss=8244.006865965473\n",
            "Iteration 16502, Train Loss=8237.791159085036\n",
            "Iteration 16503, Train Loss=8231.580049707345\n",
            "Iteration 16504, Train Loss=8225.373515150186\n",
            "Iteration 16505, Train Loss=8219.17153298069\n",
            "Iteration 16506, Train Loss=8212.974081012515\n",
            "Iteration 16507, Train Loss=8206.781137309838\n",
            "Iteration 16508, Train Loss=8200.59268018063\n",
            "Iteration 16509, Train Loss=8194.40868818352\n",
            "Iteration 16510, Train Loss=8188.229140118208\n",
            "Iteration 16511, Train Loss=8182.0540150340585\n",
            "Iteration 16512, Train Loss=8175.883292216199\n",
            "Iteration 16513, Train Loss=8169.71695120027\n",
            "Iteration 16514, Train Loss=8163.554971749989\n",
            "Iteration 16515, Train Loss=8157.397333880324\n",
            "Iteration 16516, Train Loss=8151.244017825875\n",
            "Iteration 16517, Train Loss=8145.0950040743655\n",
            "Iteration 16518, Train Loss=8138.950273321422\n",
            "Iteration 16519, Train Loss=8132.809806522166\n",
            "Iteration 16520, Train Loss=8126.67358482226\n",
            "Iteration 16521, Train Loss=8120.5415896390095\n",
            "Iteration 16522, Train Loss=8114.413802556158\n",
            "Iteration 16523, Train Loss=8108.290205452368\n",
            "Iteration 16524, Train Loss=8102.170780336324\n",
            "Iteration 16525, Train Loss=8096.055509553335\n",
            "Iteration 16526, Train Loss=8089.944375524709\n",
            "Iteration 16527, Train Loss=8083.837361084326\n",
            "Iteration 16528, Train Loss=8077.7344490625455\n",
            "Iteration 16529, Train Loss=8071.635622847361\n",
            "Iteration 16530, Train Loss=8065.540865713502\n",
            "Iteration 16531, Train Loss=8059.450161787844\n",
            "Iteration 16532, Train Loss=8053.36349497744\n",
            "Iteration 16533, Train Loss=8047.280850699052\n",
            "Iteration 16534, Train Loss=8041.202214227233\n",
            "Iteration 16535, Train Loss=8035.1275739943985\n",
            "Iteration 16536, Train Loss=8029.056919339782\n",
            "Iteration 16537, Train Loss=8022.990247396842\n",
            "Iteration 16538, Train Loss=8016.927561283512\n",
            "Iteration 16539, Train Loss=8010.868886336789\n",
            "Iteration 16540, Train Loss=8004.814274625669\n",
            "Iteration 16541, Train Loss=7998.763848867313\n",
            "Iteration 16542, Train Loss=7992.7178391653\n",
            "Iteration 16543, Train Loss=7986.676718155797\n",
            "Iteration 16544, Train Loss=7980.6413762853845\n",
            "Iteration 16545, Train Loss=7974.613582068664\n",
            "Iteration 16546, Train Loss=7968.596724715816\n",
            "Iteration 16547, Train Loss=7962.5974976463185\n",
            "Iteration 16548, Train Loss=7956.628943496641\n",
            "Iteration 16549, Train Loss=7950.716919729687\n",
            "Iteration 16550, Train Loss=7944.912549144571\n",
            "Iteration 16551, Train Loss=7939.317895740443\n",
            "Iteration 16552, Train Loss=7934.137274652235\n",
            "Iteration 16553, Train Loss=7929.781261560139\n",
            "Iteration 16554, Train Loss=7927.079092576905\n",
            "Iteration 16555, Train Loss=7927.697173692133\n",
            "Iteration 16556, Train Loss=7934.990752545424\n",
            "Iteration 16557, Train Loss=7955.519817996179\n",
            "Iteration 16558, Train Loss=8001.793378723838\n",
            "Iteration 16559, Train Loss=8094.786792054312\n",
            "Iteration 16560, Train Loss=8262.78357738711\n",
            "Iteration 16561, Train Loss=8512.090756732538\n",
            "Iteration 16562, Train Loss=8758.61860554048\n",
            "Iteration 16563, Train Loss=8760.938861494331\n",
            "Iteration 16564, Train Loss=8379.887632818534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Plot"
      ],
      "metadata": {
        "id": "5AVJhhy7pBoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_losses)), train_losses, 'b')\n",
        "plt.plot(range(len(train_losses)), test_losses, 'r')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Test Loss'])\n",
        "plt.xlabel('Step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "bCIgvOfOpHS6",
        "outputId": "c7968459-a38c-47f8-ecb5-0aad825389df"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Step')"
            ]
          },
          "metadata": {},
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot of True vs Predicted Values"
      ],
      "metadata": {
        "id": "X3q4MnqLo2Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y, nn.forward(X))\n",
        "plt.xlabel('True')\n",
        "plt.ylabel('Predicted')"
      ],
      "metadata": {
        "id": "LVQ5UR46D49s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "605ba6c0-3205-40de-db2f-70410dc92351"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Predicted')"
            ]
          },
          "metadata": {},
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}